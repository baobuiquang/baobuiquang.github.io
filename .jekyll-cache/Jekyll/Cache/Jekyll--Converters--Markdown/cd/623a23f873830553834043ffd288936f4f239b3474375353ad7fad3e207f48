I">ﬁ<h2 id="1-descriptive-analyses">1. Descriptive analyses</h2>

<p>When it comes to business decisions, humans and machines approach things very differently. One element of this is that machines have essentially perfect memory. You can give it to ‚Äòem once and they‚Äôll probably give it back to you exactly the same way later. They are also able to see all of the data at once in detail at a way that humans can‚Äôt. On the other hand, they‚Äôre not very good at spotting general patterns in data. There‚Äôs some ways around that but it‚Äôs not one of the strong points of algorithms. Human decision-makers, on the other hand, are very good at finding patterns and connecting the data to outside situations. On the other hand, humans have limited cognitive bandwidth. We can only think of so many things at a time. One of the consequences of that is that we need to simplify the data. We need to narrow it down to a manageable level and try to find the signal in the noise. And so descriptive analyses are one way of doing this. It‚Äôs a little like cleaning up the mess in your data to find clarity in the meaning of what you have. And I like to think that there are three very general steps to descriptive statistics. Number one, visualize your data, make a graph and look at it. Number two, compute univariate descriptive statistics. There‚Äôs things like the mean. It‚Äôs an easy way of looking at one variable at a time. And then go on to measures of association, or the connection between the variables in your data. But before I move on, I do want to remind you of my goal in this course. I‚Äôm not trying to teach you all of the details of every procedure. Rather, I‚Äôm trying to give you a map, an overview of what‚Äôs involved in data science. We have excellent resources here at LinkedIn Learning and when you find something that looks like it‚Äôs going to be useful for you, I encourage you to go find some of the other resources that can give you the step-by-step detail you need. Right now, we‚Äôre trying to get a feel for what is possible and what sorts of things you can integrate. And so, with that in mind, let‚Äôs go back to the first step of descriptive analyses. And that‚Äôs to start by looking at your data. We‚Äôre visual animals and visual information is very dense in data. So you might try doing something as simple as a histogram. So this shows the distribution of scores in a quantitative variable. That‚Äôs also sometimes called a continuous variable. The bell curve, which is high in the middle, tapers off nicely into each side, doesn‚Äôt have any big outliers, is our common occurrence and it forms the basis of a lot of methods for analyzing data. On the other hand, if you‚Äôre working with something like financial data, you‚Äôre going to have a lot of positively-skewed distributions. Most of the numbers are at the low end and a very small number go very, very high up. Think of the valuations at companies, the cost of houses. That requires a different approach. But it‚Äôs easy to see it by looking what you have. Or maybe you have negative skew, where most of the people are at the high end and the trailing ones are at the low end. If you think of something like birth weight, that‚Äôs an example of this. Or maybe you have a U-shaped distribution where most of the people are either all the way at the right, all the way at the left, and although it‚Äôs possible for people to be in the middle there aren‚Äôt many. That‚Äôs a little bit like a polarizing movie and the reviews that it gets. But once you get some visualizations, you can look for one number that might be able to represent the entire collection. That‚Äôs a univariate descriptive. The most common of these is going to be the mode. If each box here represents one data point the mode is simply the most common. And that‚Äôs going to be right here on the left at one because there are more ones than there are of any other score. Or maybe you want the median, the score that splits the distribution into two equal-sized halves. We have six scores down here, we have six scores right here. So the median is 3.5. That splits the data set into two equal halves. Or you have the mean. This one actually has a formula, which means the sum of X divided by N. It also has a geometric expression. The mean is actually the balance point. If you put these as actual boxes on a seesaw the mean is where it‚Äôs going to balance. And in this case, it‚Äôs exactly at four. It‚Äôs going to rest flat at that point. And so these are very common procedures. I imagine you know them already but think of them as a good place to start when you‚Äôre looking at your data. And if you can choose a second number to describe your data, you should consider a measure of variability, which tells you how different the scores are from each other. So that can include things like the range, which is simply the distance between the highest and lowest score, the quartiles or IQR, which split the data up into 25% groups, the variance and the standard deviation, two very closely-related measures that are used in a lot of statistics, and you will also want to look at associations. And so for instance, this is a scatterplot that shows the association between the psychological characteristic of openness and agreeableness at a state-by-state level. You can look at some measures that give you a numerical description of association like the correlation coefficient or regression analysis, like I just barely showed you, or depending on your data, maybe an odds ratio or a risk ratio. But remember there‚Äôs a few things. The data that you‚Äôre analyzing must be representative of the larger group you‚Äôre trying to understand. And things like the level of measurement. Is it nominal, ordinal, interval, or ratio is going to have an impact on what measures you use and the kinds of inferences you can make. You always need to be attentive to the effect of outliers. You have one score that‚Äôs very different from all the others ‚Äòcause that‚Äôs going to throw off a lot of these measures. Also open-ended scores where you have like one, two, three, four, five, plus or undefined scores where somebody started something but didn‚Äôt finish can also have a dramatic effect on the data. So you want to screen your data for these things. Now, I can‚Äôt go into the detail of all of these things right here but we do have other courses here that can do that for you, such as Data Fluency, Exploring and Describing Data. Go there, go to the other courses available that give you an introduction to these basic concepts of understanding what‚Äôs going on in your data and describing the patterns that you can find so you can get started on the further exploration of your data science analyses.</p>

<h2 id="2-predictive-models">2. Predictive models</h2>

<p>Marriage is a beautiful thing where people come together and set out on a new path full of hope and possibilities. Then again, it‚Äôs been suggested that half of the marriages in the U.S. end up in divorce, which is a huge challenge for everyone involved. But 50 percent‚Äôs just a flip of a coin. If you were trying to predict whether a particular marriage would last or whether it would end in divorce, you could just predict that everybody would stay married or maybe everybody would get divorced and you‚Äôd be right 50% of the time without even trying. In a lot of fields, being right 50% of the time would be an astounding success. For example, maybe only 5% of companies that receive venture capital funding end up performing as projected and there‚Äôs billions of dollars at stake. If you could be right 50% of the time in your venture capital investments, you‚Äôd be on fire. And that brings up the obvious question: How can you tell which companies will succeed and which will fail? Not surprisingly, many methods have been proposed. Apparently, being too wordy in your emails is a sign of eminent business failure, but I think that‚Äôs anecdotal data and not a proper data science predictive analysis. But here‚Äôs the general approach for trying to use data to predict what‚Äôs going to happen. Find and use relevant past data. It doesn‚Äôt have to be really old. It can be data from yesterday. But you always have to use data in the past because that‚Äôs the only data you can get. And then you model the outcome using any of many possible choices. And then you take that model and you apply it to new data to see what‚Äôs going on in there. There‚Äôs actually a fourth critical step and it‚Äôs separate from applying, and that‚Äôs to validate your model by testing it against new data, often against data that‚Äôs been set aside for this very purpose. This is the step that‚Äôs often neglected in a lot of scientific research, but it‚Äôs nearly universal in predictive analytics and it‚Äôs a critical part of making sure that your model works well outside of the constraints of the data that you had available. Now there‚Äôs a number of areas where predictive analytics as a field has been especially useful. Things like predicting whether a particular person will develop an illness or whether they‚Äôll recover from an illness; whether a particular person is likely to pay off a mortgage or a loan, or whether an investment will pay off for you; and then even the more mundane things like building a recommendation engine to suggest other things that people can buy when they‚Äôre shopping online. All of these are hugely influential areas and major consumers of predictive analytics methods. Now, I do want to mention there are two different meanings of the word prediction when we talk about predictive analytics. One of them is trying to predict future events, and that‚Äôs using presently available data to predict something that will happen later in the future, or use past medical records to predict future health. And again, this is what we think of when we hear the word prediction. We think about trying to look into the future. On the other hand, it‚Äôs not even necessarily the most common use of that word in predictive analytics. The other possibly more common use is using prediction to refer to alternative events, that is, approximating how a human would perform the same task. So you‚Äôre going to have a machine do something like classifying photos and you want to see whether this is a person, whether this is a dog, whether this is a house, and you‚Äôre not trying to look into the future, but you‚Äôre trying to say if a person were to do this, what would they do, and we‚Äôre trying to accurately estimate what would happen in that case. And so, you also might try inferring what additional information might reveal. So we know 20 pieces of information about this medical case; well, from that, we might infer that they have this particular disease, but we wouldn‚Äôt know for sure until we do a direct test, so we‚Äôre trying to estimate what‚Äôs happening there. Now, when you go about your analysis, there‚Äôs a few general categories of methods for doing a predictive analytics project. Number one is classification methods. That includes things like k nearest neighbors and nearest centroid classification and also is connected to clustering methods such as k means. You can also use decision trees and random forests, which is several decision trees put together, as a way of tracking the most influential data in determining where a particular case is going to end up. And then also extremely powerful in data science are neural networks, a form of machine learning that has proven to be immensely adaptive and powerful, although very hard sometimes to follow exactly what‚Äôs going on in there. But all of these methods have been very useful within trying to predict what‚Äôs going to happen with a particular case. But I do want to mention one other approach that‚Äôs been enormously useful and dates back a lot further than a lot of these, and that‚Äôs regression analysis, which gives you an understandable equation to predict a single outcome based on multiple predictor variables. And it can be a very simple thing, like this is an equation that uses the amount of time a person spends on your website to predict their purchase volume. Now this is fictional data, but you get to see we have a scatter plot, we draw a regression line through it, and we even have an equation there at the top of the chart. And this is a regression equation written entirely symbolically. I showed this to you before. It‚Äôs where you‚Äôre trying to predict an outcome of y for individual i and you‚Äôre using several predictors, X1, X2, X3, and their regression coefficients to predict their score. So for instance, the example I used was predicting salary, and you can write it out this way, too, where the salary for individual i is going to be $50,000, that‚Äôs the intercept, plus $2,000 for each year of experience, plus $5,000 in each step of their negotiating ability on a one-to-five scale, plus $30,000 if they‚Äôre the founder or owner of the company. And so that‚Äôs a regression equation, and it‚Äôd be very useful for predicting something like salary. And it‚Äôs a very easy, conceptually easy way to analyze the data and make sense of the results. And there are a few nice things about regression models. Number one is they‚Äôre very flexible in the kind of data they can work with. Different versions of regression can work with predictors or outcomes that are quantitative or continuous, ordinal, dichotomous or polychotomous categorical variables. They also can create flexible models. They‚Äôre usually linear. They can also be curvilinear, they can be quantile based, they can be multi-leveled. You have a lot of choices. And generally, they‚Äôre easy to interpret, that‚Äôs compared to many other data science procedures. The results of regression analysis are easy to read, interpret, present, and even to put into action. But this is simply one choice among many for predictive analytics where you‚Äôre trying to use your data to estimate what‚Äôs going to happen in the future. We have a lot of resources here where you can find other courses that will give you specific instruction on predictive analytics methods and help you find the actionable next steps in your data.</p>

<h2 id="3-trend-analysis">3. Trend analysis</h2>

<p>When you go out hiking in a new place, it‚Äôs nice to have a path to help you get where you‚Äôre going. It‚Äôs also nice to know that the path might actually take you to some place you want to be, so how can you see where you‚Äôre going, and how long it‚Äôs going to take you to get there? Well, as a data scientist, your job is to figure out the path your data is on, so you can inform decisions about whether to stay on the current path, or whether changes need to be made. The most basic way to do this is with trend analysis, and it starts by plotting a line. Simply make a graph of the changes over time, and then connect the points to make a clear line of one kind or another. Now, when you‚Äôre doing the analysis, you have to be worried about something a little different from other analyses we may have looked at, and that‚Äôs something called autocorrelation, or self-correlation. The idea here is that every value is influenced by the previous values, more or less. So today‚Äôs value, say, for instance, on number of visitors to your site, is going to be associated with yesterday‚Äôs value, which is going to be associated with the day before, and what you‚Äôre looking for is consistency of change, and there are several different ways to think about the change that happens over time. In fact, what you‚Äôre trying to do is to find the function, so you‚Äôre trying to get the outcome variable, like number of visitors to a site, as a function of time and the previous value. So you‚Äôre trying to find the function, like a mathematical function, for that particular line, and it actually may be cyclical. It may go up and down over time, or it may be several functions combined all at once. Let me show you some of the most basic ones, and I‚Äôm just showing you the line, not the data that would determine the line. There would be points all around it, and maybe you have something that has perfect linear growth, where you add the exact same quantity at each time period, like dollars per hour, or per year for an employee. So maybe you‚Äôll have $100 for a certain amount of time, and if you work twice as much you get 200, and so on, but it always goes up the same amount, so it‚Äôs linear growth. It‚Äôs easy to work with. On the other hand, in a lot of situations, you actually have what‚Äôs called exponential growth, where the rate of acceleration, think about when you‚Äôre squaring something, this is x squared, or you grow by 2% at every time period, or maybe 10% per year, then the curve is going to look like this. It‚Äôs what you expect when you‚Äôre adding the same percentage at each time. Or maybe you have logarithmic growth, where it starts off rapidly, but it approaches a ceiling value, like operating at 100% capacity, if the initial growth is fast, and you can‚Äôt go past that, so it diminishes, the rate of change diminishes, even if you‚Äôre still going up over time. Sometimes you can have something slightly more complicated like a sigmoid, or a logistic function, where it starts slowly and then it accelerates rapidly, and then tapers off as limits are reached. Again, like reaching market saturation once your ad campaign has been massively successful. And then you may have a sinusoidal, or a cyclic sine wave, where things go up and down over time. Sometimes you have patterns that go over time. For instance, a lot of my work appeals to people who are in school, and so I see ups and downs that correspond with the fall and the spring semester, and with winter and summer breaks, and I get a pattern, approximately like this. Now, there are a few advanced options for trend analysis, like a change point analysis, where you‚Äôre looking for a substantial, perhaps qualitative change over time, like a flock that‚Äôs moving together, then perching, moving again, and so on, and you‚Äôre trying to look at those transitions. So change points are the changes in the resting state of the data. There may be fluctuations still going around it, and you will probably want to check for historical events that can explain those changes, things you know are going on in the environment. So for instance, here‚Äôs an example that I‚Äôve used before, and it shows the number of inventions registered from 1860 up through 1960. And a change point analysis, which I conducted using the programming language R, shows that there are these relatively stable periods, with some fluctuation around it, but you can see that it jumps up dramatically around the early 1880s, and then settles down for a long period of time, and then drops down again. These are the sorts of qualitative changes that you‚Äôll want to be able to explain as result of your analysis. You can also try breaking things down from the whole into their elements, to try to see what‚Äôs happening with your data over time. This is decomposition. Think of it like disassembling a clock or some other item. You‚Äôre going to take the trend over time and break it down into several separate elements. You‚Äôre going to look at the overall trend, you‚Äôre going to look at seasonal or a cyclical trend, and you‚Äôre going to have some leftover random noise that you haven‚Äôt modeled yet. So here‚Äôs a graph showing some stock prices over a period of time, from 1990 up to about 2017. And what we can find here, is if we do the decomposition analysis, which again, I did in R, this is the original trend. It‚Äôs a compressed version of what I just showed you, but when we decompose it into several elements, we get this smooth trend, and this is removing a lot of the day to day fluctuation. You can see it‚Äôs basically uphill. Then we have the seasonal variation. I set it for a one year repetition, and you see it going up and down over time. And then this is the left over random noise. But this is one way of looking at changes over time, and trying to get some of the meaning out of it by using one kind of analysis or another. All of these start by simply plotting the dots and connecting the line over time, and then there are different ways to explore what the meaning of that might be. And depending on the situation, the field that you‚Äôre working in, or exactly what you‚Äôre trying to get out of it, you‚Äôll use one approach or another, but any of these will be an excellent way of getting started on finding out what path you‚Äôre on, and what you can expect in the near future.</p>

<h2 id="4-clustering">4. Clustering</h2>

<p>Everybody in a crowd is their own person. Each person is a unique individual, and perhaps, in an ideal world, your organization would acknowledge that and interact with each person in a tailored and unique way. But for right now, we face a lot of limitations, and there are plenty of times when it‚Äôs helpful to create groups or clusters of people that might be similar in important ways. These can include marketing segments, where you might give the same ads or the same offers to a group of people. Or developing curricula for exceptional students, like gifted and talented or artistic students, or maybe developing treatments for similar medical groups. Now, when you look at clusters in the United States, it‚Äôs easy to start with each state represented separately, but it‚Äôs really common practice to group these states into, say, four large regions of geographically adjacent states, like the South, the West, the North East and the Midwest. And that makes a lot of sense if you‚Äôre actually having to travel around from one to another, but you don‚Äôt have to group by just what‚Äôs physically next to each other. For example, a soccer team has matching jerseys, and they coordinate their movement, ‚Äòcause they‚Äôre a team. These could serve as the basis of maybe a behavioral cluster as opposed to a geographic one. And you can use a lot of different measures for assessing similarity, not just physical location. You can look at things like a K-dimensional space. So you locate each data point, each observation, in a multidimensional space with K-dimensions for K variables. So if you have five dimensions, K is five. If you have 500, then you have 500 dimensions. What you need to do then, is you need to find a way to measure the distance between each point, and you‚Äôre going to do one point, every other point, and you‚Äôre looking for clumps and gaps. You can measure distance in a lot of ways. You can use Euclidean distance, that‚Äôs the standard straight line between points in a multidimensional space. You can use things like Manhattan distance, and Jaccard distance, cosine distance, edit distance, there‚Äôs a lot of choices in how you measure the distance, or the similarity, between points in your data set. Let me give you an example, though, of cluster analysis in real life. So for instance, one of my favorite studies is based on what‚Äôs called the Big 5. I have a background in social and personality psychology. The Big 5 is a group of five very common personality factors that show up in a lot of different places under a lot of different situations. The actual factors are extraversion versus introversion, agreeableness, conscientiousness, neuroticism, which means your emotions change, and as opposed to stability, and then openness, specifically openness to new experiences. Once study I know actually tried to group the states in the U.S. using these Big 5 personality factors. They got information from social media posts, and then evaluated each state and created a profile. And from that, they found that the states in the U.S. went into three very broad groups. The big group there in orange down the middle, they called the friendly and conventional. The yellow off to the west coast, and actually a little bit off on the east coast, they called relaxed and creative, and then in green, which is mostly the north east, but also Texas, is temperamental and uninhibited, and these are different ways of thinking about the kinds of things that people think about and the way that they behave. Now, you could use psychological characteristics, or maybe you could group states by, for instance, how they search for things online, which might be more relevant if you‚Äôre doing e-commerce. So, I went to Google Correlate, and I chose a dozen search terms that I thought might roughly correspond to the Big 5 personality factors, and what that data tells you is the relative popularity of each search term on a state-by-state basis. I then did an analysis in R, doing what‚Äôs called a hierarchical cluster, where all of the states start together, and then it splits them apart one step at a time. And you can see, for instance, that my state of Utah is kind of unusual by itself over here, but you can see the degree of connection between each of the states, until finally all of the 48, it doesn‚Äôt include Alaska or Hawaii, because the original researchers didn‚Äôt have that in their personality data, but I could say, ‚ÄúGive me two groups,‚Äù and then it groups them this way. You can see we have just these five states over here listed on the right, or we could say, ‚ÄúGive us three groups,‚Äù in which case all it does is it separates Utah from those other five. But you can go down to a level that seems to work well, something that makes sense and that works with your organization‚Äôs needs. Now, I want you to know that when you‚Äôre doing a cluster analysis, you could do a hierarchical clustering, which I just did, that‚Äôs a very common approach, but there‚Äôs a lot of alternatives. You can do something called K-means, or a group centroid model. You can use density models or distribution models, or a linkage clustering model. Again, we have other resources here that will show you the details on each of these and how to carry them out. Mostly I want you to be aware that these alternatives exist, and they can be useful in different situations for putting your data together. Remember, with any analysis, something like clustering exists to help you decide how you want to do things. So use your own experience, and use common sense as you interpret and implement the results of your analysis, and you will get more value and more direction out of that for your own organization.</p>

<h2 id="5-classifying">5. Classifying</h2>

<p>So maybe you‚Äôve got a dog and maybe your dog does cute things like sticking its nose in your camera. Now, my dog‚Äôs too short for that, but you take a picture or a video to save the moment. But one interesting consequence of that process is that now your phone‚Äôs photo program is going to start analyzing the photo to determine what it‚Äôs a photo of. That way, you can search for it later by typing dog without ever having had to tell the program that‚Äôs what it is. And that‚Äôs the result of a machine learning algorithm taking the data to analyze the photos and classify it as a dog, a cat, a child, and add those labels to the data. In fact, classifying is one of the most important tasks that data science algorithms perform, and they do it on all kinds of data. The general idea of automated classification is pretty simple to describe. Locate the case in a k-dimensional space where k is the number of variables or different kinds of information that you have. And there‚Äôs probably going to be more than three. It might be hundreds or thousands. But once you get it located in that space, compare the labels on nearby data, that of course assuming that other data already has labels that it says whether it‚Äôs a photo of a cat, or a dog, or a building. And then once you‚Äôve done that, assign the new case to the same category as the nearby data. So in principle it‚Äôs a pretty simple process. Now in terms of what data you‚Äôre going to assign it to, you can do that using one of two different methods among other choices. A very common one is called k-means, and this is where you choose the number of categories that you want. You can actually say I only want two, or I want five, or I want 100. And then what the algorithm does is it creates centroids. That‚Äôs like a mean in multi-dimensional space, and it will create as many centroids as you want groups. And so when you put your new data in, it will assign that new case to the closest of those k centroids. Again, might be two, might be five, might be 100. Another approach is called k-nearest neighbors, and what it does in this case is it finds where your data is in the multi-dimensional space, it looks at the closest cases next to it, and you can pick how many you want. It might be the five closes, the 20 closest, the 100 closest. And look at the categories of those cases and assign your new data to the most common category among them. Now as you might guess, classification is a huge topic in data science, machine learning, and artificial intelligence, and so there are many, many options on how to do this process, and you‚Äôre going to have to spend a little time talking with your team to decide which approach is going to best meet your individual goals. Now some of the things you‚Äôre going to have to consider are things like whether you want to make a binary classification, that‚Äôs just a yes, no, like whether a credit card transaction is or is not fraudulent, or whether you have many possible categories like what‚Äôs in a photo or what kind of movie to recommend to someone. You also have a lot of choices for how you measure the distance, how close is it to something else. You can use euclidean distance, Manhattan distance, edit distance, and so on. And you also need to decide whether you‚Äôre going to compare it to one central point like a centroid or several nearby points. And then you also need to make a decision about confidence level, especially when you have a significant classification, how certain do you have to be that that‚Äôs the right one. Some cases fit beautifully, others are much harder to classify. Now, once you‚Äôve done the classification, you want to evaluate your performance, and there‚Äôs a few different ways to do that. You can look at the total accuracy. So if you have like a binary classification, is a legitimate transaction, is a fraudulent transaction, what percentage of the total cases got put into the right category. This is simple to calculate and it‚Äôs intuitive to understand, but it‚Äôs problematic because if one category is much more common than the others, then you can get high overall accuracy without even having a functional model. So you want to start looking at things a little more particularly like for instance sensitivity. This is the true positive rate or if a person‚Äôs supposed to be in a particular category, what‚Äôs the likelihood that that will actually happen. So if a person has a disease, what‚Äôs the probability they will be correctly diagnosed with that disease? And there‚Äôs also specificity, which is like the true negative rate, and what this means is that the case should only be categorized when it is supposed to go in there. You don‚Äôt want these other cases accidentally going in. And that‚Äôs one of the purposes of Bayes‚Äô Theorem, which I‚Äôve talked about elsewhere. Bayes‚Äô Theorem allows you to combine data about sensitivity, specificity, and the base rates, how common the thing is overall. And again, my goal here is not to show you the step-by-step details, but to give you an overall map. For more information on classification methods, we have a wide variety of courses uses languages like Python and R that can walk you through the entire process. But the goal is the same, using automated methods to help you identify what you have and by placing it into relevant categories, helping you get more context and more value out of the data so you can provide better products and services.</p>

<h2 id="6-anomaly-detection">6. Anomaly detection</h2>

<p>Some time ago, I opened up Launchpad on my Mac, which is a way of launching applications, and it‚Äôs supposed to look like this. However, this particular time, something weird happened, and this is what I saw instead. Now, normally, when you get an anomaly like this, you just restart the app or reboot the computer, but it turns out I‚Äôm fascinated by generative art or art that comes through as the result of an algorithm, often with a fair amount of randomness thrown in. So before I restarted things and got it back to normal, I took a screenshot, and I‚Äôve gone back to it several times. I consider this an excellent example of found generative art, really, a happy digital glitch or a fortuitous anomaly. It‚Äôs also an example of serendipity or the unexpected insight that can come along. Well-known examples of serendipity include Silly Putty, Velcro, Popsicles, and of course, the Post-It Notes that every office has. You can think about these as trying to find anomalies, unusual things, and latching onto them. Now, usually when we talk about anomalies, we talk about things like fraud detection. Is a particular transaction legitimate or fraudulent? You can also use it to detect imminent process failure like a machine‚Äôs going to break or an employee has a heightened risk of burnout or leaving the company, but you can also think of this as a way of identifying cases with potentially untapped value, a new category, a new audience that you can work with. Now, what all of these have in common are the focus on outliers. These are cases that are distant from the others in a multidimensional space. They also can be cases that don‚Äôt follow an expected pattern or a trend over time, or in the case of fraud, they may be cases that match known anomalies or other fraudulent cases. Any of these can be ways of identifying these anomalies and responding appropriately to them, and when you do that, it brings up the usual suspects, the usual method for analyzing data in data science, things like regression. Does this particular observation fit well with the prediction, or is there a large error? You can do Bayesian analysis to get a posterior probability that this is a fraudulent transaction. You can do hierarchical clustering or even do neural networks as a way of finding how well the data fits these known patterns, and if it doesn‚Äôt, you may have an anomaly. Now, there are a couple of things that make this a little harder than it might be otherwise. Number one is that we are dealing with rare events. By definition, if it‚Äôs an anomaly, it‚Äôs not common. So things like fraud are uncommon, and that leads to what are called unbalanced models. When you‚Äôre trying to predict something that happens only 1% or 1/10 of a percent of the time, you got to have a huge amount of data, and you have to have a model that can deal well with that kind of categorical imbalance. The second thing is difficult data. You may not be dealing just with a nice SQL database. You may have biometrics data. You may have multimedia data. You may have things like time-sensitive signatures where you have to measure how it happened over an event. So as an example of all of this, think about when you‚Äôve used your credit card to make a purchase online. You, the online store, and your credit card company all have a vested interest in making sure the transaction is legitimate because fraud costs money, it takes times, and it causes headaches. So your credit card company could take several steps to identify legitimate cases and potential anomalies. They might look at something like the purchase characteristics. What was purchased? For how much? When and where? Through what means, and so on. I got a call a few years ago from my credit card company when someone tried to spend several thousand dollars on a hotel several thousand miles from my home. You can also use personal measures, things like biometrics or latency and typing, or you can measure a person‚Äôs height approximately by the angle at which they hold their cell phone, the mode of transportation they‚Äôre on by getting the vibrations through the accelerometer, or an interesting one is a signature of their name or a person trying to find the cursor on their computer is, in fact, a signature that can be measured and stored, and new data can be compared against that, and then, there are general trends. Are there new scams going around? Are they more common in one area or another? Have scammers found ways around old technology and so on. Needless to say, fraud detection‚Äôs a cat-and-mouse game, so there‚Äôs constant research and progress in data science to deal with new methods of fraud and harnessing the evolving capabilities of machines and algorithms, and that means that you‚Äôll want to stay in touch with the resources available here to learn the specific details about the latest and greatest methods in data science for things like fraud detection and any kind of anomaly detection, including the potential for new discoveries via serendipity.</p>

<h2 id="7-dimensionality-reduction">7. Dimensionality reduction</h2>

<p>Back in 1957 in his legendary song, Rock ‚Äòn Roll Music, Chuck Berry sang a lament about musicians who make things too complicated and to quote, ‚Äúchange the beauty of the melody ‚Äúuntil it sounds just like a symphony.‚Äù And that‚Äôs why he loves rock ‚Äòn roll music. The same idea explains why most bands have four people like the Beatles right here or possibly three or five. That‚Äôs enough people, enough instruments to fill all the sonic regions without overwhelming with too much information and resulting in cacophony. Jumping way ahead in time, there‚Äôs a similar problem in data science. We think of data coming down in a matrix-like stream here as really cool, but it‚Äôs hard to get meaning out of it and it‚Äôs hard to know what to do as a result of it. We need a way to get through the confusion and the haze and pull things into focus. Fortunately, there‚Äôs a way to do that in data science. The idea of dimension reduction is to actually reduce the number of variables and the amount of data that you‚Äôre dealing with. So instead of dealing with dozens or hundreds or maybe even thousands of variables, you‚Äôre dealing with a single score like how likely a person is to behave in a particular way. It sounds counterintuitive, but there are actually some very good reasons for doing this. First off, each variable, each factor or feature has error associated with it. It doesn‚Äôt measure exactly what you want. It brings in some other stuff. But when you have many variables or features together that you combine, the errors tend to cancel out. So if they‚Äôre all pointing in slightly different directions, you end up centered on what it is you want. Also, by going from many individual measurements to a single conglomerate measurement, that reduces the effect of something called colinearity, which is the association, the overlap between predictor variables in the model, which creates some significant problems. So if you have fewer variables, there‚Äôs less problems for colinearity. Also, not surprisingly when you have a few features you‚Äôre dealing with instead of hundreds, you are able to do things faster. Your computer is able to process the information with greater speed. And another really nice consequence of this is it improves generalizability. Again, because you‚Äôre getting rid of or averaging out the idiosyncratic variation with each observation, with each variable, and you‚Äôre going to get something much more stable that you‚Äôre able to apply to new situations better. Now, there are two general ways to do this. There are a lot more options, but the two most common are these. Number one is principal component analysis, often just called principal components or PCA. And the idea here is that you take your multiple correlated variables and you combine them into a single component score. So let‚Äôs say you give a personality questionnaire and it‚Äôs got 50 questions on it, but you have 10 questions for each element of personality, then you can combine those into 10 components if the analysis supports that combination. And then you only have five things to deal with as opposed to 50 and it‚Äôs much easier to deal with. Another very common approach is factor analysis. And functionally, it works exactly the same way. People use it for the same thing, although the philosophy behind factor analysis is very different. Here your goal is to find the underlying common factor that gives rise to multiple indicators. So in principal component analysis, the variables come first and the component results from it. In factor analysis, this hidden factor comes first and gives rise to the individual variables. That said, even though they are conceptually very different that way, people tend to use the two often interchangeably. What they let you do is group variables in ways that make sense. Now, there are a lot of variations on methods for dimension reduction. You, for instance, might be engaged in an exploratory analysis where you‚Äôre just trying to find out what‚Äôs there in the data in front of you or a confirmatory analysis where you have a known structure and you‚Äôre trying to see how well your current data fit. You have different methods for different levels of measurement. If you have a quantitative thing where you‚Äôre looking at how it takes somebody to do something or the value of their purchases, that‚Äôs one approach. But if you‚Äôre counting yes or no are they in this particular category, you‚Äôre going to need to do something else. Also, there are multiple algorithms, many different ways of measuring the similarity between variables and measuring ways of overlap and the degree that they share. And so there are some very important details in these, but we can save that for a more detailed analysis in another video. Right now I want you to know that the procedure exists. Right now I want you to know that there‚Äôs the possibility of doing this and that it‚Äôs worth looking into. I mean, think about it. Dimension reduction in data is like learning to read a language. At first you just see random shapes. Then you see individual characters, then words, then sentences, and then finally ideas. You can go from 100 pieces of information, a line here or a circle there to just a handful and that‚Äôs what you need to get meaning out of your data and to do something useful with it.</p>

<h2 id="8-feature-selection-and-creation">8. Feature selection and creation</h2>

<p>I teach statistics to undergraduate students who don‚Äôt always see how it connects to their lives. I can give specific examples about each of the fields, but I found that even the most recalcitrant student can get excited about data when we talk about sports like baseball. Baseball‚Äôs a data-friendly sport. It‚Äôs been going on for over 100 years, there are 162 games in the regular season, and they count everything. If you‚Äôre trying to figure out how good, for example, a particular batter is, you can start with these basic bits of data and you‚Äôll have an enormous amount of information to work with. These are the features in the dataset that you start with. But if you‚Äôre a coach or a manager, you can do a lot more than just use those raw data points to make a strategy. You can start combining them to create new features in your dataset and finding value and possibilities in your team. Now you can start with really simple ones. This is the batting average, and all it is is the number of hits divided by the number of at bats, and you need to know that those are defined in particular ways, but it‚Äôs just one number divided by the other. Or you can get something more sophisticated like the on-base percentage, where you take three things, the number of hits, the number of bases on balls and hit by pitch and divide that by four things. At bats, bases on balls, hit by a pitch, and sacrificed flies. That gives you a better measure, according to some judgment, or if you want to jump ahead to the 21st century, you can start getting really fancy with something like the weighted runs created plus where you have a whole plethora of things you‚Äôre putting together and what‚Äôs interesting is every one of these is actually its own formula going into it, so that one‚Äôs complicated, but it‚Äôs all based on these little bits and pieces of information that are available. Before I go ahead, I want to mention one thing and that‚Äôs feature selection and creation is a different process than the dimension reduction that I mentioned elsewhere. Dimension reduction‚Äôs often used as a part of getting the data ready so you can then start looking at which features to include in the models you‚Äôre creating and that‚Äôs what we‚Äôre addressing right now. So given that you have these formulas to create all these new features to pick the best players or best outcomes, which one should you actually use when you‚Äôre making that decision? Which ones have the greatest decision value? Well, if you‚Äôve seen the movie Moneyball, which is a dramatized account of how the Oakland A‚Äôs general manager Billy Beane used data to select and assign players, you will remember he keeps directing the scouts towards one primary factor over any other. Whether a player reliably gets on base. He had data to drive that decision and to guide him to that focus, although they didn‚Äôt share that process with us in the movie, but I can tell you basically how it works outside of baseball. There are a few methods that you can use for feature selection and feature creation in your data science projects. So for instance, you can start with just basic correlation. Is this variable correlated with the outcome or is this variable correlated? Which one has a bigger correlation? That works, but it‚Äôs one variable at a time and ‚Äòcause correlation generally looks at linear associations, it has some limits. Or you could do something called stepwise regression where you take all of your potential predictor variables, you put them in the computer, and you say this is the outcome variable and it looks at correlations and picks the best one and then it starts doing what are called partial correlations, and it‚Äôs a really easy way to sift through the data. You know, you just hit go and it‚Äôs done. The problem, however, is that stepwise regression really capitalizes on chance fluctuation in your dataset, and you can set stuff that simply is not going to generalize to anything else, and so stepwise is generally a poor choice, even if it‚Äôs an easy one. On the other hand, more modern methods like lasso regression, that‚Äôs least absolute shrinkage and selection operator, and ridge regression are better ways that are most robust to these flukes of chance variation, and they give a better impression of the variables‚Äô role in the equation and which ones you should emphasize. And if you do something like a neural network, you can do variable importance and there‚Äôs a lot of other different ways of evaluating each one of these, but when you‚Äôre selecting your variables, there‚Äôs a few things you want to keep in mind. Number one, is it something that you can control? Ideally, if you‚Äôre trying to bring about a particular outcome, you can to have the ability to make it happen. So look at variables that are under your control or that you can select, at least, and then look at the ROI, the return on investment. Not everything that can be manipulated or controlled can be controlled easily or inexpensively, and os you need to look at the combined cost and the value or the benefit that you get from working with that particular predictor. And then the third one is is it sensible? Does the model make sense? Does it make sense to include this particular variable in your equation? You‚Äôve got experience. You know your domain. Always keep that in mind as you‚Äôre evaluating the information that you can use in your model to make predictions. That taken together will let you make an informed choice about the best things in your data for predicting and ideally, for bringing about the things that matter to you and to your organization.</p>

<h2 id="9-validating-models">9. Validating models</h2>

<p>Several years ago, my wife and I adopted our second child. No, this isn‚Äôt her, this is stunt double. We‚Äôve loved and we‚Äôve raised her the best we could but we did make one critical error that has actually caused her unanticipated grief and could take her years to recover from. We used a non-standard character in her name. Those two dots over the E are a diaeresis, which indicates that the second vowel is to be pronounced as its own syllable. And, by the way, that‚Äôs not to be confused with the identical looking, but functionally distinct umlaut which softens the sound of a vowel or indicates that you‚Äôre a heavy metal band. It turns out, there is still a lot of web forms out there that don‚Äôt like non ASCII characters and will tell you that you‚Äôve entered invalid data even if it is the name that your parents gave you. And there are other people whose names have trouble with modern computers, aside from having things like apostrophes or hyphens. So maybe you‚Äôve got a mononym, just a single name. It‚Äôs common among pop singers and it‚Äôs also not unheard of in Indonesia and Yanmar. Or even have as single letter for a name. O happens in Korea, although it‚Äôs often spelled as OH and E occurs occasionally in China. Or, you are Hawaiian and you‚Äôve got a really expansive last name or the most challenging of all are the handful of people whose last name is Null, which wreaks all sorts of havoc with databases and leads to either hilarity or immense frustration trying to get things done online. Now, what‚Äôs happening here, I imagine, is that a well meaning programmer has created a name validation system that checks whether a person has entered an actual name. And they‚Äôve checked how well that worked against a database of names that they had that unfortunately wasn‚Äôt representative of all the possibilities. Remember, the world is a big place, and apparently programmers don‚Äôt name their kids Zoe so problems come up and the system breaks down. What that lets you know is that you should always check your work. That‚Äôs the principle of validating your models in data science. You may have tailored it really nicely to the data that you had, but is it going to work with anything else? Is it going to survive out there? It reminds me of one of the all time great quotes from computer science, beware of bugs in the above code, I have only proved it correct, not tried it. And that‚Äôs from Donald Knuth, an illustrious computer science professor and author of the Multi-volume, The Art of Computer Programming. So, you need to validate your data. You need to make sure that your model works. The basic principle is pretty easy, even if people outside of data science don‚Äôt do it very often. Take your data and split it up into two groups. Training data and testing data. And training data‚Äôs like the data that you send to class and you teach it how to identify the outcome that you‚Äôre looking at and you build a model with that training data. But then you need to check how well you‚Äôve taught it and you go into the testing data, now there‚Äôs two kinds of testing data, one of them doesn‚Äôt really count and that‚Äôs cross-validation. Now you can say it‚Äôs testing data but it‚Äôs actually using the training data. What you do here is you take the training data and you split it up into several pieces, maybe six different groups and then you use five at a time to build a model and then you use the sixth group to test it and then you rotate through a different set of five and you verify them against a different one sixth of the data and so on an so forth. So that‚Äôs internal, but it still allows you to build models that are going to be a little more robust against variations in the data. But the gold standard is holdout testing data, or holdout validation. This is where you take that maybe 20% of data that you set aside way at the beginning and you‚Äôve never looked at it, you‚Äôve never touched it, now you take the model you‚Äôve built from your training data and ideally you have gone through the cross validation process also, you take that model and you apply it just once to the holdout data and that one is going to give you the true measure of the accuracy of your model. Not, when you‚Äôre able to twist it and tweak it however you wanted but testing it against data sort of in the wild and the general idea here is now you have an idea of how robust your model is and how well it can function outside the box. Once you do that, you‚Äôre going to be certain that your work does what you think it should do and you can be confident that your model is ready to go out there into the real world.</p>

<h2 id="10-aggregating-models">10. Aggregating models</h2>

<p>There‚Äôs a saying that life imitates art. Well, for some time as I‚Äôve been preparing for this presentation I‚Äôve planned to talk about how people estimate the amount of money in a jar full of coins. And then literally today we inherited a giant jar of coins from my mother-in-law and we‚Äôre currently asking extended family members for their guesses as to how much money is in this very heavy jar. I‚Äôve guessed a low-ball estimate of 165. My wife was more optimistic and guessed $642.50. We actually won‚Äôt know the answer until next week when we cash in all the change, but I bring this up because it illustrates an important point. Any one guess, like an arrow shot at a target, maybe high, maybe low, maybe more or less accurate, but as it happens, if you take many guesses and average them, the errors tend to cancel out, and you end up with a composite estimate that‚Äôs generally closer to the true value than any one single guess is. This is sometimes called the wisdom of the crowd, although in statistics it‚Äôs a function of something called the central limit theorem and the way that sampling distributions behave. But the idea is that combining information is nearly always better than individual bits and pieces of information. In data science, the wisdom of the crowd comes in the form of combining the results of several different models looking at the same outcome and the same data set. Or you maybe use models like linear regression, and a lasso regression, and a decision tree, and a neural network, all predicting the same outcomes. And there‚Äôs several different ways that you can combine the estimates from each of your models to create an ensemble estimate. If you‚Äôre modeling a categorical or nominal outcome like trying to decide what‚Äôs in a picture or whether a new contact is likely to make a purchase, each model will give its prediction and then you can take the most common category across the predictions. It‚Äôs like having people raise their hands to vote. Or, if you‚Äôre modeling a quantitative or a continuous outcome like the number of people who will respond to a nonprofit‚Äôs fundraising appeal, you can take the numbers that each model predicts and average them, sort of like mixing ingredients in a bowl. Or maybe you want to try something a little more sophisticated and use Bayesian methods to combine the posterior probabilities that each model gives you into a single aggregated or ensemble probability. Now, these are just simple methods and ways of thinking about it. There‚Äôs a lot of work in the field of ensemble modeling, or models that combine the results of multiple models but this gives you a general idea. Now, let me explain very quickly what the benefits of this are, ‚Äòcause it does take a little extra time and effort. First off you get multiple perspectives on your data and on your outcome. Each method that you use, each algorithm, has its own strengths but it also things that it tends to skip over or get tripped up by. By combining the results you can compensate for some of those weaknesses and capitalize on those combined strengths. You also have a greater ability to find the signal, meaning the true value, the true outcome amid the noise of random variation and measurement error. You tend to get estimates that are both more stable across time and across methods and that are more generalizable to new cases and new situations. All of those are enormous benefits. You can think of it as a kind of cooperation between the models, the ensemble modeling. It‚Äôs the idea that many eyes on the same problem can lead to the best possible solution and that really is why you got involved in data science in the first place.</p>
:ET