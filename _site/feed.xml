<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-04-02T22:08:54+07:00</updated><id>/feed.xml</id><title type="html">Creatoper</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Publish your package to npm</title><link href="/notebook/publish-your-package" rel="alternate" type="text/html" title="Publish your package to npm" /><published>2021-03-25T20:00:00+07:00</published><updated>2021-03-25T20:00:00+07:00</updated><id>/notebook/publish-your-package</id><content type="html" xml:base="/notebook/publish-your-package">&lt;h2 id=&quot;1-what-is-npm-create-an-npm-account&quot;&gt;1. What is &lt;a href=&quot;https://www.npmjs.com/&quot;&gt;npm&lt;/a&gt;? Create an npm account.&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;npm&lt;/code&gt; is a package manager for the JavaScript programming language.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;npm&lt;/code&gt; is the command line client that allows developers to install and publish packages - packaged modules of code.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;npm&lt;/code&gt; is an &lt;a href=&quot;https://github.com/npm&quot;&gt;open source&lt;/a&gt; project and free to use.&lt;/p&gt;

&lt;p&gt;Official website: &lt;strong&gt;&lt;a href=&quot;https://npmjs.com/&quot;&gt;npm&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Documentation: &lt;strong&gt;&lt;a href=&quot;https://docs.npmjs.com/&quot;&gt;npm Docs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./post_img/npm/1-npm-landing-page.png&quot; alt=&quot;NPM Landing Page&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Create an npm account at: &lt;strong&gt;&lt;a href=&quot;https://www.npmjs.com/signup&quot;&gt;Sign Up&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Login: &lt;strong&gt;&lt;a href=&quot;https://www.npmjs.com/login&quot;&gt;Login&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../post_img/npm/2-npm-sign-up.png&quot; alt=&quot;NPM Sign Up&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-what-is-nodejs-download-nodejs&quot;&gt;2. What is &lt;a href=&quot;https://nodejs.org/en/&quot;&gt;Node.js&lt;/a&gt;? Download Node.js.&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Node.js&lt;/code&gt; is an open-source, cross-platform, back-end JavaScript runtime environment.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Node.js&lt;/code&gt; is an &lt;a href=&quot;https://github.com/nodejs&quot;&gt;open source&lt;/a&gt; project and free to use.&lt;/p&gt;

&lt;p&gt;Official website: &lt;strong&gt;&lt;a href=&quot;https://nodejs.org/en/&quot;&gt;Nodejs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Documentation: &lt;strong&gt;&lt;a href=&quot;https://nodejs.org/en/docs/&quot;&gt;Nodejs Docs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../post_img/npm/3-nodejs-landing-page.png&quot; alt=&quot;NPM Landing Page&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Download Node.js at: &lt;strong&gt;&lt;a href=&quot;https://nodejs.org/en/download/&quot;&gt;Download Node.js&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../post_img/npm/4-nodejs-download.png&quot; alt=&quot;Nodejs Download&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-prepare-your-package&quot;&gt;3. Prepare your package&lt;/h2&gt;

&lt;p&gt;Put all your package‚Äôs files in 1 folder&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;üìÅyour-pkg-name
‚îú‚îÄ‚îÄ üìÅyour-pkg-name-01
‚îú‚îÄ‚îÄ üìÅyour-pkg-name-02
‚îî‚îÄ‚îÄ üìÅyour-pkg-name-03
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;package.json&lt;/code&gt; file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;üìÅyour-pkg-name
‚îú‚îÄ‚îÄ üìÅyour-pkg-name-01
‚îú‚îÄ‚îÄ üìÅyour-pkg-name-02
‚îú‚îÄ‚îÄ üìÅyour-pkg-name-03
‚îî‚îÄ‚îÄ package.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Edit the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;package.json&lt;/code&gt; file&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;your-pkg-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1.0.1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;description&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Write your package's description here.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;main&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;your-pkg-name.js&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;scripts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;test&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;echo &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Error&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &amp;amp;&amp;amp; exit 1&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;keywords&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;your-package-keyword&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;author&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Your Name Here&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;license&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;MIT&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;4-publish-your-package&quot;&gt;4. Publish your package&lt;/h2&gt;

&lt;style&gt;
.youtube {
    width: 100%; height: 46.87vw;
    border: none; background: transparent;
} @media only screen and (min-width: 768px) {
    .youtube {
        width: 42vw; height: 23.625vw;
    }
}
&lt;/style&gt;

&lt;iframe class=&quot;youtube&quot; src=&quot;https://www.youtube.com/embed/lxxndOskI1o&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Open command line and navigate to your package‚Äôs folder&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cmd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Login your npm account&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npm login
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that when you input the password, there is no clue show that you are typing. This will be confused if this is the first time you use the command line.&lt;/p&gt;

&lt;p&gt;Initialize npm package manager&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npm init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you have prepared your package, all you need to do is just enter.&lt;/p&gt;

&lt;p&gt;Finally, type&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Publish your package&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npm publish
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you see the line&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+ your-pkg-name@1.0.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;your package has been published on npm successfully!&lt;/p&gt;

&lt;p&gt;If there is any error, you should:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Check your package‚Äôs name, it maybe used by other user (for example: you can‚Äôt name your package &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;react&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vue&lt;/code&gt;). In this case, you should change your package‚Äôs name, or publish your package under your name or organization‚Äôs name.&lt;/li&gt;
  &lt;li&gt;Check your folder‚Äôs structure, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;package.json&lt;/code&gt; file.&lt;/li&gt;
  &lt;li&gt;Check your npm account.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Bui Quang Bao</name></author><category term="technology" /><category term="web" /><category term="npm" /><summary type="html">1. What is npm? Create an npm account.</summary></entry><entry><title type="html">All markdown syntax</title><link href="/notebook/all-markdown-syntax" rel="alternate" type="text/html" title="All markdown syntax" /><published>2021-03-20T09:50:00+07:00</published><updated>2021-03-20T09:50:00+07:00</updated><id>/notebook/all-markdown-syntax</id><content type="html" xml:base="/notebook/all-markdown-syntax">&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas id imperdiet odio. Etiam quis volutpat mauris. Duis ligula lacus, maximus vel est sed, molestie finibus nisl. Aliquam erat volutpat. Mauris sit amet pretium urna, sit amet tristique enim. In eget arcu mollis, ultricies metus venenatis, tincidunt enim.&lt;/p&gt;

&lt;p&gt;Vestibulum ac sodales nisi, et malesuada tellus. Mauris eu nibh tortor. Aenean egestas enim in est imperdiet, in posuere arcu facilisis. Donec rutrum elit vitae sodales congue. Suspendisse sit amet dolor laoreet quam tempus rhoncus. Morbi viverra diam eu orci convallis, id sollicitudin justo blandit. Curabitur mattis dolor non ex rutrum, a auctor nisi malesuada.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# This is an &amp;lt;h1&amp;gt; tag
## This is an &amp;lt;h2&amp;gt; tag
### This is an &amp;lt;h3&amp;gt; tag
#### This is an &amp;lt;h4&amp;gt; tag
##### This is an &amp;lt;h5&amp;gt; tag
###### This is an &amp;lt;h6&amp;gt; tag
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;this-is-an-h1-tag&quot;&gt;This is an &amp;lt;h1&amp;gt; tag&lt;/h1&gt;
&lt;p&gt;Donec rutrum elit vitae sodales congue. Suspendisse sit amet dolor laoreet quam tempus rhoncus. Morbi viverra diam eu orci convallis, id sollicitudin justo blandit. Curabitur mattis dolor non ex rutrum, a auctor nisi malesuada.&lt;/p&gt;
&lt;h2 id=&quot;this-is-an-h2-tag&quot;&gt;This is an &amp;lt;h2&amp;gt; tag&lt;/h2&gt;
&lt;p&gt;Donec rutrum elit vitae sodales congue. Suspendisse sit amet dolor laoreet quam tempus rhoncus. Morbi viverra diam eu orci convallis, id sollicitudin justo blandit. Curabitur mattis dolor non ex rutrum, a auctor nisi malesuada.&lt;/p&gt;
&lt;h3 id=&quot;this-is-an-h3-tag&quot;&gt;This is an &amp;lt;h3&amp;gt; tag&lt;/h3&gt;
&lt;p&gt;Donec rutrum elit vitae sodales congue. Suspendisse sit amet dolor laoreet quam tempus rhoncus. Morbi viverra diam eu orci convallis, id sollicitudin justo blandit. Curabitur mattis dolor non ex rutrum, a auctor nisi malesuada.&lt;/p&gt;
&lt;h4 id=&quot;this-is-an-h4-tag&quot;&gt;This is an &amp;lt;h4&amp;gt; tag&lt;/h4&gt;
&lt;h5 id=&quot;this-is-an-h5-tag&quot;&gt;This is an &amp;lt;h5&amp;gt; tag&lt;/h5&gt;
&lt;h6 id=&quot;this-is-an-h6-tag&quot;&gt;This is an &amp;lt;h6&amp;gt; tag&lt;/h6&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;*Italic text*
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Italic text&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;**Bold text**
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Bold text&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~~Strikethrough text~~
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;del&gt;Strikethrough text&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Combine: ***bold + italic*** or **bold + *italic* + ~~strikethrough~~**
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Combine: &lt;strong&gt;&lt;em&gt;bold + italic&lt;/em&gt;&lt;/strong&gt; or &lt;strong&gt;bold + &lt;em&gt;italic&lt;/em&gt; + &lt;del&gt;strikethrough&lt;/del&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* Item 1
* Item 2
  * Item 2a
  * Item 2b
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Item 1&lt;/li&gt;
  &lt;li&gt;Item 2
    &lt;ul&gt;
      &lt;li&gt;Item 2a&lt;/li&gt;
      &lt;li&gt;Item 2b&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. Item 1
2. Item 2
   1. Item 3a
   2. Item 3b
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;Item 1&lt;/li&gt;
  &lt;li&gt;Item 2
    &lt;ol&gt;
      &lt;li&gt;Item 3a&lt;/li&gt;
      &lt;li&gt;Item 3b&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;![Alt Text](https://images.unsplash.com/photo-1535952548450-d7447587e733)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;![Alt Text](https://images.unsplash.com/photo-1535952548450-d7447587e733)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[This is a Link](http://buiquangbao.github.io/)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;http://buiquangbao.github.io/&quot;&gt;This is a Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; ‚ÄúYour time is limited, so don‚Äôt waste it living someone else‚Äôs life.‚Äù&amp;lt;br&amp;gt;
&amp;gt; Steve Jobs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;‚ÄúYour time is limited, so don‚Äôt waste it living someone else‚Äôs life.‚Äù&lt;br /&gt;
Steve Jobs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;There are 3 important files: `index.html`, `styles.css` and `scripts.js`
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;There are 3 important files: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.html&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;styles.css&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scripts.js&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;
&lt;pre&gt;
&lt;div class=&quot;highlight&quot;&gt;
```javascript
function plus(p1, p2) {
  return p1 + p2;
}
```
&lt;/div&gt;
&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;plus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;p1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;p2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;p1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;p2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;| First   | Second  | Third   |
| ------- | ------- | ------- |
| Content | Content | Content |
| Content | Content | Content |
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;First&lt;/th&gt;
      &lt;th&gt;Second&lt;/th&gt;
      &lt;th&gt;Third&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Content&lt;/td&gt;
      &lt;td&gt;Content&lt;/td&gt;
      &lt;td&gt;Content&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Content&lt;/td&gt;
      &lt;td&gt;Content&lt;/td&gt;
      &lt;td&gt;Content&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- [x] this is a complete item
- [x] this is a complete item
- [ ] this is an incomplete item
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;this is a complete item&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;this is a complete item&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;this is an incomplete item&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Bui Quang Bao</name></author><category term="technology" /><category term="web" /><category term="markdown" /><summary type="html">Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas id imperdiet odio. Etiam quis volutpat mauris. Duis ligula lacus, maximus vel est sed, molestie finibus nisl. Aliquam erat volutpat. Mauris sit amet pretium urna, sit amet tristique enim. In eget arcu mollis, ultricies metus venenatis, tincidunt enim.</summary></entry><entry><title type="html">Acting on Data Science</title><link href="/notebook/acting-on-ds" rel="alternate" type="text/html" title="Acting on Data Science" /><published>2021-01-01T00:00:09+07:00</published><updated>2021-01-01T00:00:09+07:00</updated><id>/notebook/acting-on-ds</id><content type="html" xml:base="/notebook/acting-on-ds">&lt;h2 id=&quot;1-interpretability&quot;&gt;1. Interpretability&lt;/h2&gt;

&lt;p&gt;You‚Äôve done all the work. You‚Äôve planned the project. You‚Äôve found data. You cleaned and organized the data. You created a model, you validated the model and you just want to put a bow on it and be done. Well, one thing that you need to consider before you wrap it all up, is who is going to make the decision? Who‚Äôs using the results and the insights that you got from your analysis? Because you have a couple of choices. One is, maybe you‚Äôre developing something that is for the benefit and use of algorithms. This is, for instance, a recommendation system which automatically puts something in front of people or a mortgage application system, which process it immediately while people are still on the website. In that case, the machines are the ones making the decisions and machines don‚Äôt need to understand what they‚Äôre working with. They have the data and if you set up the algorithm properly, they can just kind of run with it. Also, machines and algorithms can create complex models much more complex than a human can easily understand and implement them directly and immediately. And so if you‚Äôve done your analysis in such a way there is going to be finished working with by an algorithm, then you don‚Äôt need to worry too much about how interpretable it is, because the machines not spending time on that. On the other hand, if you have done your work for the benefit of humans, humans need to understand the principles involved. They need to know why things are happening the way that they are. So, they can then take that information, they can reason from the data, to apply it to new situations. It‚Äôs the principles that are going to be important and so you‚Äôre going to have to be able to explain that to them as a result of your work in Data Science. Now, the trick is, some results are easy to interpret. Here‚Äôs a decision tree I showed you earlier. It‚Äôs about classifying flowers as one of the three different species. You only have to make three decisions. It says, first look at the pedal length and if it‚Äôs long, then look at the pedal width and if that is short, then look at the pedal length again and if you do that, you can make a very good classification. This is a very simple system and human accessible, anybody can work with this. On the other hand, some other results are very difficult to interpret. This is another decision tree that I showed you earlier. It‚Äôs enormously complicated by regular human standards, you‚Äôd have kind of a hard time following through with this. And algorithms that are made in Data Science, like with Deep Learning, are infinitely more complex than this. And so you‚Äôre going to have a hard time explaining to somebody else, how this works and why it set up the way it is and what they can do with it. The point of all this, is it in your analysis, interpretability is critical. You‚Äôre telling a story and you need to be able to make sense of your findings, so you can make reasonable and justifiable recommendations. Tell a story that makes sense, is clear and compelling. Only then, can you see the value from your Data Science project.&lt;/p&gt;

&lt;h2 id=&quot;2-actionable-insights&quot;&gt;2. Actionable insights&lt;/h2&gt;

&lt;p&gt;If you‚Äôre working in a startup or really any entrepreneurial or organizational setting then you know that your work is all about getting results and that brings up something that I mentioned earlier in this course I want to mention again from one of my heroes, the American psychologist and philosopher William James who said ‚Äúmy thinking is first and last ‚Äúand always for the sake of my doing.‚Äù His point is that human cognition is designed to fulfill goals, to help us reach particular ends and I like to summarize that and apply it towards data science with this thought that data and data science is for doing. It exists to help us do things and the reason we do these projects is to help us accomplish things that are important to us. Remember, when you did the project, there was a goal, there was some motivation. What motivated the project? What sorts of things did you stick up on the wall? These are questions you wanted answered. Why was the project conducted? The goal is usually to direct some kind of particular action. Should we open a new store over here? Should we reduce the price over here? Should we partner up with this other organization over here and your analysis should be able to guide those actions and so can you remember what those clear questions were and can you give a clear well articulated and justifiable response to those questions based on your data science project? When you do, there‚Äôs a few things you want to keep in mind. You need to focus on things that are controllable. The analysis might say that companies founded in the 80s have greater success but if you‚Äôre founded in 2015, there‚Äôs not much you can do about it. So focus on something that is under your control and try to make it something that‚Äôs specific. Also, be practical. Think about the return on investment or ROI. You need to work on things and give actions where the impact will be large enough to justify the efforts. Also, if you‚Äôre giving recommendations to a client, make sure it‚Äôs something that they are actually capable of doing and then also you want to build up. You want to have sequential steps. You want to make a small recommendation, carry through on it and then build on it as you see the results of each earlier step. The data science project is designed to fulfill all of these requirements in a way that the benefits will be visible to the client as you help them find an answer to their question and if you can get that done then you‚Äôve done exactly what you meant to do when you started in data science and that is worthy of an office celebration so congratulations.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;Consider learning new things, like, for instance, how to program in Python or R, or how to work with open data, or how to build algorithms for machine learning and artificial intelligence. Any of these would be fantastically useful tools and approaches in data science. Also, learn how to apply the things that you‚Äôve worked with. Get some courses on data-driven decision making in business settings, get more information on business strategy and how the information you use can help any organization make better decisions in their own operations. And then get information on how people work, and the elements that are most important in fields like marketing or nonprofits or healthcare or education, or whatever is of greatest use and interest to you.&lt;/p&gt;</content><author><name>Learning Archive</name></author><category term="technology" /><category term="web" /><category term="markdown" /><category term="learning_archive" /><summary type="html">1. Interpretability</summary></entry><entry><title type="html">Analyses for Data Science</title><link href="/notebook/analyses-for-ds" rel="alternate" type="text/html" title="Analyses for Data Science" /><published>2021-01-01T00:00:08+07:00</published><updated>2021-01-01T00:00:08+07:00</updated><id>/notebook/analyses-for-ds</id><content type="html" xml:base="/notebook/analyses-for-ds">&lt;h2 id=&quot;1-descriptive-analyses&quot;&gt;1. Descriptive analyses&lt;/h2&gt;

&lt;p&gt;When it comes to business decisions, humans and machines approach things very differently. One element of this is that machines have essentially perfect memory. You can give it to ‚Äòem once and they‚Äôll probably give it back to you exactly the same way later. They are also able to see all of the data at once in detail at a way that humans can‚Äôt. On the other hand, they‚Äôre not very good at spotting general patterns in data. There‚Äôs some ways around that but it‚Äôs not one of the strong points of algorithms. Human decision-makers, on the other hand, are very good at finding patterns and connecting the data to outside situations. On the other hand, humans have limited cognitive bandwidth. We can only think of so many things at a time. One of the consequences of that is that we need to simplify the data. We need to narrow it down to a manageable level and try to find the signal in the noise. And so descriptive analyses are one way of doing this. It‚Äôs a little like cleaning up the mess in your data to find clarity in the meaning of what you have. And I like to think that there are three very general steps to descriptive statistics. Number one, visualize your data, make a graph and look at it. Number two, compute univariate descriptive statistics. There‚Äôs things like the mean. It‚Äôs an easy way of looking at one variable at a time. And then go on to measures of association, or the connection between the variables in your data. But before I move on, I do want to remind you of my goal in this course. I‚Äôm not trying to teach you all of the details of every procedure. Rather, I‚Äôm trying to give you a map, an overview of what‚Äôs involved in data science. We have excellent resources here at LinkedIn Learning and when you find something that looks like it‚Äôs going to be useful for you, I encourage you to go find some of the other resources that can give you the step-by-step detail you need. Right now, we‚Äôre trying to get a feel for what is possible and what sorts of things you can integrate. And so, with that in mind, let‚Äôs go back to the first step of descriptive analyses. And that‚Äôs to start by looking at your data. We‚Äôre visual animals and visual information is very dense in data. So you might try doing something as simple as a histogram. So this shows the distribution of scores in a quantitative variable. That‚Äôs also sometimes called a continuous variable. The bell curve, which is high in the middle, tapers off nicely into each side, doesn‚Äôt have any big outliers, is our common occurrence and it forms the basis of a lot of methods for analyzing data. On the other hand, if you‚Äôre working with something like financial data, you‚Äôre going to have a lot of positively-skewed distributions. Most of the numbers are at the low end and a very small number go very, very high up. Think of the valuations at companies, the cost of houses. That requires a different approach. But it‚Äôs easy to see it by looking what you have. Or maybe you have negative skew, where most of the people are at the high end and the trailing ones are at the low end. If you think of something like birth weight, that‚Äôs an example of this. Or maybe you have a U-shaped distribution where most of the people are either all the way at the right, all the way at the left, and although it‚Äôs possible for people to be in the middle there aren‚Äôt many. That‚Äôs a little bit like a polarizing movie and the reviews that it gets. But once you get some visualizations, you can look for one number that might be able to represent the entire collection. That‚Äôs a univariate descriptive. The most common of these is going to be the mode. If each box here represents one data point the mode is simply the most common. And that‚Äôs going to be right here on the left at one because there are more ones than there are of any other score. Or maybe you want the median, the score that splits the distribution into two equal-sized halves. We have six scores down here, we have six scores right here. So the median is 3.5. That splits the data set into two equal halves. Or you have the mean. This one actually has a formula, which means the sum of X divided by N. It also has a geometric expression. The mean is actually the balance point. If you put these as actual boxes on a seesaw the mean is where it‚Äôs going to balance. And in this case, it‚Äôs exactly at four. It‚Äôs going to rest flat at that point. And so these are very common procedures. I imagine you know them already but think of them as a good place to start when you‚Äôre looking at your data. And if you can choose a second number to describe your data, you should consider a measure of variability, which tells you how different the scores are from each other. So that can include things like the range, which is simply the distance between the highest and lowest score, the quartiles or IQR, which split the data up into 25% groups, the variance and the standard deviation, two very closely-related measures that are used in a lot of statistics, and you will also want to look at associations. And so for instance, this is a scatterplot that shows the association between the psychological characteristic of openness and agreeableness at a state-by-state level. You can look at some measures that give you a numerical description of association like the correlation coefficient or regression analysis, like I just barely showed you, or depending on your data, maybe an odds ratio or a risk ratio. But remember there‚Äôs a few things. The data that you‚Äôre analyzing must be representative of the larger group you‚Äôre trying to understand. And things like the level of measurement. Is it nominal, ordinal, interval, or ratio is going to have an impact on what measures you use and the kinds of inferences you can make. You always need to be attentive to the effect of outliers. You have one score that‚Äôs very different from all the others ‚Äòcause that‚Äôs going to throw off a lot of these measures. Also open-ended scores where you have like one, two, three, four, five, plus or undefined scores where somebody started something but didn‚Äôt finish can also have a dramatic effect on the data. So you want to screen your data for these things. Now, I can‚Äôt go into the detail of all of these things right here but we do have other courses here that can do that for you, such as Data Fluency, Exploring and Describing Data. Go there, go to the other courses available that give you an introduction to these basic concepts of understanding what‚Äôs going on in your data and describing the patterns that you can find so you can get started on the further exploration of your data science analyses.&lt;/p&gt;

&lt;h2 id=&quot;2-predictive-models&quot;&gt;2. Predictive models&lt;/h2&gt;

&lt;p&gt;Marriage is a beautiful thing where people come together and set out on a new path full of hope and possibilities. Then again, it‚Äôs been suggested that half of the marriages in the U.S. end up in divorce, which is a huge challenge for everyone involved. But 50 percent‚Äôs just a flip of a coin. If you were trying to predict whether a particular marriage would last or whether it would end in divorce, you could just predict that everybody would stay married or maybe everybody would get divorced and you‚Äôd be right 50% of the time without even trying. In a lot of fields, being right 50% of the time would be an astounding success. For example, maybe only 5% of companies that receive venture capital funding end up performing as projected and there‚Äôs billions of dollars at stake. If you could be right 50% of the time in your venture capital investments, you‚Äôd be on fire. And that brings up the obvious question: How can you tell which companies will succeed and which will fail? Not surprisingly, many methods have been proposed. Apparently, being too wordy in your emails is a sign of eminent business failure, but I think that‚Äôs anecdotal data and not a proper data science predictive analysis. But here‚Äôs the general approach for trying to use data to predict what‚Äôs going to happen. Find and use relevant past data. It doesn‚Äôt have to be really old. It can be data from yesterday. But you always have to use data in the past because that‚Äôs the only data you can get. And then you model the outcome using any of many possible choices. And then you take that model and you apply it to new data to see what‚Äôs going on in there. There‚Äôs actually a fourth critical step and it‚Äôs separate from applying, and that‚Äôs to validate your model by testing it against new data, often against data that‚Äôs been set aside for this very purpose. This is the step that‚Äôs often neglected in a lot of scientific research, but it‚Äôs nearly universal in predictive analytics and it‚Äôs a critical part of making sure that your model works well outside of the constraints of the data that you had available. Now there‚Äôs a number of areas where predictive analytics as a field has been especially useful. Things like predicting whether a particular person will develop an illness or whether they‚Äôll recover from an illness; whether a particular person is likely to pay off a mortgage or a loan, or whether an investment will pay off for you; and then even the more mundane things like building a recommendation engine to suggest other things that people can buy when they‚Äôre shopping online. All of these are hugely influential areas and major consumers of predictive analytics methods. Now, I do want to mention there are two different meanings of the word prediction when we talk about predictive analytics. One of them is trying to predict future events, and that‚Äôs using presently available data to predict something that will happen later in the future, or use past medical records to predict future health. And again, this is what we think of when we hear the word prediction. We think about trying to look into the future. On the other hand, it‚Äôs not even necessarily the most common use of that word in predictive analytics. The other possibly more common use is using prediction to refer to alternative events, that is, approximating how a human would perform the same task. So you‚Äôre going to have a machine do something like classifying photos and you want to see whether this is a person, whether this is a dog, whether this is a house, and you‚Äôre not trying to look into the future, but you‚Äôre trying to say if a person were to do this, what would they do, and we‚Äôre trying to accurately estimate what would happen in that case. And so, you also might try inferring what additional information might reveal. So we know 20 pieces of information about this medical case; well, from that, we might infer that they have this particular disease, but we wouldn‚Äôt know for sure until we do a direct test, so we‚Äôre trying to estimate what‚Äôs happening there. Now, when you go about your analysis, there‚Äôs a few general categories of methods for doing a predictive analytics project. Number one is classification methods. That includes things like k nearest neighbors and nearest centroid classification and also is connected to clustering methods such as k means. You can also use decision trees and random forests, which is several decision trees put together, as a way of tracking the most influential data in determining where a particular case is going to end up. And then also extremely powerful in data science are neural networks, a form of machine learning that has proven to be immensely adaptive and powerful, although very hard sometimes to follow exactly what‚Äôs going on in there. But all of these methods have been very useful within trying to predict what‚Äôs going to happen with a particular case. But I do want to mention one other approach that‚Äôs been enormously useful and dates back a lot further than a lot of these, and that‚Äôs regression analysis, which gives you an understandable equation to predict a single outcome based on multiple predictor variables. And it can be a very simple thing, like this is an equation that uses the amount of time a person spends on your website to predict their purchase volume. Now this is fictional data, but you get to see we have a scatter plot, we draw a regression line through it, and we even have an equation there at the top of the chart. And this is a regression equation written entirely symbolically. I showed this to you before. It‚Äôs where you‚Äôre trying to predict an outcome of y for individual i and you‚Äôre using several predictors, X1, X2, X3, and their regression coefficients to predict their score. So for instance, the example I used was predicting salary, and you can write it out this way, too, where the salary for individual i is going to be $50,000, that‚Äôs the intercept, plus $2,000 for each year of experience, plus $5,000 in each step of their negotiating ability on a one-to-five scale, plus $30,000 if they‚Äôre the founder or owner of the company. And so that‚Äôs a regression equation, and it‚Äôd be very useful for predicting something like salary. And it‚Äôs a very easy, conceptually easy way to analyze the data and make sense of the results. And there are a few nice things about regression models. Number one is they‚Äôre very flexible in the kind of data they can work with. Different versions of regression can work with predictors or outcomes that are quantitative or continuous, ordinal, dichotomous or polychotomous categorical variables. They also can create flexible models. They‚Äôre usually linear. They can also be curvilinear, they can be quantile based, they can be multi-leveled. You have a lot of choices. And generally, they‚Äôre easy to interpret, that‚Äôs compared to many other data science procedures. The results of regression analysis are easy to read, interpret, present, and even to put into action. But this is simply one choice among many for predictive analytics where you‚Äôre trying to use your data to estimate what‚Äôs going to happen in the future. We have a lot of resources here where you can find other courses that will give you specific instruction on predictive analytics methods and help you find the actionable next steps in your data.&lt;/p&gt;

&lt;h2 id=&quot;3-trend-analysis&quot;&gt;3. Trend analysis&lt;/h2&gt;

&lt;p&gt;When you go out hiking in a new place, it‚Äôs nice to have a path to help you get where you‚Äôre going. It‚Äôs also nice to know that the path might actually take you to some place you want to be, so how can you see where you‚Äôre going, and how long it‚Äôs going to take you to get there? Well, as a data scientist, your job is to figure out the path your data is on, so you can inform decisions about whether to stay on the current path, or whether changes need to be made. The most basic way to do this is with trend analysis, and it starts by plotting a line. Simply make a graph of the changes over time, and then connect the points to make a clear line of one kind or another. Now, when you‚Äôre doing the analysis, you have to be worried about something a little different from other analyses we may have looked at, and that‚Äôs something called autocorrelation, or self-correlation. The idea here is that every value is influenced by the previous values, more or less. So today‚Äôs value, say, for instance, on number of visitors to your site, is going to be associated with yesterday‚Äôs value, which is going to be associated with the day before, and what you‚Äôre looking for is consistency of change, and there are several different ways to think about the change that happens over time. In fact, what you‚Äôre trying to do is to find the function, so you‚Äôre trying to get the outcome variable, like number of visitors to a site, as a function of time and the previous value. So you‚Äôre trying to find the function, like a mathematical function, for that particular line, and it actually may be cyclical. It may go up and down over time, or it may be several functions combined all at once. Let me show you some of the most basic ones, and I‚Äôm just showing you the line, not the data that would determine the line. There would be points all around it, and maybe you have something that has perfect linear growth, where you add the exact same quantity at each time period, like dollars per hour, or per year for an employee. So maybe you‚Äôll have $100 for a certain amount of time, and if you work twice as much you get 200, and so on, but it always goes up the same amount, so it‚Äôs linear growth. It‚Äôs easy to work with. On the other hand, in a lot of situations, you actually have what‚Äôs called exponential growth, where the rate of acceleration, think about when you‚Äôre squaring something, this is x squared, or you grow by 2% at every time period, or maybe 10% per year, then the curve is going to look like this. It‚Äôs what you expect when you‚Äôre adding the same percentage at each time. Or maybe you have logarithmic growth, where it starts off rapidly, but it approaches a ceiling value, like operating at 100% capacity, if the initial growth is fast, and you can‚Äôt go past that, so it diminishes, the rate of change diminishes, even if you‚Äôre still going up over time. Sometimes you can have something slightly more complicated like a sigmoid, or a logistic function, where it starts slowly and then it accelerates rapidly, and then tapers off as limits are reached. Again, like reaching market saturation once your ad campaign has been massively successful. And then you may have a sinusoidal, or a cyclic sine wave, where things go up and down over time. Sometimes you have patterns that go over time. For instance, a lot of my work appeals to people who are in school, and so I see ups and downs that correspond with the fall and the spring semester, and with winter and summer breaks, and I get a pattern, approximately like this. Now, there are a few advanced options for trend analysis, like a change point analysis, where you‚Äôre looking for a substantial, perhaps qualitative change over time, like a flock that‚Äôs moving together, then perching, moving again, and so on, and you‚Äôre trying to look at those transitions. So change points are the changes in the resting state of the data. There may be fluctuations still going around it, and you will probably want to check for historical events that can explain those changes, things you know are going on in the environment. So for instance, here‚Äôs an example that I‚Äôve used before, and it shows the number of inventions registered from 1860 up through 1960. And a change point analysis, which I conducted using the programming language R, shows that there are these relatively stable periods, with some fluctuation around it, but you can see that it jumps up dramatically around the early 1880s, and then settles down for a long period of time, and then drops down again. These are the sorts of qualitative changes that you‚Äôll want to be able to explain as result of your analysis. You can also try breaking things down from the whole into their elements, to try to see what‚Äôs happening with your data over time. This is decomposition. Think of it like disassembling a clock or some other item. You‚Äôre going to take the trend over time and break it down into several separate elements. You‚Äôre going to look at the overall trend, you‚Äôre going to look at seasonal or a cyclical trend, and you‚Äôre going to have some leftover random noise that you haven‚Äôt modeled yet. So here‚Äôs a graph showing some stock prices over a period of time, from 1990 up to about 2017. And what we can find here, is if we do the decomposition analysis, which again, I did in R, this is the original trend. It‚Äôs a compressed version of what I just showed you, but when we decompose it into several elements, we get this smooth trend, and this is removing a lot of the day to day fluctuation. You can see it‚Äôs basically uphill. Then we have the seasonal variation. I set it for a one year repetition, and you see it going up and down over time. And then this is the left over random noise. But this is one way of looking at changes over time, and trying to get some of the meaning out of it by using one kind of analysis or another. All of these start by simply plotting the dots and connecting the line over time, and then there are different ways to explore what the meaning of that might be. And depending on the situation, the field that you‚Äôre working in, or exactly what you‚Äôre trying to get out of it, you‚Äôll use one approach or another, but any of these will be an excellent way of getting started on finding out what path you‚Äôre on, and what you can expect in the near future.&lt;/p&gt;

&lt;h2 id=&quot;4-clustering&quot;&gt;4. Clustering&lt;/h2&gt;

&lt;p&gt;Everybody in a crowd is their own person. Each person is a unique individual, and perhaps, in an ideal world, your organization would acknowledge that and interact with each person in a tailored and unique way. But for right now, we face a lot of limitations, and there are plenty of times when it‚Äôs helpful to create groups or clusters of people that might be similar in important ways. These can include marketing segments, where you might give the same ads or the same offers to a group of people. Or developing curricula for exceptional students, like gifted and talented or artistic students, or maybe developing treatments for similar medical groups. Now, when you look at clusters in the United States, it‚Äôs easy to start with each state represented separately, but it‚Äôs really common practice to group these states into, say, four large regions of geographically adjacent states, like the South, the West, the North East and the Midwest. And that makes a lot of sense if you‚Äôre actually having to travel around from one to another, but you don‚Äôt have to group by just what‚Äôs physically next to each other. For example, a soccer team has matching jerseys, and they coordinate their movement, ‚Äòcause they‚Äôre a team. These could serve as the basis of maybe a behavioral cluster as opposed to a geographic one. And you can use a lot of different measures for assessing similarity, not just physical location. You can look at things like a K-dimensional space. So you locate each data point, each observation, in a multidimensional space with K-dimensions for K variables. So if you have five dimensions, K is five. If you have 500, then you have 500 dimensions. What you need to do then, is you need to find a way to measure the distance between each point, and you‚Äôre going to do one point, every other point, and you‚Äôre looking for clumps and gaps. You can measure distance in a lot of ways. You can use Euclidean distance, that‚Äôs the standard straight line between points in a multidimensional space. You can use things like Manhattan distance, and Jaccard distance, cosine distance, edit distance, there‚Äôs a lot of choices in how you measure the distance, or the similarity, between points in your data set. Let me give you an example, though, of cluster analysis in real life. So for instance, one of my favorite studies is based on what‚Äôs called the Big 5. I have a background in social and personality psychology. The Big 5 is a group of five very common personality factors that show up in a lot of different places under a lot of different situations. The actual factors are extraversion versus introversion, agreeableness, conscientiousness, neuroticism, which means your emotions change, and as opposed to stability, and then openness, specifically openness to new experiences. Once study I know actually tried to group the states in the U.S. using these Big 5 personality factors. They got information from social media posts, and then evaluated each state and created a profile. And from that, they found that the states in the U.S. went into three very broad groups. The big group there in orange down the middle, they called the friendly and conventional. The yellow off to the west coast, and actually a little bit off on the east coast, they called relaxed and creative, and then in green, which is mostly the north east, but also Texas, is temperamental and uninhibited, and these are different ways of thinking about the kinds of things that people think about and the way that they behave. Now, you could use psychological characteristics, or maybe you could group states by, for instance, how they search for things online, which might be more relevant if you‚Äôre doing e-commerce. So, I went to Google Correlate, and I chose a dozen search terms that I thought might roughly correspond to the Big 5 personality factors, and what that data tells you is the relative popularity of each search term on a state-by-state basis. I then did an analysis in R, doing what‚Äôs called a hierarchical cluster, where all of the states start together, and then it splits them apart one step at a time. And you can see, for instance, that my state of Utah is kind of unusual by itself over here, but you can see the degree of connection between each of the states, until finally all of the 48, it doesn‚Äôt include Alaska or Hawaii, because the original researchers didn‚Äôt have that in their personality data, but I could say, ‚ÄúGive me two groups,‚Äù and then it groups them this way. You can see we have just these five states over here listed on the right, or we could say, ‚ÄúGive us three groups,‚Äù in which case all it does is it separates Utah from those other five. But you can go down to a level that seems to work well, something that makes sense and that works with your organization‚Äôs needs. Now, I want you to know that when you‚Äôre doing a cluster analysis, you could do a hierarchical clustering, which I just did, that‚Äôs a very common approach, but there‚Äôs a lot of alternatives. You can do something called K-means, or a group centroid model. You can use density models or distribution models, or a linkage clustering model. Again, we have other resources here that will show you the details on each of these and how to carry them out. Mostly I want you to be aware that these alternatives exist, and they can be useful in different situations for putting your data together. Remember, with any analysis, something like clustering exists to help you decide how you want to do things. So use your own experience, and use common sense as you interpret and implement the results of your analysis, and you will get more value and more direction out of that for your own organization.&lt;/p&gt;

&lt;h2 id=&quot;5-classifying&quot;&gt;5. Classifying&lt;/h2&gt;

&lt;p&gt;So maybe you‚Äôve got a dog and maybe your dog does cute things like sticking its nose in your camera. Now, my dog‚Äôs too short for that, but you take a picture or a video to save the moment. But one interesting consequence of that process is that now your phone‚Äôs photo program is going to start analyzing the photo to determine what it‚Äôs a photo of. That way, you can search for it later by typing dog without ever having had to tell the program that‚Äôs what it is. And that‚Äôs the result of a machine learning algorithm taking the data to analyze the photos and classify it as a dog, a cat, a child, and add those labels to the data. In fact, classifying is one of the most important tasks that data science algorithms perform, and they do it on all kinds of data. The general idea of automated classification is pretty simple to describe. Locate the case in a k-dimensional space where k is the number of variables or different kinds of information that you have. And there‚Äôs probably going to be more than three. It might be hundreds or thousands. But once you get it located in that space, compare the labels on nearby data, that of course assuming that other data already has labels that it says whether it‚Äôs a photo of a cat, or a dog, or a building. And then once you‚Äôve done that, assign the new case to the same category as the nearby data. So in principle it‚Äôs a pretty simple process. Now in terms of what data you‚Äôre going to assign it to, you can do that using one of two different methods among other choices. A very common one is called k-means, and this is where you choose the number of categories that you want. You can actually say I only want two, or I want five, or I want 100. And then what the algorithm does is it creates centroids. That‚Äôs like a mean in multi-dimensional space, and it will create as many centroids as you want groups. And so when you put your new data in, it will assign that new case to the closest of those k centroids. Again, might be two, might be five, might be 100. Another approach is called k-nearest neighbors, and what it does in this case is it finds where your data is in the multi-dimensional space, it looks at the closest cases next to it, and you can pick how many you want. It might be the five closes, the 20 closest, the 100 closest. And look at the categories of those cases and assign your new data to the most common category among them. Now as you might guess, classification is a huge topic in data science, machine learning, and artificial intelligence, and so there are many, many options on how to do this process, and you‚Äôre going to have to spend a little time talking with your team to decide which approach is going to best meet your individual goals. Now some of the things you‚Äôre going to have to consider are things like whether you want to make a binary classification, that‚Äôs just a yes, no, like whether a credit card transaction is or is not fraudulent, or whether you have many possible categories like what‚Äôs in a photo or what kind of movie to recommend to someone. You also have a lot of choices for how you measure the distance, how close is it to something else. You can use euclidean distance, Manhattan distance, edit distance, and so on. And you also need to decide whether you‚Äôre going to compare it to one central point like a centroid or several nearby points. And then you also need to make a decision about confidence level, especially when you have a significant classification, how certain do you have to be that that‚Äôs the right one. Some cases fit beautifully, others are much harder to classify. Now, once you‚Äôve done the classification, you want to evaluate your performance, and there‚Äôs a few different ways to do that. You can look at the total accuracy. So if you have like a binary classification, is a legitimate transaction, is a fraudulent transaction, what percentage of the total cases got put into the right category. This is simple to calculate and it‚Äôs intuitive to understand, but it‚Äôs problematic because if one category is much more common than the others, then you can get high overall accuracy without even having a functional model. So you want to start looking at things a little more particularly like for instance sensitivity. This is the true positive rate or if a person‚Äôs supposed to be in a particular category, what‚Äôs the likelihood that that will actually happen. So if a person has a disease, what‚Äôs the probability they will be correctly diagnosed with that disease? And there‚Äôs also specificity, which is like the true negative rate, and what this means is that the case should only be categorized when it is supposed to go in there. You don‚Äôt want these other cases accidentally going in. And that‚Äôs one of the purposes of Bayes‚Äô Theorem, which I‚Äôve talked about elsewhere. Bayes‚Äô Theorem allows you to combine data about sensitivity, specificity, and the base rates, how common the thing is overall. And again, my goal here is not to show you the step-by-step details, but to give you an overall map. For more information on classification methods, we have a wide variety of courses uses languages like Python and R that can walk you through the entire process. But the goal is the same, using automated methods to help you identify what you have and by placing it into relevant categories, helping you get more context and more value out of the data so you can provide better products and services.&lt;/p&gt;

&lt;h2 id=&quot;6-anomaly-detection&quot;&gt;6. Anomaly detection&lt;/h2&gt;

&lt;p&gt;Some time ago, I opened up Launchpad on my Mac, which is a way of launching applications, and it‚Äôs supposed to look like this. However, this particular time, something weird happened, and this is what I saw instead. Now, normally, when you get an anomaly like this, you just restart the app or reboot the computer, but it turns out I‚Äôm fascinated by generative art or art that comes through as the result of an algorithm, often with a fair amount of randomness thrown in. So before I restarted things and got it back to normal, I took a screenshot, and I‚Äôve gone back to it several times. I consider this an excellent example of found generative art, really, a happy digital glitch or a fortuitous anomaly. It‚Äôs also an example of serendipity or the unexpected insight that can come along. Well-known examples of serendipity include Silly Putty, Velcro, Popsicles, and of course, the Post-It Notes that every office has. You can think about these as trying to find anomalies, unusual things, and latching onto them. Now, usually when we talk about anomalies, we talk about things like fraud detection. Is a particular transaction legitimate or fraudulent? You can also use it to detect imminent process failure like a machine‚Äôs going to break or an employee has a heightened risk of burnout or leaving the company, but you can also think of this as a way of identifying cases with potentially untapped value, a new category, a new audience that you can work with. Now, what all of these have in common are the focus on outliers. These are cases that are distant from the others in a multidimensional space. They also can be cases that don‚Äôt follow an expected pattern or a trend over time, or in the case of fraud, they may be cases that match known anomalies or other fraudulent cases. Any of these can be ways of identifying these anomalies and responding appropriately to them, and when you do that, it brings up the usual suspects, the usual method for analyzing data in data science, things like regression. Does this particular observation fit well with the prediction, or is there a large error? You can do Bayesian analysis to get a posterior probability that this is a fraudulent transaction. You can do hierarchical clustering or even do neural networks as a way of finding how well the data fits these known patterns, and if it doesn‚Äôt, you may have an anomaly. Now, there are a couple of things that make this a little harder than it might be otherwise. Number one is that we are dealing with rare events. By definition, if it‚Äôs an anomaly, it‚Äôs not common. So things like fraud are uncommon, and that leads to what are called unbalanced models. When you‚Äôre trying to predict something that happens only 1% or 1/10 of a percent of the time, you got to have a huge amount of data, and you have to have a model that can deal well with that kind of categorical imbalance. The second thing is difficult data. You may not be dealing just with a nice SQL database. You may have biometrics data. You may have multimedia data. You may have things like time-sensitive signatures where you have to measure how it happened over an event. So as an example of all of this, think about when you‚Äôve used your credit card to make a purchase online. You, the online store, and your credit card company all have a vested interest in making sure the transaction is legitimate because fraud costs money, it takes times, and it causes headaches. So your credit card company could take several steps to identify legitimate cases and potential anomalies. They might look at something like the purchase characteristics. What was purchased? For how much? When and where? Through what means, and so on. I got a call a few years ago from my credit card company when someone tried to spend several thousand dollars on a hotel several thousand miles from my home. You can also use personal measures, things like biometrics or latency and typing, or you can measure a person‚Äôs height approximately by the angle at which they hold their cell phone, the mode of transportation they‚Äôre on by getting the vibrations through the accelerometer, or an interesting one is a signature of their name or a person trying to find the cursor on their computer is, in fact, a signature that can be measured and stored, and new data can be compared against that, and then, there are general trends. Are there new scams going around? Are they more common in one area or another? Have scammers found ways around old technology and so on. Needless to say, fraud detection‚Äôs a cat-and-mouse game, so there‚Äôs constant research and progress in data science to deal with new methods of fraud and harnessing the evolving capabilities of machines and algorithms, and that means that you‚Äôll want to stay in touch with the resources available here to learn the specific details about the latest and greatest methods in data science for things like fraud detection and any kind of anomaly detection, including the potential for new discoveries via serendipity.&lt;/p&gt;

&lt;h2 id=&quot;7-dimensionality-reduction&quot;&gt;7. Dimensionality reduction&lt;/h2&gt;

&lt;p&gt;Back in 1957 in his legendary song, Rock ‚Äòn Roll Music, Chuck Berry sang a lament about musicians who make things too complicated and to quote, ‚Äúchange the beauty of the melody ‚Äúuntil it sounds just like a symphony.‚Äù And that‚Äôs why he loves rock ‚Äòn roll music. The same idea explains why most bands have four people like the Beatles right here or possibly three or five. That‚Äôs enough people, enough instruments to fill all the sonic regions without overwhelming with too much information and resulting in cacophony. Jumping way ahead in time, there‚Äôs a similar problem in data science. We think of data coming down in a matrix-like stream here as really cool, but it‚Äôs hard to get meaning out of it and it‚Äôs hard to know what to do as a result of it. We need a way to get through the confusion and the haze and pull things into focus. Fortunately, there‚Äôs a way to do that in data science. The idea of dimension reduction is to actually reduce the number of variables and the amount of data that you‚Äôre dealing with. So instead of dealing with dozens or hundreds or maybe even thousands of variables, you‚Äôre dealing with a single score like how likely a person is to behave in a particular way. It sounds counterintuitive, but there are actually some very good reasons for doing this. First off, each variable, each factor or feature has error associated with it. It doesn‚Äôt measure exactly what you want. It brings in some other stuff. But when you have many variables or features together that you combine, the errors tend to cancel out. So if they‚Äôre all pointing in slightly different directions, you end up centered on what it is you want. Also, by going from many individual measurements to a single conglomerate measurement, that reduces the effect of something called colinearity, which is the association, the overlap between predictor variables in the model, which creates some significant problems. So if you have fewer variables, there‚Äôs less problems for colinearity. Also, not surprisingly when you have a few features you‚Äôre dealing with instead of hundreds, you are able to do things faster. Your computer is able to process the information with greater speed. And another really nice consequence of this is it improves generalizability. Again, because you‚Äôre getting rid of or averaging out the idiosyncratic variation with each observation, with each variable, and you‚Äôre going to get something much more stable that you‚Äôre able to apply to new situations better. Now, there are two general ways to do this. There are a lot more options, but the two most common are these. Number one is principal component analysis, often just called principal components or PCA. And the idea here is that you take your multiple correlated variables and you combine them into a single component score. So let‚Äôs say you give a personality questionnaire and it‚Äôs got 50 questions on it, but you have 10 questions for each element of personality, then you can combine those into 10 components if the analysis supports that combination. And then you only have five things to deal with as opposed to 50 and it‚Äôs much easier to deal with. Another very common approach is factor analysis. And functionally, it works exactly the same way. People use it for the same thing, although the philosophy behind factor analysis is very different. Here your goal is to find the underlying common factor that gives rise to multiple indicators. So in principal component analysis, the variables come first and the component results from it. In factor analysis, this hidden factor comes first and gives rise to the individual variables. That said, even though they are conceptually very different that way, people tend to use the two often interchangeably. What they let you do is group variables in ways that make sense. Now, there are a lot of variations on methods for dimension reduction. You, for instance, might be engaged in an exploratory analysis where you‚Äôre just trying to find out what‚Äôs there in the data in front of you or a confirmatory analysis where you have a known structure and you‚Äôre trying to see how well your current data fit. You have different methods for different levels of measurement. If you have a quantitative thing where you‚Äôre looking at how it takes somebody to do something or the value of their purchases, that‚Äôs one approach. But if you‚Äôre counting yes or no are they in this particular category, you‚Äôre going to need to do something else. Also, there are multiple algorithms, many different ways of measuring the similarity between variables and measuring ways of overlap and the degree that they share. And so there are some very important details in these, but we can save that for a more detailed analysis in another video. Right now I want you to know that the procedure exists. Right now I want you to know that there‚Äôs the possibility of doing this and that it‚Äôs worth looking into. I mean, think about it. Dimension reduction in data is like learning to read a language. At first you just see random shapes. Then you see individual characters, then words, then sentences, and then finally ideas. You can go from 100 pieces of information, a line here or a circle there to just a handful and that‚Äôs what you need to get meaning out of your data and to do something useful with it.&lt;/p&gt;

&lt;h2 id=&quot;8-feature-selection-and-creation&quot;&gt;8. Feature selection and creation&lt;/h2&gt;

&lt;p&gt;I teach statistics to undergraduate students who don‚Äôt always see how it connects to their lives. I can give specific examples about each of the fields, but I found that even the most recalcitrant student can get excited about data when we talk about sports like baseball. Baseball‚Äôs a data-friendly sport. It‚Äôs been going on for over 100 years, there are 162 games in the regular season, and they count everything. If you‚Äôre trying to figure out how good, for example, a particular batter is, you can start with these basic bits of data and you‚Äôll have an enormous amount of information to work with. These are the features in the dataset that you start with. But if you‚Äôre a coach or a manager, you can do a lot more than just use those raw data points to make a strategy. You can start combining them to create new features in your dataset and finding value and possibilities in your team. Now you can start with really simple ones. This is the batting average, and all it is is the number of hits divided by the number of at bats, and you need to know that those are defined in particular ways, but it‚Äôs just one number divided by the other. Or you can get something more sophisticated like the on-base percentage, where you take three things, the number of hits, the number of bases on balls and hit by pitch and divide that by four things. At bats, bases on balls, hit by a pitch, and sacrificed flies. That gives you a better measure, according to some judgment, or if you want to jump ahead to the 21st century, you can start getting really fancy with something like the weighted runs created plus where you have a whole plethora of things you‚Äôre putting together and what‚Äôs interesting is every one of these is actually its own formula going into it, so that one‚Äôs complicated, but it‚Äôs all based on these little bits and pieces of information that are available. Before I go ahead, I want to mention one thing and that‚Äôs feature selection and creation is a different process than the dimension reduction that I mentioned elsewhere. Dimension reduction‚Äôs often used as a part of getting the data ready so you can then start looking at which features to include in the models you‚Äôre creating and that‚Äôs what we‚Äôre addressing right now. So given that you have these formulas to create all these new features to pick the best players or best outcomes, which one should you actually use when you‚Äôre making that decision? Which ones have the greatest decision value? Well, if you‚Äôve seen the movie Moneyball, which is a dramatized account of how the Oakland A‚Äôs general manager Billy Beane used data to select and assign players, you will remember he keeps directing the scouts towards one primary factor over any other. Whether a player reliably gets on base. He had data to drive that decision and to guide him to that focus, although they didn‚Äôt share that process with us in the movie, but I can tell you basically how it works outside of baseball. There are a few methods that you can use for feature selection and feature creation in your data science projects. So for instance, you can start with just basic correlation. Is this variable correlated with the outcome or is this variable correlated? Which one has a bigger correlation? That works, but it‚Äôs one variable at a time and ‚Äòcause correlation generally looks at linear associations, it has some limits. Or you could do something called stepwise regression where you take all of your potential predictor variables, you put them in the computer, and you say this is the outcome variable and it looks at correlations and picks the best one and then it starts doing what are called partial correlations, and it‚Äôs a really easy way to sift through the data. You know, you just hit go and it‚Äôs done. The problem, however, is that stepwise regression really capitalizes on chance fluctuation in your dataset, and you can set stuff that simply is not going to generalize to anything else, and so stepwise is generally a poor choice, even if it‚Äôs an easy one. On the other hand, more modern methods like lasso regression, that‚Äôs least absolute shrinkage and selection operator, and ridge regression are better ways that are most robust to these flukes of chance variation, and they give a better impression of the variables‚Äô role in the equation and which ones you should emphasize. And if you do something like a neural network, you can do variable importance and there‚Äôs a lot of other different ways of evaluating each one of these, but when you‚Äôre selecting your variables, there‚Äôs a few things you want to keep in mind. Number one, is it something that you can control? Ideally, if you‚Äôre trying to bring about a particular outcome, you can to have the ability to make it happen. So look at variables that are under your control or that you can select, at least, and then look at the ROI, the return on investment. Not everything that can be manipulated or controlled can be controlled easily or inexpensively, and os you need to look at the combined cost and the value or the benefit that you get from working with that particular predictor. And then the third one is is it sensible? Does the model make sense? Does it make sense to include this particular variable in your equation? You‚Äôve got experience. You know your domain. Always keep that in mind as you‚Äôre evaluating the information that you can use in your model to make predictions. That taken together will let you make an informed choice about the best things in your data for predicting and ideally, for bringing about the things that matter to you and to your organization.&lt;/p&gt;

&lt;h2 id=&quot;9-validating-models&quot;&gt;9. Validating models&lt;/h2&gt;

&lt;p&gt;Several years ago, my wife and I adopted our second child. No, this isn‚Äôt her, this is stunt double. We‚Äôve loved and we‚Äôve raised her the best we could but we did make one critical error that has actually caused her unanticipated grief and could take her years to recover from. We used a non-standard character in her name. Those two dots over the E are a diaeresis, which indicates that the second vowel is to be pronounced as its own syllable. And, by the way, that‚Äôs not to be confused with the identical looking, but functionally distinct umlaut which softens the sound of a vowel or indicates that you‚Äôre a heavy metal band. It turns out, there is still a lot of web forms out there that don‚Äôt like non ASCII characters and will tell you that you‚Äôve entered invalid data even if it is the name that your parents gave you. And there are other people whose names have trouble with modern computers, aside from having things like apostrophes or hyphens. So maybe you‚Äôve got a mononym, just a single name. It‚Äôs common among pop singers and it‚Äôs also not unheard of in Indonesia and Yanmar. Or even have as single letter for a name. O happens in Korea, although it‚Äôs often spelled as OH and E occurs occasionally in China. Or, you are Hawaiian and you‚Äôve got a really expansive last name or the most challenging of all are the handful of people whose last name is Null, which wreaks all sorts of havoc with databases and leads to either hilarity or immense frustration trying to get things done online. Now, what‚Äôs happening here, I imagine, is that a well meaning programmer has created a name validation system that checks whether a person has entered an actual name. And they‚Äôve checked how well that worked against a database of names that they had that unfortunately wasn‚Äôt representative of all the possibilities. Remember, the world is a big place, and apparently programmers don‚Äôt name their kids Zoe so problems come up and the system breaks down. What that lets you know is that you should always check your work. That‚Äôs the principle of validating your models in data science. You may have tailored it really nicely to the data that you had, but is it going to work with anything else? Is it going to survive out there? It reminds me of one of the all time great quotes from computer science, beware of bugs in the above code, I have only proved it correct, not tried it. And that‚Äôs from Donald Knuth, an illustrious computer science professor and author of the Multi-volume, The Art of Computer Programming. So, you need to validate your data. You need to make sure that your model works. The basic principle is pretty easy, even if people outside of data science don‚Äôt do it very often. Take your data and split it up into two groups. Training data and testing data. And training data‚Äôs like the data that you send to class and you teach it how to identify the outcome that you‚Äôre looking at and you build a model with that training data. But then you need to check how well you‚Äôve taught it and you go into the testing data, now there‚Äôs two kinds of testing data, one of them doesn‚Äôt really count and that‚Äôs cross-validation. Now you can say it‚Äôs testing data but it‚Äôs actually using the training data. What you do here is you take the training data and you split it up into several pieces, maybe six different groups and then you use five at a time to build a model and then you use the sixth group to test it and then you rotate through a different set of five and you verify them against a different one sixth of the data and so on an so forth. So that‚Äôs internal, but it still allows you to build models that are going to be a little more robust against variations in the data. But the gold standard is holdout testing data, or holdout validation. This is where you take that maybe 20% of data that you set aside way at the beginning and you‚Äôve never looked at it, you‚Äôve never touched it, now you take the model you‚Äôve built from your training data and ideally you have gone through the cross validation process also, you take that model and you apply it just once to the holdout data and that one is going to give you the true measure of the accuracy of your model. Not, when you‚Äôre able to twist it and tweak it however you wanted but testing it against data sort of in the wild and the general idea here is now you have an idea of how robust your model is and how well it can function outside the box. Once you do that, you‚Äôre going to be certain that your work does what you think it should do and you can be confident that your model is ready to go out there into the real world.&lt;/p&gt;

&lt;h2 id=&quot;10-aggregating-models&quot;&gt;10. Aggregating models&lt;/h2&gt;

&lt;p&gt;There‚Äôs a saying that life imitates art. Well, for some time as I‚Äôve been preparing for this presentation I‚Äôve planned to talk about how people estimate the amount of money in a jar full of coins. And then literally today we inherited a giant jar of coins from my mother-in-law and we‚Äôre currently asking extended family members for their guesses as to how much money is in this very heavy jar. I‚Äôve guessed a low-ball estimate of 165. My wife was more optimistic and guessed $642.50. We actually won‚Äôt know the answer until next week when we cash in all the change, but I bring this up because it illustrates an important point. Any one guess, like an arrow shot at a target, maybe high, maybe low, maybe more or less accurate, but as it happens, if you take many guesses and average them, the errors tend to cancel out, and you end up with a composite estimate that‚Äôs generally closer to the true value than any one single guess is. This is sometimes called the wisdom of the crowd, although in statistics it‚Äôs a function of something called the central limit theorem and the way that sampling distributions behave. But the idea is that combining information is nearly always better than individual bits and pieces of information. In data science, the wisdom of the crowd comes in the form of combining the results of several different models looking at the same outcome and the same data set. Or you maybe use models like linear regression, and a lasso regression, and a decision tree, and a neural network, all predicting the same outcomes. And there‚Äôs several different ways that you can combine the estimates from each of your models to create an ensemble estimate. If you‚Äôre modeling a categorical or nominal outcome like trying to decide what‚Äôs in a picture or whether a new contact is likely to make a purchase, each model will give its prediction and then you can take the most common category across the predictions. It‚Äôs like having people raise their hands to vote. Or, if you‚Äôre modeling a quantitative or a continuous outcome like the number of people who will respond to a nonprofit‚Äôs fundraising appeal, you can take the numbers that each model predicts and average them, sort of like mixing ingredients in a bowl. Or maybe you want to try something a little more sophisticated and use Bayesian methods to combine the posterior probabilities that each model gives you into a single aggregated or ensemble probability. Now, these are just simple methods and ways of thinking about it. There‚Äôs a lot of work in the field of ensemble modeling, or models that combine the results of multiple models but this gives you a general idea. Now, let me explain very quickly what the benefits of this are, ‚Äòcause it does take a little extra time and effort. First off you get multiple perspectives on your data and on your outcome. Each method that you use, each algorithm, has its own strengths but it also things that it tends to skip over or get tripped up by. By combining the results you can compensate for some of those weaknesses and capitalize on those combined strengths. You also have a greater ability to find the signal, meaning the true value, the true outcome amid the noise of random variation and measurement error. You tend to get estimates that are both more stable across time and across methods and that are more generalizable to new cases and new situations. All of those are enormous benefits. You can think of it as a kind of cooperation between the models, the ensemble modeling. It‚Äôs the idea that many eyes on the same problem can lead to the best possible solution and that really is why you got involved in data science in the first place.&lt;/p&gt;</content><author><name>Learning Archive</name></author><category term="technology" /><category term="web" /><category term="markdown" /><category term="learning_archive" /><summary type="html">1. Descriptive analyses</summary></entry><entry><title type="html">Mathematics for Data Science</title><link href="/notebook/mathematics-for-ds" rel="alternate" type="text/html" title="Mathematics for Data Science" /><published>2021-01-01T00:00:07+07:00</published><updated>2021-01-01T00:00:07+07:00</updated><id>/notebook/mathematics-for-ds</id><content type="html" xml:base="/notebook/mathematics-for-ds">&lt;h2 id=&quot;1-algebra&quot;&gt;1. Algebra&lt;/h2&gt;

&lt;p&gt;When you‚Äôre working with data, it‚Äôs not too hard to come up with a solution when you have only one problem at a time, and it‚Äôs basically stationary. But it‚Äôs a whole different story when you have thousands of problems stampeding at you at the same time. In that case, you need a more flexible approach, and this is where algebra can come to the rescue. There are two reasons that it‚Äôs important to understand algebra and data science. Number one is that it allows you to scale up. The solution you create to a problem should deal efficiently with many instances at once. Basically create it once, run it many times. And the other one closely related to that is the ability to generalize. Your solution should not apply to just a few specific cases with what‚Äôs called Magic Numbers, but to cases that vary in a wide range of arbitrary ways, so you want to prepare for as many contingencies as possible. And so, we‚Äôll start with the basic building blocks of data science, which is Elementary Algebra. An algebraic equation looks like this. This, in fact, is a linear regression equation, but what we‚Äôre doing is reusing letters to stand in for numbers. That actually is an interesting thing because a lot of people think that people who work in mathematics and work in data, work with numbers all the time, we actually work with variables, and so, this is the association between variables. So, let‚Äôs start right here. Here on the far left, Y is the outcome, and the subscript i means it‚Äôs for the case i, person or observation i. That could be one or two or 1,000 or a million. Right next to that is a Greek letter, that‚Äôs a lowercase beta, like a b, and it has a zero because it is the Y intercept. It‚Äôs sort of the starting value before we add anything else. Next to that is another beta, but this time with a sub one, this is a regression coefficient, and it‚Äôs the slope that we use for this variable X one, so that‚Äôs going to be our first predictor variable, and we‚Äôre going to multiply it for the value of case i. Then we do similar things with the second regression coefficient, and the second predictor variable, then the third regression coefficient, and the third predictor variable. And here at the end, we have an epsilon, and this stands for error, and it means, how far off is our prediction from the actual values for each person, one at a time? And as wonderful as a regression equation like that is, the power of computing comes in when we go past a single dimension to the rows and columns of a matrix. That‚Äôs how your computer likes to see math, and how it processes it. This is what‚Äôs known as Linear Algebra, it works with matrices and vectors. Over here on the far left is a vector that has all of the outcome scores for each case. In this situation, there‚Äôs only two, there‚Äôs Y sub one for the first person, and Y sub two for the second. If there were 100 people, we would have 100 numbers all arranged vertically, if there were a million, we‚Äôd have a million numbers arranged all vertically. Those are the outcomes, and they‚Äôre in a vector. Right next to this is all the scores, this is a matrix because it has both rows and columns, and it contains the data for each individual person. Next to that is another vector, which has the regression coefficients written again with the beta, and we have the intercept at the top, and then, each of the three slopes. And then, we finish with a vector of error terms for each individual person, there‚Äôs only two in this case, but there could be 1,000 or a million. Let me fill this in with some numbers so you can see how it works in practice. If we‚Äôre going to estimate the salary for a person working in data science, and this is actually based loosely on real data from a few years ago, let‚Äôs say we have two people, and the first one has a known salary of 137,000, the other one has a salary of 80,000, and what we‚Äôre going to do is see how we can run that through this matrix equation, this linear algebra, to estimate those numbers. The first thing here is we have their data in the matrix on the left and the regression coefficients on the right. This one is the intercept, everybody gets a one, because everybody‚Äôs number is multiplied times 50,000, that‚Äôs the starting value that everybody gets. Next to that is a number that, in this case, years of experience. So, this person has nine years, and for each year, we estimate an additional $2,000 in salary. Next to that is a number that indicates negotiating ability on a one to five scale, where one is very low, very poor negotiator, five is a very strong negotiator. And for each step up, we predict a $5,000 increase in annual salary. This person has a three, they‚Äôre middling, so we would add $15,000 onto their expected salary. This last one is an indicator variable, it‚Äôs a zero/one, zero is no, one is yes, to say whether this person is a founder or an owner of the company. If they are, then we would expect them to make about $30,000 more per year, that‚Äôs reasonable. And when you put all of these things together, we predict 113,000 for this first person. Now, they actually had 137,000, so we‚Äôre off by a little bit, and that‚Äôs the error equation. And that doesn‚Äôt necessarily mean we messed up, we only have three variables in the equation, we didn‚Äôt put down where they live, we didn‚Äôt put down what sector they‚Äôre working in, and we didn‚Äôt put down what kind of client projects they have, all of those things would influence it as well. But this shows you how you can use these vectors and matrices to do the algebra for an important question, like estimating salary. Now, one of the neat things about matrix notation is that it‚Äôs like packing a suitcase, you can go from lots of different individual things into a much more compact and maneuverable and agile package. This is matrix notation, and what it is here is this is the same information that I showed you just a moment ago, except now, you see how it‚Äôs in bold, each of these symbols now stands for the entire vector or matrix. This Y on the side stands for every single outcome score for every person in our data. This X is the matrix of all of the predictor data, and this bolded beta is the vector of all of the regression coefficients. And this over here, the epsilon, is the vector of all error terms. So, it‚Äôs very compact, and this is the way that computers like to deal with the information, and it makes it much easier for them to manipulate it and get the things that you‚Äôre looking for. Now, even though you‚Äôre going to be using the computer to do this, there‚Äôs a couple of reasons you want to be aware of how the algebra functions. Number one is it allows you to choose procedures well. You can know which algorithms will work best with the data that you currently have to answer the questions that motivated your project in the first place. And the second one is it can hep you resolve problems. Things don‚Äôt always go as planned, and you‚Äôll know what to do when things don‚Äôt go as expected, so you can respond thoughtfully, and get the insight and the actionable steps you need out of your data.&lt;/p&gt;

&lt;h2 id=&quot;2-calculus&quot;&gt;2. Calculus&lt;/h2&gt;

&lt;p&gt;You may have the best product or service in the world, but if you want to get paid, you‚Äôve got to make the sale and you got to do it in a way that‚Äôs profitable for you. Surprisingly, calculus may be one of the things to help you do just that. The idea here is that calculus is involved any time you‚Äôre trying to do a maximization and a minimization, when you‚Äôre trying to find the balance between these disparate demands. Let me give you an example of this might work. Let‚Äôs say that you sell a corporate coaching package online and that you currently sell it for $500 and that you have 300 sales per week. That‚Äôs $150,000 revenue per week. But let‚Äôs say that, based on your experience with adjusting prices, you‚Äôve determined that for every $10 off of the price, you can add 15 sales per week. And let‚Äôs also assume, just for purposes of this analysis, that there‚Äôs no increase in overhead. So, the idea here is you can change the sales by adjusting the price, but where are you going to have the maximum revenue? Well, let‚Äôs start with a simple thing, the formula. Revenue is equal to the price times the number of sales, that‚Äôs easy. Well, just a second ago I said that the price is $500, but for every $10 of discount, you can change sales, so we have $10 times d, which is the units of discount. And then next to that is sales and currently, you‚Äôre having 300 sales per week, but for each unit of discount, you can add 15 more sales. Okay, we‚Äôve got an equation here and if you multiply this through, go back to high school, then what you get is negative 150d squared plus 4500d plus 150,000 and this is the thing that we can use to maximize the revenue. This is where calculus comes in. What we‚Äôre going to do is we‚Äôre going to take the derivative of this formula. Now, this one actually wouldn‚Äôt be too hard to do by hand. You can also just stick it into a calculator online, it‚Äôll do it for you, but the derivative is what‚Äôs going to help us find the best discount for maximizing revenue. So, if we get the derivative, it‚Äôs negative 300 times d minus 15. All right, we want to find out when this is equal to zero because that let‚Äôs us know where the maximum of the distribution is. So, we set it equal to zero, we divide both sides by negative 300, that just cancels out, and then we add 15 to both sides and we get d is equal to 15. Now, let me show you what that actually is representing. This is a graph of the equation that I showed you earlier and it has the units of discount across the bottom and it has the weekly revenue up the side and you can see that it goes up and then it curves back down. We want to find where that curve is the highest. Now, one way to do that‚Äôs put a vertical line across the top and the highest point actually is this one right here, it‚Äôs 15 units of discount, which is the same thing we got from the calculus. Now, let‚Äôs go back and determine what that means for our price. The price is $500 minus $10 per unit of discount. We decided that 15 was the optimal solution. $10 times 15 is 150, $500 minus 150 is $350, so that‚Äôs the price that‚Äôs going to get us the optimal revenue. Well, let‚Äôs see how that affects sales. We go back to sales, we originally have 300 per week and we had determined that for every unit of discount, we get 15 more sales per week. Well, we decided that 15 was the ideal units of discount, 15 times 15 is 225, add that to 300, and you get 525 sales per week once we make the change. So, our current revenue is $500 times 300 sales per week, that‚Äôs $150,000 in revenue per week, but if we were to change the price and drop it down to 350, we would increase the sales to 525 and that would gives an estimated total revenue of $183,750 and that‚Äôs a lot more money. In fact, the ratio is 1.225, which means it‚Äôs a 22.5% improvement in revenue. In fact, let‚Äôs look at the revenue this way. If we lower the price by 30%, going from $500 to $350 is 30%, we are able to increase the sales by 75% and that, taken together, increases the revenue by 22.5%. That‚Äôs an increase of almost $2 million annually, simply by making things more affordable and reaching a wider audience and helping them reach their own professional dreams. And that is the way that calculus can help you get paid.&lt;/p&gt;

&lt;h2 id=&quot;3-optimization-and-the-combinatorial-explosion&quot;&gt;3. Optimization and the combinatorial explosion&lt;/h2&gt;

&lt;p&gt;If you want to have a winning team, you need to have both excellent players and excellent teams or combinations of players. Now, you might be tempted to simply take the players that you have and try them in every possible situation, see where they work together in the different positions. If you‚Äôre in a sport that only has a few people, like say, for instance, beach volleyball where there‚Äôs two people on each team, this is something you can do. Let‚Äôs say you have four players total to choose from and you want to try them out in two, where you put each person into a position and you try all the possible permutations. Well, the formula for that is this one right here where we take the n players, that‚Äôs how many we‚Äôre choosing from, that‚Äôs four, and r is how many we‚Äôre taking at a time, that‚Äôs two, and that gives us 12 possible permutations. That‚Äôs something you could do in a day or two and feel confident about the decision that you‚Äôve made. But what if you have more people? Let‚Äôs say you‚Äôre actually dealing with basketball. Well, well now you‚Äôre going to have five people on the court at a time and you‚Äôre going to have 15 people on an NBA roster to choose from. So now you have a permutations of 15, that‚Äôs your n players taken five at a time, and if you‚Äôre randomly shuffling them around in positions to see who does better at what and how well they work together, this is your formula, and unfortunately, now you have 360360 possible permutations. That‚Äôs going to keep you busy for a really, really long time, and you know what? It‚Äôs not even as bad as it gets. Let‚Äôs go to baseball. And let‚Äôs say you want to try out the 25 players on your roster where you put nine of them on the field at a time, and this is actually the sport where I first heard people talk about this. Well, the math gets out of hand very quickly. You‚Äôre doing permutations where n is 25 players taken r as nine at a time, and that gets you over 741 billion possible permutations, which is possibly longer than the entire universe has been in existence, and so that‚Äôs just not going to work. You‚Äôre trying to find an optimum solution, but randomly going through every possibility doesn‚Äôt work. This is called the combinatorial explosion because the growth is explosive as the number of units and the number of possibilities rises and so you need to find another way that can save you some time and still help you find an optimum solution. There are a few simple ways of approaching this. Number one is just to go into Excel. If you can estimate a basic function, you can use trial and error to modeling the function and look for a local maximum. Excel also has what if functions and scenarios that help you do that. You can also use calculus for simple functions. You can use a calculus-based approach that I‚Äôve demonstrated elsewhere. You, on the other hand, you need to be able to estimate the functions and get a derivative. And then there‚Äôs also optimization, also known as mathematical optimization or mathematical programming, and certain versions of which are known as linear programming. And I want to show you very quickly how I can use optimization in Excel to answer a basic question. Now my goal here is not to show you every step involved in Excel. What I‚Äôm try to do is show you that it is possible and what it looks like. If you decide it‚Äôs something you want to pursue, you‚Äôre going to need to go back and spend a little more time trying the ins and outs of how this works. I‚Äôm going to be using a little piece of software that is an included add-in called Solver for Excel. And what I‚Äôve done is I‚Äôve set up a hypothetical situation where a person owns a yoga studio and they‚Äôre trying to maximize their revenue for the amount of time they spend. And the first thing I do is over here, I put down a list of possible things a person could spend their time on, from responding to social media and writing a newsletter to teaching classes to recording videos, and I say about how long it takes to do each one and how much each of those impacts the bottom line. And then one of the important things about optimization is putting in constraints, and so here I say we need to do at least this many on the left. We need to do no more than this many on the right. You know, you have to respond to your email, but you can‚Äôt teach classes all day long. You get burned out, and so you have these minima and these maxima, and then we‚Äôre going to use the Solver to try to find an optimal number of units. How much of your time you should spend on each of these activities. We‚Äôre going to maximize the impact, which is basically revenue, and then we‚Äôre going to make it so it‚Äôs no more than 40 hours because you still have a life you‚Äôve got to live. Now when you call up the Solver dialog box, it looks like this. This is where you say what you‚Äôre trying to maximize, what you‚Äôre going to constrain, what it‚Äôs allowed to change, and there‚Äôs a number of options here that can get pretty complex. Again, my goal here is not to show you how to use it, but simply that it exists and it can solve this kind of problem. When I hit solve, what it does is it adjusts the column in F and it says this is your optimal distribution. It says spend this much time on each of these things, and what that is going to do is it‚Äôs going to maximize the impact, which is going to be associated with revenue. And it also says how much time you‚Äôre going to spend on each one. I also put a little dot plot here on the end just to give a visualization. You‚Äôre going to spend most of your time teaching group classes in the studio, but you‚Äôll do at least a little bit of everything else as a way of maximizing the impact. This is a way of reaching a conclusion about the optimal way of allocating your time, maybe even recruiting a team, without having to go through billions and billions of possibilities. That‚Äôs one of the beauties of data analysis and data science, to help you cut through the mess and find the most direct way to the goals that are important to you.&lt;/p&gt;

&lt;h2 id=&quot;4-bayes-theorem&quot;&gt;4. Bayes‚Äô theorem&lt;/h2&gt;

&lt;p&gt;Performing is hard. You put in a lifetime of training, weeks of preparation for an event and even when you‚Äôve done the very best, you can never be completely certain that everything‚Äôs going to work out exactly the way you wanted it to. There‚Äôs a certain element of luck or probability associated with it. It‚Äôs the same thing in data science. No matter how big your dataset, no matter how sophisticated your analysis and the resources available to your organization, there‚Äôs still an inescapable element of probability. It‚Äôs just how it works. And one of the best things you can do is to explicitly incorporate that uncertainty into your data science work to give you more meaningful and more reliable insights. This is Bayes‚Äô Theorem and this is one of the keys to incorporating that uncertainty. What Bayes‚Äô Theorem does is it gives you the posterior or after-the-data probability of a hypothesis as a function of the likelihood of the data given the hypothesis, the prior probability of the hypothesis and the probability of getting the data you found. Now you can also write it as a formula like this but it‚Äôs going to be a lot easier if I give you an example and do the work graphically. So, let‚Äôs take a look at medical diagnosis as a way of applying Bayesian analysis or Bayes‚Äô Theorem to interpreting the results of a study. There are three things you need to know. First off, you need to know the base rate of a disease, how common is it overall? Let‚Äôs assume that we have a disease that affects 5% of the people who are tested. Then we need to know the true positive rate, so 90% of the people with the disease will test positive. It means that 10% won‚Äôt. Those will be false negatives. There‚Äôs also a false positive rate, that‚Äôs 10% of the people who do not have the disease will also test positive. It depends on how the test is set up but that‚Äôs not completely unlikely and so, we have the base rate, the true positive rate and the false positive rate. We can use those to answer this question. If a person tests positive and the test is advertised as 90% accurate, then what is the probability that they actually have the disease? Well, I‚Äôm going to give you a hint. The answer is not 90%. And it has to do because of the way we incorporate the base rates and the false positives. So let‚Äôs go and look at this. This square represents 100% of the people tested for the disease. Up here at the top we have 5% of the total, that‚Äôs the people who actually have the disease. Below that are the people without the disease, that‚Äôs 95% of the total. Then if we take the people with the disease and give them a test, the people who have the disease who test positive, that‚Äôs in blue, 90% of them, that‚Äôs 90% of 5% is 4.5% of the total number of people and these are the true positives. Now next to that, we add the people without the disease who test positive. That‚Äôs 10% of the 95%, so that‚Äôs 9.5% of the total and those are the false positives and so, you can see, everybody in blue got a positive result but what‚Äôs the probability that you actually have the disease? Well, to do that, we‚Äôre going to calculate the posterior probability of the disease. We take the true positives and divide it by all of the positives, the true and the false positives. Now in this particular case, that‚Äôs 4.5% divided by the sum of 4.5 and 9.5% which is 14%. 4.5 divided by 14% is 32.1% and what this means is that even though the test is advertised as 90% accurate, depending on the base rate of the disease, and the false positive, a positive test result may still mean that you have less than a 1/3 chance of having the disease. That‚Äôs a huge difference between what people expect the result to be and how it‚Äôs actually going to play out in practice and that‚Äôs one of the most important things you can do for accurately interpreting and putting into action the results of your data science analyses to get meaningful and accurate insight.&lt;/p&gt;</content><author><name>Learning Archive</name></author><category term="technology" /><category term="web" /><category term="markdown" /><category term="learning_archive" /><summary type="html">1. Algebra</summary></entry><entry><title type="html">Tools for Data Science</title><link href="/notebook/tools-for-ds" rel="alternate" type="text/html" title="Tools for Data Science" /><published>2021-01-01T00:00:06+07:00</published><updated>2021-01-01T00:00:06+07:00</updated><id>/notebook/tools-for-ds</id><content type="html" xml:base="/notebook/tools-for-ds">&lt;h2 id=&quot;1-applications-for-data-analysis&quot;&gt;1. Applications for data analysis&lt;/h2&gt;

&lt;p&gt;When people think about data science, machine learning and artificial intelligence, the talk turns almost immediately to tools. Things like programming languages and sophisticated computer setups, but remember, the tools are simply a means to an end, and even then only a part of it. The most important part of any data science project by far is the question itself, and the creativity that comes in exploring that question, and working to find possible answers using the tools that best match your questions. And sometimes, those tools are simple ones. It‚Äôs good to remember even in data science that we should start with the simple, and not move on to the complicated until it‚Äôs necessary. And for that reason, I suggest we start with data science applications. And so, you may wonder, ‚ÄúWhy apps?‚Äù Well number one, they‚Äôre more common. They‚Äôre generally more accessible, more people are able to use them. They‚Äôre often very good for exploring the data, browsing the data. And they can be very good for sharing. Again, because so many people have them and know how to use them. By far the most common application for data work is going to be the humble spreadsheet, and there are a few reasons why this should be the case. Number one, I consider spreadsheets the universal data tool. It‚Äôs my untested theory that there are more datasets in spreadsheets than in any other format in the world. The rows and columns are very familiar to a very large number of people and they know how to explore the data and access it using those tools. The most common by far is Microsoft Excel and its many versions. Google Sheets is also extremely common, and there are others. The great thing about spreadsheets is they‚Äôre good for browsing. You sort through the data, you filter the data. It makes it really easy to get a hands-on look at what‚Äôs going on in there. They‚Äôre also great for exporting and sharing the data. Any program in the world can read a .csv file, a ‚Äúcomma separated values‚Äù, which is the generic version of a spreadsheet. Your client will probably give you the data in a spreadsheet, they‚Äôll probably want the results back in a spreadsheet. You can do want in-between, but that spreadsheet is going to serve as the common ground. Another very common data tool, even though it‚Äôs not really an application, but a language, is S-Q-L or SQL, which stands for ‚ÄúStructured Query Language.‚Äù This is a way of accessing data storing databases, usually relational databases, where you select the data, you specify the criteria you want, you can combine it and reformat in ways that best work. You only need maybe a dozen or so commands in SQL to accomplish the majority of tasks that you need. So a little bit of familiarity with SQL is going to go a very long way. And then there are the dedicated apps for visualization. That includes things like Tableau, both the desktop and the public and server version, and Qlik. What these do is they facilitate data integration, that‚Äôs one of their great things. They bring in data from lots of different sources and formats, and put it together in a pretty seamless way. Their purpose is interactive data exploration. To click on set groups, to drill down, to expand what you have, and they‚Äôre very very good at that. And then there are apps for data analysis. So these are applications that are specifically designed for point-and-click data analysis. And I know a lot of data scientists think that coding is always better at everything, but the point-and-click graphical user interface makes things accessible to a very large number of people. And so this includes common programs like SPSS, or JASP, or my personal favorite, jamovi. JASP and jamovi are both free and open source. And what they do is they make the analysis friendly. Again, the more people you can get working with data, the better, and these applications are very good at democratizing data. But whatever you do, just remember to stay focused on your question, and let the tools and the techniques follow your question. Start simple, with the basic applications, and move on only as the question requires it. That way, you can be sure to find the meaning and the value as you uncover it in your data.&lt;/p&gt;

&lt;h2 id=&quot;2-languages-for-data-science&quot;&gt;2. Languages for data science&lt;/h2&gt;

&lt;p&gt;I love the saxophone I play it very badly but I love to listen to it and one of the curious things about the saxophone is that if you want to play gigs professionally you can‚Äôt play just the saxophone at the very least you have to play both the alto and tenor saxophone as well as the flute and the clarinet and for other gigs you may need to be able to play obo, English horn, bassoon bass clarinet, and even recorder and chrome horn like one of my teachers. You have to be a musical polyglot. I mention this because one of the most common questions in data science is whether you should work in Python or in R, two very common languages for working with data. The reason this question comes up is because programming languages give you immense control over your work and data science. You may find that your questions go beyond the capabilities of data analysis applications and so the ability to create something custom tailored that matches your needs exactly, which is the whole point of data science in the first place is going to be critical. But let me say something more about Python and R Now Python is currently the most popular language for data science and machine learning. It‚Äôs a general purpose programming language. You can do anything with Python and people do enormous numbers of things that are outside of data science with it. Also, Python code is very clean and it‚Äôs very easy to learn So there are some great advantages to Python R on the other hand, is a programming language that was developed specifically for work in data analysis. And R is still very popular with scientists and with researchers. Now, there are some important technical differences between the two such as the fact that R works natively with vectorized operations and as non standard evaluation and Python manages memory and large data sets better in its default setup but neither of those is fixed, both of them can be adapted to do other things. And really, the thumb of this is that like any professional saxophonist is going to need to be able to play several different instruments any professional data scientist is going to need to be able to work comfortably in several different languages. So those languages can include both Python and R they can include SQL or Structured Query Language or Java or Julia or Scala or MATLAB really, all of these serve different purposes they overlap but depending both on the question that you are trying to answer, the kind of data that you have, and the level at which you‚Äôre working, you may need to work with some, many or all of these. Now, I do want to mention one other reason why programming languages are so helpful in data science and that‚Äôs because you can expand their functionality with packages these are collections of code that you can download that give extra functionality or facilitate the entire process of working with data and often, it is the packages that are more influential than the actual language. So things like TensorFlow which make it so easy to do deep learning neural networks you can use that in Python or in R and it‚Äôs going to facilitate your work but no matter what language you use and what packages you use it is true that the programming languages that are used in data science are going to give you this really fine level control over your analysis and let you tailor it to the data and to the questions that you have.&lt;/p&gt;

&lt;h2 id=&quot;3-machine-learning-as-a-service&quot;&gt;3. Machine learning as a service&lt;/h2&gt;

&lt;p&gt;One of the things that is most predictable about technology is that things get faster, smaller, easier, and better over time. This is the essence of Moore‚Äôs Law, which originally talked about just the density of transistors on circuits doubling every two years, but think about, for instance, the women working here on ENIAC, that‚Äôs the Electronic Numerical Integrator and Computer, which was the first electronic general-purpose computer back in 1945. It was huge. It filled up a room and it took a whole team of people to run it. Then things evolved, for instance, to very colorful reel-to-reel computers, then you get your desktop Macintosh, I still have my Classic II, and before you know it, you‚Äôre running your billion-dollar tech company from your cell phone. One of the most important developments in the internet era has been SaaS, or software as a service. Just think of anytime you‚Äôve used an online application like Excel Online instead of an application installed locally on your computer like the regular desktop version of Excel. That is a way of making the application more accessible because anybody can get onto it with any machine connected to the internet, and really more useful to people. Well, a similar revolution is happening now in data science with machine learning as a service. It‚Äôs not as easy to pronounce MLaaS, but it‚Äôs a way of making the entire process of data science, machine learning, and artificial intelligence easier to access, easier to setup, and easier to get going. All the major cloud data providers have recently announced these machine learning as a service offerings. So for instance, there‚Äôs Microsoft Azure ML, there‚Äôs Amazon Machine Learning, and Google AutoML, and IBM Watson Analytics, and all of these have some very closely-related advantages, things that make your life a lot easier. Number one, they put the analysis where the data is stored. You‚Äôve got your massive and complex data sets but they‚Äôre stored in the servers that each of these services have, and so you don‚Äôt have to do an export and import, you just go right where the data is. Also, most of them give you very flexible computing requirements. It‚Äôs not like you have to purchase the hardware. You can rent CPUs and GPUs. You can rent RAM and hard drive space as you need it. They also frequently give you a drag-and-drop interface that makes the programming and the setup of the analysis dramatically easier. Again, the point here is that it democratizes the process, it makes it more accessible for more people in more circumstances. And that is part of the promise of data science. Now, it‚Äôs too early to project very far into the future and see what all of the exact consequences of this revolution will be, especially because new services are being announced and major changes are being made all the time. But the idea, again, with machine learning as a service is that it puts the analysis where the data is and it makes it more open and more available to more people. Again, it‚Äôs the democratization of data science, and that‚Äôs the promise of machine learning as a service.&lt;/p&gt;</content><author><name>Learning Archive</name></author><category term="technology" /><category term="web" /><category term="markdown" /><category term="learning_archive" /><summary type="html">1. Applications for data analysis</summary></entry><entry><title type="html">Sources of Rules</title><link href="/notebook/sources-of-rules" rel="alternate" type="text/html" title="Sources of Rules" /><published>2021-01-01T00:00:05+07:00</published><updated>2021-01-01T00:00:05+07:00</updated><id>/notebook/sources-of-rules</id><content type="html" xml:base="/notebook/sources-of-rules">&lt;h2 id=&quot;1-the-enumeration-of-explicit-rules&quot;&gt;1. The enumeration of explicit rules&lt;/h2&gt;

&lt;p&gt;Let‚Äôs say you meet someone at a party and after talking for awhile you start to wonder if that person might be interested in you. This is apparently a question that is on a lot of people‚Äôs minds. If Google‚Äôs auto-complete is to be trusted, assessing attraction is a major research question. Of the top 10 statements to start how to tell, the first two are on this topic. Shortly followed by how to tell an egg is bad and in fact, men appear to be sufficiently difficult to read that they get to appear twice on the top 10 list. And so that let‚Äôs you know we need an answer to this question, how can you tell if somebody is interested in you? Well maybe we can propose some rules. Maybe they‚Äôre interested in you because they said so. Or maybe they smiled and made eye contact. Or maybe they just swiped right. And again, there are the insecure doubts that pop up and undermine your belief in these things. Maybe it‚Äôs wonderful to meet you isn‚Äôt diagnostic and they say that to everybody. Maybe they smile when they‚Äôre bored or maybe they slipped on the ice and fell and accidentally swiped the wrong way. All these things that can undermine your faith in the rules that you have. But lest you think I‚Äôm just being silly in these examples, I want to point out that this is a legitimate data science problem. Dating apps are a multi-billion dollar business. And if you can help people find someone they love, then you have truly accomplished something worthwhile too. So, if we want to write a program to help people figure out if someone likes them, maybe we just need to include a little more detail and create a flowchart with explicit rules and qualifications before concluding with a definitive yes or no. This is an example of what‚Äôs called an expert system. An expert system is an approach to machine decision-making in which algorithms are designed that mimic the decision-making process of a human domain expert. In this case, maybe Jane Austen. She was an incisive and strategic decision-maker and when it came to matters of the heart. For example, here‚Äôs a set of observations from ‚ÄúPride and Prejudice‚Äù where we‚Äôre pointed out that being wholly engrossed by one person and inattentive to others, offending people by not asking them to dance, having people speak to you and not answering, she says, ‚ÄúCould there be finer symptoms?‚Äù Could there be more diagnostic criteria for amorous inclinations? She says, ‚ÄúIs not general incivility ‚Äúthe very essence of love?‚Äù And so maybe we could create an expert system based on these criteria. Now, I can imagine a lot of situations where this might not work well but there are many other situations in which expert systems which model the decision-making of an expert have worked remarkably well. Those include things like flowcharts to help you determine which data analysis method is most appropriate for answering the question you have given the data that‚Äôs available to you. Or criteria for medical diagnoses. I‚Äôm in psychology and so we use the DSM which has the Diagnostic and Statistical Manual of the American Psychiatric Association to say if a person has these symptoms but not these, than this is the likely diagnoses. Or even business strategies where you can give general checklists of approaches on what to do first, what to do next and how to reach the goals that you have. But it‚Äôs important to remember that like any system, logic and the flow of expert based decisions has its limits. You‚Äôre eventually going to hit the wall. You‚Äôre going to meet situations that you just can‚Äôt accurately predict or they‚Äôll be things that you never anticipated and so it turns out that there are significant limits to enumerating explicit rules for decision-making and in fact we need more flexible and more powerful methods which is what I‚Äôm going to turn to next.&lt;/p&gt;

&lt;h2 id=&quot;2-the-derivation-of-rules-from-data-analysis&quot;&gt;2. The derivation of rules from data analysis&lt;/h2&gt;

&lt;p&gt;A dancer has to spend years working on their craft to deliver a masterful performance. One of the paradoxes of this training is that sometimes you have to think a little bit less in order to move better. Your conscious processes can interfere with fluid and meaningful movement. Sometimes you just have to calm down all your ideas about expert decision-making systems and the rules that they bring along, and let the data have a say in how you should go about your work. We‚Äôll start by looking at linear regression, which is a common and powerful technique for combining many variables in an equation to predict a single outcome, the same way that many different streams can all combine into a single river. We‚Äôll do this by looking at an example based on a data science salary survey. So this is based on real data, and the coefficients are based on the actual analysis, although I‚Äôm only showing a few of the variables that went into the equation. What the researchers found is that you could predict a person‚Äôs salary in data science by first starting with a value of $30500 per year, that‚Äôs the intercept, and then for each year above 18, so take their age and subtract 18, you add $1400, and then to that you add $5900 for each point on a five point bargaining scale, from one to five, so people who are better at bargaining made more money each year, and to that you can add $380 for each hour of the week that they work through the year. Taken together, you can combine that information, age and bargaining ability and time spent working, to make a single prediction about their salary working in data science. It‚Äôs an easy way to take these multiple sources and combine them using this rule which comes from the data that you fed it to know how to best predict the one outcome. Another method that‚Äôs frequently used in data science is what‚Äôs called a decision tree. This is a whole series, a sequence of binary decisions, based on your data, that can combine to predict an outcome. It‚Äôs called a tree because it branches out from one decision to the next, and in this example we‚Äôll look at a simple analysis of the classic data set on classifying iris flowers as one of three different species depending on the length and the width of the petal and the sepal, and the decision tree looks like this. It‚Äôs extremely simple because there‚Äôs only four variables in the data set. But what you start with is, based on the analysis, is the single most important thing is to first know what is the length of the petal, and if the petal is less than or equal to 1.9 centimeters, then we would predict that 100% of them are going to be iris setosa, that‚Äôs a particular species. On the other hand, if the length of the petal is more than 1.9 centimeters, we have to make another decision. That‚Äôs, we have to look at the width of the petal, and if that width is less than or equal to 1.7 centimeters, then we need to look at the length of the petal again, and if it‚Äôs less than or equal to 4.8 centimeters, then there‚Äôs a very high probability that it‚Äôs a versicolor, and the algorithm used to calculate this does this by finding out which variable split produced the best predictions, and then do you need to split them any more. So it‚Äôs not like somebody sat down and said I think length is most important, let‚Äôs do that first, instead, the data drove the entire decision, and you‚Äôll notice by the way, there‚Äôs only two variables in this. There‚Äôs petal length which appears twice, and petal width. The two other variables in the dataset, the sepal length and the sepal width, don‚Äôt work into it at all, not in this simple model, and what this lets you do is it lets you have a data-driven method of classifying observations into one case or another. You can then feed that into your algorithm and use it to process new information. Again, either approach, linear regression, decision trees, or so many others, can give you the information you need for data-based decision making, either in-person, or as a part of your data science algorithms, and that will get you on your way.&lt;/p&gt;

&lt;h2 id=&quot;3-the-generation-of-implicit-rules&quot;&gt;3. The generation of implicit rules&lt;/h2&gt;

&lt;p&gt;When the aliens land, we‚Äôll have a better chance of understanding them because of our experience with machine learning and artificial intelligence. AI doesn‚Äôt see things the way that people do and it doesn‚Äôt reason the way that people do. For example, DeepMind‚Äôs AlphaGo, AlphaGo Zero and AlphaZero are three generations of AIs that have come to completely master the game, Go, which is massively more complex than chess. But these AI‚Äôs beat all of their opponents, not by mimicking the moves of human Go masters, but by coming up with their own unorthodox, but extremely effective strategies. One computer science researcher described these AI‚Äôs game play in these terms. ‚ÄúIt‚Äôs like an alien civilization inventing ‚Äúits own mathematics which allows it ‚Äúto do things like time travel.‚Äù So, there‚Äôs something very significant going on here. And part of the issue is this. Neural networks, a particularly powerful kind of machine learning technique, can do amazing things with games, and can also quickly spot differences between, for example, two closely related breeds of dogs, like the English Mastiff and the French Mastiff. Humans will tell you that the English Mastiff is generally larger in size, about 200 pounds, with a large muzzle and often with a distinctive black mask. Whereas the French Mastiff is not quite so huge, tends to have a shorter muzzle, and slight different coloration. Neural networks, on the other hand, can distinguish these two breeds reliably, but because of the peculiar things they focus on, may have trouble distinguishing either of them, perhaps, from an image of a toaster that has certain pixels strategically doctored. And you can see the course, AI Accountability Essential Training, if you want to learn more about the peculiarities of AI vision. Neural networks look at things in a different way than humans do and in certain situations they‚Äôre able to develop rules for classification, even when humans can‚Äôt see anything more than static. So, for instance, there are implicit rules. The algorithm knows what the decision rules are and can apply them to new cases, even if the rules tend to be extraordinarily complex. But computers are able to keep track of these things better than humans can. So the implicit rules are rules that help the algorithms function. They are the rules that they develop by analyzing the test data. And they‚Äôre implicit because they cannot be easily described to humans. They may rely on features that humans can‚Äôt even detect or they may be nonsensical to humans. But the trick is, those implicit rules can still be very effective and that‚Äôs one of the tricks, that‚Äôs one of the trade-offs that goes on with machine learning. You can think of it this way. It leaves you, as a human decision-maker, in an interesting position. You can trust the machine to do its work very quickly and often very accurately, but in a slightly mysterious way. Relying on implicit rules that it, the algorithm inferred from the data, or you can use other, more explicit processes, like regression and decision trees that are easier to understand and monitor, but may not be as effective overall. It‚Äôs a difficult decision, but one that should definitely be left in the hands of the humans.&lt;/p&gt;</content><author><name>Learning Archive</name></author><category term="technology" /><category term="web" /><category term="markdown" /><category term="learning_archive" /><summary type="html">1. The enumeration of explicit rules</summary></entry><entry><title type="html">Sources of Data</title><link href="/notebook/sources-of-data" rel="alternate" type="text/html" title="Sources of Data" /><published>2021-01-01T00:00:04+07:00</published><updated>2021-01-01T00:00:04+07:00</updated><id>/notebook/sources-of-data</id><content type="html" xml:base="/notebook/sources-of-data">&lt;h2 id=&quot;1-data-preparation&quot;&gt;1. Data preparation&lt;/h2&gt;

&lt;p&gt;Anybody who‚Äôs cooked knows how time-consuming food prep can be, and that doesn‚Äôt say anything about actually going to the market, finding the ingredients, putting things together in bowls and sorting them, let alone cooking the food. And it turns out there‚Äôs a similar kind of thing that happens in data science, and that‚Äôs the data preparation part. The rule of thumb is that 80% of the time on any data science project is typically spent just getting the data ready. So data preparation, 80%, and everything else falls into about 20%, and you know, that can seem massively inefficient and you may wonder what is your motivation to go through something that‚Äôs so time consuming and really, this drudgery? Well, if you want in one phrase, it‚Äôs GIGO, that is garbage in, garbage out. That‚Äôs a truism from computer science. The information you‚Äôre going to get from your analysis is only as good as the information that you put into it, and if you want to put in really starker terms, there‚Äôs a wonderful phrase from Twitter, and it says most people who think that they want machine learning or AI really just need linear regression on cleaned-up data. Linear regression is a very basic, simple, and useful procedure and it lets you know just as a rule of thumb, if your data is properly prepared, then the analysis can be something that is quick and clean and easy and easy to interpret. Now when it comes to data preparation and data science, one of the most common phrases you‚Äôll hear is tidy data, which seems a little bit silly, but the concept comes from data scientist Hadley Wickham, and it refers to a way of getting your data set up so it can be easily imported into a program and easily organized and manipulated. And it revolves around some of these very basic principles. Number one, each column in your file is equivalent to a variable, and each row in your file is the same thing as a case or observation. Also you should have one sheet per file. If you have an Excel sheet, you know you can have lots of different sheets in it, but a CSV file has only one sheet and also that each file should have just one level of observations. So you might have a sheet on orders, another one on the SKUs, another one on individual clients, another one on companies and so on and so forth. If you do this, then it makes it very easy to import the data and to get the program up and running. Now this stuff may seem really obvious and you say, well, why do we even have to explain that? It‚Äôs because data in spreadsheets frequently is not tidy. You have things like titles and you have images and figures and graphs and you have merged cells and you have color to indicate some data value or you have sub-tables within the sheet or you have summary values or you have comments and notes that might actually contain important data. All of that can be useful if you‚Äôre never going beyond that particular spreadsheet, but if you‚Äôre trying to take it into another program, all of that gets in the way. And then there are other problems that show up in any kind of data. Things like, for instance, do you actually know what the variable and value labels are? Do you know what the name of this variable is? ‚ÄòCause sometimes, they‚Äôre cryptic. Or what does a three on employment status mean? Do you have missing values where you should have data? Do you have misspelled text? If people are writing down the name of the town that they live in or the company they work for, they could write that really in infinite number of ways. Or in a spreadsheet, it‚Äôs not uncommon for numbers to accidentally be represented in the spreadsheet as text, and then you can‚Äôt do numerical manipulations with it. And then there‚Äôs a question of what to do with outliers and then there‚Äôs metadata, things like where did the data come from? Who‚Äôs the sample? How was it processed? All of this is information you need to have in order to have a clean dataset that you know the context and the circumstances around it that you can analyze it. And that‚Äôs to say nothing about trying to get data out of things like scanned PDFs or print tables or print graphs, all of which require either a lot of manual transcription or a lot of very fancy coding. I mean, even take something as simple as emojis, which are now a significant and meaningful piece of communication, especially in social media. This is the rolling on the floor laughing emoji. There are at least 17 different ways of coding this digitally. Here‚Äôs a few of them, and if you‚Äôre going to be using this as information, you need to prepare your data to code all of these in one single way so that you can then look at these summaries all together and try to get some meaning out of it. I know it‚Äôs a lot of work, but just like food prep, is a necessary step to get something beautiful and delicious. Data prep is a necessary, vital step to get something meaningful and actionable out of your data. So give it the time and the attention it deserves, you‚Äôll be richly rewarded.&lt;/p&gt;

&lt;h2 id=&quot;2-in-house-data&quot;&gt;2. In-house data&lt;/h2&gt;

&lt;p&gt;Data science projects can feel like this epic expedition or this massive group project. But sometimes, you can get started right here, right now. That is, your organization may already have the data that you need. And there a few major advantages to using this kind of in-house data. The first is it‚Äôs fast. It‚Äôs the fastest way to start. It‚Äôs right there and it‚Äôs ready to go. An interesting one is that certain restrictions on data may not apply for use that is entirely within the boundaries of your organization. So if you have data that includes individual identifiers, you may be able to use that for your organization‚Äôs own research. Next, you may actually be able to talk with the people who gathered the data in the first place. You can have questions for them. They can tell you how they sampled it, what the things mean, why they did it in this particular way, all of that can save you an enormous amount of time and headaches, and also it may be in the same format that you currently need. And what that means, is that pieces may fit together perfectly. They may have the same code, they may have the same software, they may use the same standards and style guides, and that can save you a lot of time. One the other hand, there are some potential downsides to in-house data. Number one is, if it was an ad-hoc project, it may not be well documented. They may have just kind of thrown it together, and never quite put in all the information about it. It may not be well maintained. Maybe they gathered it five years ago, and kind of let it slip since then. And the biggest one is the data simply may not exist. Maybe what you need really isn‚Äôt there in your organization. And so, in-house isn‚Äôt an option in that particular case. So, there are some potential downsides. But really, the benefits are so big, are so meaningful, that it‚Äôs always worth your time to take just a few minutes to look around and see what‚Äôs there, and get started on your project right now.&lt;/p&gt;

&lt;h2 id=&quot;3-open-data&quot;&gt;3. Open data&lt;/h2&gt;

&lt;p&gt;I love my public library here in Salt Lake City and this right here is the main library downtown. It‚Äôs a beautiful building. There‚Äôs desk by a wall of windows looking out over the Wasatch Mountains, it‚Äôs a great place to be but even more, I love it because it‚Äôs a library and they have books there. They have a collection of over half a million books and other resources including rows and rows of beautiful books on architecture and landscaping that I could never purchase on my own. I love to browse the shelves, find something unexpected and beautiful, go by the windows and enjoy it. Open data is the like the public library of data science. It‚Äôs where you can get beautiful things that you might not be able to gather or process on your own and it‚Äôs all for the public benefit. Basically it‚Äôs data that is free because it has no cost and it‚Äôs free to use that you can integrate in your projects and do something amazing. Now there are a few major sources of open data. Number one is government data, number two is scientific data and the third one is data from social media and tech companies and I want to give you a few examples of each of these. Let‚Äôs start with government sources. In the United States, the biggest and most obvious one is data.gov which is home of open data from the U.S Federal Government where you can find an enormous amount of datasets on anything from consumer spending, to local government to finance, a wide range of topics that would be really useful for data science. For example, you can look up the Housing Affordability Data System which is going to actually have a big impact on where you can employ people to work for you and where potential customers are for your products and services. If you‚Äôre in the United Kingdom, then data.gov.uk is going to be the major choice or if you‚Äôre in Sweden, you‚Äôve got open data sources from the government there too. In fact, the Open Knowledge Foundation Runs what they call the Global Open Data Index which gives you information about the open data sources in every country around. At a state-by-state level, you also have open data sources. I live in Utah and this utah.gov‚Äôs Open Data Catalog and you can even drill down to the city level, it‚Äôs within the same website but now I can focus on specific cases from my city. For scientific data, one great source is the ISCU‚Äôs World Data System which is a way of hosting a wide range of scientific datasets that you can use in your own work. There‚Äôs also ‚ÄúNature‚Äù, that‚Äôs the major science journal has put together a resource called Scientific Data which is designed to both house and facilitate the sharing of the datasets used within this kind of research. There‚Äôs the Open Science Data Cloud which by the way, is not to be confused with the Open Data Science Conference, a wonderful event. And then there‚Äôs also the Center for Open Science which gives you the ability to house your actual research there, so many opportunities to upload data, to find data, to download and share it and then share your results with other people. And then in social data, one of the favorite ones is Google Trends where you can look at national trends on search terms, so for instance, here‚Äôs one that is looking at the relative popularity over time of the terms data science, machine learning, and artificial intelligence. You can also use Google Correlate to get a state-by-state view or weekly time series. If you simply click on Compare US states here in the corner and then enter the term that you want, you can do data science and from this, you can see that California‚Äôs high on data science, those are standard deviations there, 1.8 standard deviations above the natural average. Washington is high. Massachusetts is at the very top and that gives you a great way of looking at the relative interest in topics across the country and you can download the data in CSV format and then Yahoo! Finance is one of the great sources for stock market information. And then Twitter allows you to search by tags, by users and it lets you look at things like #DataScience and you can download that information by using the Twitter APIs and include that in your own analyses. So, there‚Äôs an enormous range of possibilities with open data. It‚Äôs like bringing the best of library availability and teamwork to your own data science projects, so find what‚Äôs out there and get the information you need to answer your specific questions and get rolling with the actionable insights you need.&lt;/p&gt;

&lt;h2 id=&quot;4-apis&quot;&gt;4. APIs&lt;/h2&gt;

&lt;p&gt;When You draw a picture or write a letter, chances are that you can draw well with one of your hands, your dominant hand and not so much with the other. I recently heard someone describe this as having a well developed API for your dominant hand but only a clunky one for the non-dominant hand. An API or Application Programming Interface isn‚Äôt a source of data but rather it‚Äôs a way of sharing data, it can take data from one application to another or from a server to your computer. It‚Äôs the thing that routes the data, translates it, and gets it ready for use. I want to show you a simple example of how this works. So I‚Äôve gone to this website that has what‚Äôs called the JSON Placeholder. JSON stands for JavaScript Object Notation, it‚Äôs a data format and if we scroll down here, you‚Äôll see this little, tiny piece of code and what it says is go to this web address and get the data there and then show it, include it and you can just click on this to see what it does. There‚Äôs the data in JSON format. If you want to go to just this web address directly, you can and there‚Äôs the same data. You can include this information in a Python script or a R script or some other web application that you are developing. It brings it in and it allows you to get up and running very, very quickly. Now API‚Äôs can be used for a million different things, three very common categories include social API‚Äôs that allow you to access data from Twitter or Facebook and other sources as well as use them as logins for your own sites. Utilities, things like Dropbox and Google Maps so you can include that information in your own apps. Or commerce, Stripe for processing payments or MailChimp for email marketing or things like Slack or a million other applications. The data can be opened which means all you need is the address to get it or it maybe proprietary maybe you have to have a subscription or you purchase it and then you‚Äôll need to log in. But the general process is the same. You include this byte code and it brings the data in and gets you up and running, you can then use that data in data analysis, so it becomes one step of a data science project or maybe your creating a app, you can make a commercial application that relies on data that it pulls from any of several different API‚Äôs like weather and directions. Really the idea here is that API‚Äôs are teamwork. API‚Äôs facilitate the process of bringing things together and then adding value to your analysis and to the data science based services that you offer.&lt;/p&gt;

&lt;h2 id=&quot;5-scraping-data&quot;&gt;5. Scraping data&lt;/h2&gt;

&lt;p&gt;Watts Towers in Los Angeles is a collection of sculptures and structures by Simon Rodia that are nearly a hundred feet tall and made from things that he found around him. Scrap pieces of rebar, pieces of porcelain, tile, glass, bottles, sea shells, mirrors, broken pottery and so on. But the towers are testament to what a creative and persistent person can do with the things that they find all around them. Data scraping is, in a sense, the found art of data science. It‚Äôs when you take the data that‚Äôs around you, tables on pages and graphs in newspapers, and integrate that information into your data science work. Unlike the data that‚Äôs available with API‚Äôs or Application Programming Interfaces, which is specifically designed for sharing, Data scraping is for data that isn‚Äôt necessarily created with that integration in mind. But I need to immediately make a quick statement about ethics and data science. Even though it‚Äôs possible to scrape data from digital and print sources, there‚Äôs still legal and ethical constraints that you need to be aware of. For instance, you need to respect people‚Äôs privacy. If the data is private, you still need to maintain that privacy. You need to respect copyright. Just because something‚Äôs on the web doesn‚Äôt mean that you can use it for whatever you want. The idea here is Visible Doesn‚Äôt Mean Open just like in an open market just because it‚Äôs there in front of you and doesn‚Äôt have a price tag doesn‚Äôt mean it‚Äôs free. There are still these important elements of laws, policies, social practices that need to be maintained to not get yourself in some very serious trouble. And so keep that in mind when you‚Äôre doing Data scraping. So for instance, let‚Äôs say you‚Äôre at Wikipedia and you find a table with some data that you want. Here‚Äôs a list of dance companies. And you can actually copy and paste this information but you can also use some very, very simple tools for scraping. In fact, if you want to to put into a Google Sheet, there‚Äôs even a function that‚Äôs designed specifically for that. All you need to do is open up a Google Sheet, and then you use this function: IMPORT HTML. You just give it the address that you‚Äôre looking for, say that you‚Äôre importing a table and then if there‚Äôs more than one table you have to give it the numbers. This one only has one so I can just put in 1, and there‚Äôs the data. It just fills it in automatically. And that makes it possible for us to get this huge jumpstart on our analysis. Some other kinds of data that you might be interested in scraping might include things like reviews online and ranking data. You can use specialized apps for scraping consistently structured data or you can use packages and programming languages like Python and R. Now, one interesting version of this is taking something like a heat map or a choropleth is actually what it‚Äôs called in this particular case. Here‚Äôs a map that I created in Google Correlate on the relative popularity of the term data science. Now, if you‚Äôre on Google Correlate, this is interactive. You can hover over and you can download the data. But right now this is just a static image. And if you wanted to get data from this, you could, for instance, write a script that‚Äôd go through the image pixel by pixel, get the color of each pixel, you can then compare to the scales at the bottom left. You can get the x, y coordinates for each pixel and then compare that to a shape file of the U.S. and then ultimately put the data in a spreadsheet. It‚Äôs not an enormous amount of work and it is a way of recovering the data that was used to create a map like this. If you want to see a really good example of Data scraping after (mumbles) where you‚Äôre taking image data and getting something useful out of it, here at at publicintellegence.net, they‚Äôre reporting on a project that‚Äôs done with Syrian refugee camps and what they‚Äôre using here are satellite photos and then they‚Äôre using a machine learning algorithm to count the number of tents within the camp and that‚Äôs important because you see as it goes over time, it gets much, much larger. And this can be used to access the severity of a humanitarian crisis and really how well you can respond to it. But it‚Äôs the same basic principles of Data scraping. Taking something that was created in one way, just an image, but then using the computer algorithms to extract some information out of it and give you the ability to get some extra insight and figure out what your next steps in your project need to be.&lt;/p&gt;

&lt;h2 id=&quot;6-creating-data&quot;&gt;6. Creating data&lt;/h2&gt;

&lt;p&gt;Sometimes you need something special, something that‚Äôs not already there. In the data science world, there‚Äôs a lot of data you can get from in house data, open data APIs, and even data scraping, but if you still can‚Äôt get the data you need to answer the questions you care about, then you can go the DIY route, and get your own data. There are several different ways to go about this. The first one I would recommend is just natural observation. See what people are doing. Go outside, see what‚Äôs happening in the real world, or observe online, see what people are saying about the topics that you‚Äôre interested in. Just watching is going to be the first, and most helpful way of gathering new data. Once you‚Äôve watched a little bit about what‚Äôs happening, you can try having informal discussions with, for instance, potential clients. You can do this in person in a one on one, or a focus group setting. You can do it online through email, or through chat, and this time you‚Äôre asking specific questions to get the information you need to focus your own projects. If you‚Äôve gone through that, you might consider doing formal interviews. This is where you have a list of questions, things you are specifically trying to focus on, and getting direct feedback from potential customers, and clients. And if you want to go one step beyond that, you can do surveys, or questionnaires, and you can start asking close ended questions. Ask people to check off, you know, excellent, good, or you can ask them to say yes, or no, or recommend something in particular. You usually don‚Äôt want to do that, however, until you‚Äôve done the other things ahead of time, because this makes a lot of assumptions. And the idea here is that you already know what the range of responses are, and so make sure you don‚Äôt get ahead of yourself with this one. Throughout all of this, one general principle is important, especially in preliminary research, and that is that words are greater than numbers. When you‚Äôre trying to understand people‚Äôs experience, be as open ended as possible. Let them describe it in their own terms. You can narrow things down, and quantify them later by counting, for instance, how many people put this general response, or that general response, but start by letting people express themselves as freely as possible. Also, a couple of things to keep in mind when you‚Äôre trying to gather your own data. Number one is don‚Äôt get ahead of yourself. Again, start with the very big picture, and then slowly narrow it down. Don‚Äôt do a survey until you‚Äôve done interviews. Don‚Äôt do interviews until you‚Äôve watched how people are behaving. Start with the general information, and then move to more specific things as you focus on the things that you can actually have some influence on, the actionable steps in your own projects. Also be respectful of people‚Äôs time and information. They‚Äôre giving you something of value, so make sure that you are that. Don‚Äôt take more time than you need to. Don‚Äôt gather information you don‚Äôt need to, just what is necessary to help you get the insights you need in your own particular project. Now, another method of gathering data that can be extremely useful is an experiment. Now, I come from a background in experimental social psychology. The experiments are very time consuming, very labor intensive, but that‚Äôs not what we‚Äôre talking about in the ecommerce, and the tech world. Usually here we‚Äôre referring to what‚Äôs called AB Testing where for instance you prepare two different versions of a website, or a landing page, and you see how many people click on things as a result of those two different versions. That‚Äôs extremely quick, and easy, and effective, and people should constantly be doing that to try to refine, and focus on the most effective elements of their website. But through all of this, there is one overarching principle. When you‚Äôre gathering data, when you‚Äôre engaging with people, seriously, keep it simple, simpler is better, more focused is more efficient, and you‚Äôre going to be better able to interpret it to get useful, next steps out of the whole thing. I also want to mention two elements of research ethics that are especially important when you are gathering new data. Number one is informed consent. When you‚Äôre gathering data from people, they need to know what you want from them, and they also need to know what you‚Äôre going to do with it so they can make an informed decision about whether they want to participate. Remember, they‚Äôre giving you something of value. This is something that needs to be their own free choice. The second one, which can be expressed in many different ways is privacy. Also sometimes confidentiality, or anonymity. You need to keep identifiers to a minimum. Don‚Äôt gather information that you don‚Äôt need, and keep the data confidential, and protected. Now that‚Äôs actually a challenge because when you get this kind of data, it‚Äôs often of value, and it‚Äôs easy for it to get compromised. So maintaining privacy really is incumbent upon the researcher, and it‚Äôs part of building the trust, and the good faith for people to work with you, and others again in the future. And then finally to repeat something I said before, people are sharing something of value with you. So it‚Äôs always a good idea to show gratitude, and response both by saying thank you in the short term. And also by providing them with better, more useful services, they‚Äôre going to make things better in their own lives.&lt;/p&gt;

&lt;h2 id=&quot;7-passive-collection-of-training-data&quot;&gt;7. Passive collection of training data&lt;/h2&gt;

&lt;p&gt;Some things you can learn faster than others, and food is a good example. A lot of people love eating mussels, but some people eat them, get sick, and then never want to touch them again, and I‚Äôm one of those people. This is called a Conditioned Taste Aversion, and results from something that is psychologically unusual, and that‚Äôs one-trial learning. You only have to do it once to get in ingrained into your behavior, but if you‚Äôre working in data science, and especially, if you‚Äôre training a machine-learning algorithm, you‚Äôre going to need a lot more than one trial for learning. You‚Äôre going to need enormous amount of labeled training data to get your algorithm up and working properly. One of the interesting things about data science, is that gathering enormous amounts of data doesn‚Äôt always involve enormous amounts of work. In certain respects, you can just sit there and wait for it to come to you. That‚Äôs the beauty of Passive Data Collection. Now, there are a few examples of this, and are pretty common. One is, for instance, Photo Classification. You don‚Äôt have to classify the photos. The people who take the photos and load them online will often tag them for you, put titles on them, or share ‚Äòem in a folder, that‚Äôs classification that comes around, that you can use in machine-learning without you having to go through the work personally. Or, Autonomous Cars, as they drive, they are gathering enormous amounts of information from the whole plethora of sensors that they have. That information is combined, it‚Äôs uploaded to distant servers, and that allows you to improve the way that your automobiles function. Or even Health Data, people who have smart watches are able to constantly gather information about their activity, the number of steps, how many flights they‚Äôve walked, how long they‚Äôve slept, their heart rate, all this information is gathered without you doing any extra work, and if you‚Äôre the provider of an app that measures health, you can get that information directly without having to do anything else for it. Now, there‚Äôs several benefits to this kind of passive collection of training data. Number one is that, you can get a lot of it. You can get enormous amounts of data very quickly, simply by setting up the procedure and then letting it roll, either automatically, or outsourcing it to the people who use it. The data that you gather can be either very general, or it can be very, very specific; general like, categorizing photos as cats or dogs, or specific like, knowing what a particular person‚Äôs heart rate is at a particular time of day. There are, on the other hand, some challenges associated with this passive data collection. One, and this is actually a huge issue, is that you need to ensure that you have adequate representation; things like categorizing photos. You need to make sure you have lots of different kinds of photos of lots of different kinds of things, and different kinds of people, so you can get all of those categorized, and that your algorithm understands the diversity of the information it will be encountering. You also need to check for shared meaning. What that is, for instance, is that something being happy, or beautiful, you need to make sure that people interpret that in the same way. You also need to check for limit cases. Think about, for instance, heart rate. Some people are going to have just higher heart rate than others, and you don‚Äôt want to have an algorithm that always says, anything above this level is a heart problem, anything below is fine, because that is going to vary from one person to another, and one situation to another, but what all of this does together, is it helps you assemble the massive amounts of data that you need to get critical work done in data science.&lt;/p&gt;

&lt;h2 id=&quot;8-self-generated-data&quot;&gt;8. Self-generated data&lt;/h2&gt;

&lt;p&gt;When I was growing up, I remember an ad for toys that said wind it up and watch it go. But now you can do a similar kind of thing with data science. You can do this looping back process. This is where computers, the algorithms in them, can engage themselves to create the data they need for machine learning algorithms. It‚Äôs a little bit like the mythical self-consuming snake that comes all the way back around. And the reason this is important is because you need data for training your machine learning algorithms so they can determine how to categorize something or the best way to proceed and having the machines generate it by engaging with themselves is an amazingly efficient and useful way of doing that. There are at least three different versions of this and I‚Äôm giving a little bit of my own terminology here. The first one is what I am calling external reinforcement learning. Now reinforcement learning is a very common term. It means an algorithm that is designed to reach a particular outcome like for instance running through the levels of a game. I‚Äôm calling it external because it‚Äôs focusing on some outside contingency and it‚Äôs this method for example that allowed Google‚Äôs Deep Mind algorithms teach an AI to actually learn on its own how to move through the levels of a video game. It was a major accomplishment to do it with no instruction except for move forward and get to the end. There‚Äôs also generative adversarial networks and they are used a lot in things like generating audio or video or images that seem photorealistic. It‚Äôs both exciting and scary at the same time. The idea here is that one neural network generates an image and that a second neural network tries to determine whether that image is legitimate or whether it‚Äôs been modified in some way. And that if it gets caught with a fake, then the other one has to learn how to do it better. Again, this has gotten to the point where you can have photorealistic face switching in videos. Again, both exciting and scary, but done with what‚Äôs called a generative adversarial network. And then there‚Äôs another kind of reinforcement learning which I‚Äôm calling internal and this is the kind where the algorithm works with itself and the best example of this is Deep Mind again learning to play chess and go and other games by playing millions and millions of games against itself in just some a few hours and mastering the game completely. Now there are a few important benefits to this. One of course is that you can get millions of variations and millions of trials, a gargantuan amount of data very, very quickly. And that sometimes the algorithms can create scenarios that humans wouldn‚Äôt. I mean, something they wouldn‚Äôt even think of or deem possible. And this kind of data is needed for creating the rules that go into the algorithms of machine learning that I‚Äôll talk about later. And the reason this is so important is because the kind of data that you can create so quickly using this method is exactly the kind of data both the variability and the quantity that you need for creating effective rules in machine learning algorithms which is what we‚Äôre going to turn to next.&lt;/p&gt;</content><author><name>Learning Archive</name></author><category term="technology" /><category term="web" /><category term="markdown" /><category term="learning_archive" /><summary type="html">1. Data preparation</summary></entry><entry><title type="html">Ethics and Agency in Data Science</title><link href="/notebook/ethics-and-agency-in-ds" rel="alternate" type="text/html" title="Ethics and Agency in Data Science" /><published>2021-01-01T00:00:03+07:00</published><updated>2021-01-01T00:00:03+07:00</updated><id>/notebook/ethics-and-agency-in-ds</id><content type="html" xml:base="/notebook/ethics-and-agency-in-ds">&lt;h2 id=&quot;legal-ethical-and-social-issues-of-data-science&quot;&gt;Legal, ethical, and social issues of data science&lt;/h2&gt;

&lt;p&gt;Data science can make you feel like a superhero who‚Äôs here to save the world or at least your business‚Äôs world. But an alarming amount of data work can also end up in court or on the wrong side of a protest so I want to talk about a few things that can help keep you, your company and your data science work on the up and up. First, there are some important legal issues. Now it used to be when data science first came about, you know, oh, 10 years ago, we were kind of in the Wild West and people just kind of did what they wanted but now we‚Äôve had some major developments in the legal frameworks that govern data and its use. Probably the most important of these is an entire collection of privacy laws, the most significant of which at the moment is the GDPR, that‚Äôs the European Union‚Äôs General Data Protection Regulation. This is a law about privacy that has some very serious teeth. It can potentially have fines of billions of dollars for companies that seriously violate its policies. This is why you have so many cookie notices and why opting in becomes such an important thing when you go to a website. And in the United States, there are a lot of other regulations that also affect privacy, things like HIPAA, that‚Äôs the Health Insurance Portability and Accountability Act and FERPA, the Family Educational Rights and Privacy Act. And then there are state laws like the California Consumer Privacy Act. All of these place serious regulations on how you can gather data, the consent that you need to get from people, what you can do from the data, whether people can request it, whether they can be forgotten from the system and it‚Äôs important for you to remember all of these so you don‚Äôt end up crossing a very significant line when doing your work. And I want to remind you, it‚Äôs not just the 31 member states of the European Union and the European Economic Area that have these kinds of privacy laws. They‚Äôre spreading around all over the world and there are more coming every day. I mean, that‚Äôs an awful lot of the world that‚Äôs covered by these various regulations that you need to keep in mind when doing your work. The next thing is ethical issues. These may or may not specifically address legal barriers but they have a lot to do with how your work is perceived. Now for instance, there are the three forms of fairness. We talked about distributing things by equity, that is proportional to some kind of input or equality, everybody gets the same amount or need, the people who need the most, and get the most. Or forms of justice. This includes distributive justice which is the actual things that you end up with, procedural justice, how are the decisions made and interactional justice, how is the decision communicated? How are people involved in it? And then there‚Äôs issues of authenticity. You need to know who you‚Äôre dealing with and what you‚Äôre dealing with and none of these specifically address legal issues but they have a very big impact on whether people feel that your organization and your work is ethical and can be trusted and whether they want to engage with you. And that finally gets to the social issues. Whenever you or your company or clients engage with people, you need to engage with them with respect. People don‚Äôt like getting fooled, they don‚Äôt like getting exploited, they don‚Äôt like getting ignored and all of those are serious risks when working with data and don‚Äôt forget that unpopular projects can lead to protests, by the general populace and walkouts by the people in your own company and so, there‚Äôs a lot more that I could say about all of these and in fact, in another course, AI Accountability Essential Training, I do say a lot more about every one of these elements and all of them can have an impact on your ability to use data to do something that‚Äôs productive for your company but not just in the short term, also in the long term in a way that is sustainable and respective of the people and the environment that you work in.&lt;/p&gt;

&lt;h2 id=&quot;agency-of-algorithms-and-decision-makers&quot;&gt;Agency of algorithms and decision-makers&lt;/h2&gt;

&lt;p&gt;When we think about Artificial Intelligence and how it works, and how it might make decisions, and act on it‚Äôs own, we tend to think of things like this. You‚Äôve got the robot holding the computer right next to you. But the fact is, most of the time when we‚Äôre dealing with Artificial Intelligence, it‚Äôs something a lot closer to this. Nevertheless, I want to suggest at least four ways that working data science can contribute to the interplay of human and Artificial Intelligence of personal and machine agency. The first is what I call simple Recommendations. And then there‚Äôs Human-in-the-Loop decision making. Then Human-Accessible decisions, and then Machine-Centric processing and action. And I want to talk a little more about each of these. Let‚Äôs start with Recommendations. This is where the algorithm processes your data and makes a recommendation, or suggestion to you and you can either take it or leave it. A few places where this approach shows up are things like, for instance, online shopping, where you have a recommendation engine that says ‚ÄúBased on your past purchase history, ‚Äúyou might want to look at this.‚Äù Or, the same thing with online movies or music, it looks at what you did, it looks at what you like, and it suggests other things. And, you can decide whether you want to pick up on that or not. Another one is an online News Feed. This says ‚ÄúBased on what you‚Äôve clicked in the past ‚Äúand the things that you‚Äôve selected, ‚Äúyou might like this.‚Äù It‚Äôs a little bit different, because this time it‚Äôs just a yes or no decision. But, it‚Äôs still up to you what you click on. Another one is Maps, where you enter your location and it suggests a route to you based on traffic, based on time and you can follow it or you can do something else if you want. But in all of these, data science is being used to take, truly, a huge amount of information about your own past behavior, about what other people have done under similar circumstances, and how that can be combined to give the most likely recommendations to you. But, the agency still rests in the human. They get to decide what to do. Next is Human-in-the-Loop decision making. And, this is where advanced algorithms can make and even implement their own decisions, as with self-driving cars. And, I remember the first time my car turned it‚Äôs steering wheel on it‚Äôs own. But humans are usually at the ready to take over, if needed. Another example might be something as simple as spam filters. You go in every now and then, and you check up on how well it‚Äôs performing. So, it can do it on it‚Äôs own, but you need to be there to take over just in case. A third kind of decision making in the interplay between the algorithm and the human is what I call Human-Accessible decision making. Many algorithmic decisions are made automatically, and even implemented automatically. But they‚Äôre designed such that humans can at least understand what happened in them. Such as, for instance, with an online mortgage application. You put the information in, and it can tell you immediately whether you‚Äôre accepted or rejected. But because of recent laws, such as the European Union GDPR, that‚Äôs the General Data Protection Regulation, the organizations who run these algorithms need to be able to interpret how it reached its decision. Even if they‚Äôre not usually involving humans in making of the decisions, it still has to be open to humans. And then finally, there‚Äôs Machine-Centric. And this is when machines are talking to other machines. And the best example of this is the internet of things. And that can include things like Wearables. My smart watch talks to my phone, which talks to the internet, which talks to my car in sharing and processing data at each point. Also Smart Homes. You can say hello to your Smart Speaker which turns on the lights, adjusts the temperature, starts the coffee, plays the news and so on. And there are Smart Grids, which allows for example, for two way communication between maybe a power utility and the houses or businesses they serve. It lets them have more efficient routing end of power, recovery from blackouts, integration with consumer generated power, and so on. The important thing about this one is this last category, the Machine-Centric decisions or the internet of things, is starting to constitute an enormous amount of the data that‚Äôs available for data science work. But any one of these approaches from the Simple Recommendations up to the Machine-Centric, all of them show the different kinds of relationships between data, human decision makers, and machine algorithms, and the conclusions that they reach. Any one of these is going to work in different circumstances. And so it‚Äôs your job, as somebody who may be working in data science, to find the best balance of the speed and efficiency of machine decision making and respect for the autonomy and individuality of the humans that you‚Äôre interacting with.&lt;/p&gt;</content><author><name>Learning Archive</name></author><category term="technology" /><category term="web" /><category term="markdown" /><category term="learning_archive" /><summary type="html">Legal, ethical, and social issues of data science</summary></entry><entry><title type="html">The place of Data Science</title><link href="/notebook/the-place-of-data-science" rel="alternate" type="text/html" title="The place of Data Science" /><published>2021-01-01T00:00:02+07:00</published><updated>2021-01-01T00:00:02+07:00</updated><id>/notebook/the-place-of-data-science</id><content type="html" xml:base="/notebook/the-place-of-data-science">&lt;h2 id=&quot;artificial-intelligence&quot;&gt;Artificial intelligence&lt;/h2&gt;

&lt;p&gt;At this exact moment in history, when people think about data science, the mind turns inexorably towards artificial intelligence, often with humanoid robots lost deep in thought. But before I compare and contrast data science and AI, I want to mention a few things about the nature of categories and definitions. First, categories are constructs, and by construct I mean something that you have to infer, something that is created in the mind, doesn‚Äôt have this essential existence. It‚Äôs a little bit like, when is something comedy and when is something performance art, and when is something acting? There‚Äôs nothing that clearly separates one from the other. These are all mental categories, and the same thing is true of any category or definition, including things like data science and AI. The second one is that categories serve functional purposes. A letter opener is anything that‚Äôs used to open letters. I actually use a knife to open letters. On the other hand, I know a family that uses knives to scoop ice cream exclusively. And, the tool is whatever you use it for. It‚Äôs defined by its utility. The same thing is true of categories. And then finally, the use of categories varies by needs. If you‚Äôre putting books on the shelf, you can use the Library of Congress system, the Dewey Decimal system. I know people who stack them by size or by color, or turn them around and do it decoratively. Any of those is going to work because they‚Äôre serving different purposes. And so, when we‚Äôre trying to think about categories and defining whether a particular activity is AI or whether it‚Äôs data science, all of these principles are going to apply. A good example of this is the question of whether tomatoes are fruits or vegetables. Everyone knows that tomatoes are supposed to be fruit, but everyone also knows you‚Äôd never put tomatoes in a fruit salad. Tomatoes go on veggie plates along with carrots and celery. The answer to this paradox, its fruit versus vegetable nature, is simple. The word fruit is a botanical term, and the word vegetable is a culinary term. They‚Äôre not parallel or even very well coordinated systems of categorizations, which is why confusion can arise. It‚Äôs a little like the joke about the bar that plays both kinds of music, country and western. The categories don‚Äôt always divide logically or exclusively, and the same is true for artificial intelligence and data science. So, what exactly is artificial intelligence? Well I‚Äôm going to let you know, there are a lot of different statements about this, and none of them are taken as definitive. And some of them I find to be useful, and some of them I find to be less useful. There‚Äôs a little joke that AI means anything that computers can‚Äôt do. Well, obviously, computers learn to do new things, but as soon as a computer learns how to do something, people say, well that‚Äôs not intelligent, that‚Äôs just a machine doing stuff. And so there‚Äôs a sort of moving target here to this particular definition in terms of things computers can‚Äôt do. You can also think of AI in terms of tasks that normally require humans. Like placing a phone call and making an appointment. Or like returning an email, or categorizing text. Traditionally humans have done that, but when a machine is able to do that, when a program‚Äôs able to do it, that‚Äôs probably a good example of artificial intelligence. Probably the most basic and useful definition is that artificial intelligence refers to programs that learn from data. And so, you give them some data, they build a model, and that that model adapts over time. A few common examples of this are things like categorizing photos. Is this a photo of a horse, a car, a balloon, a person? And programs learn how to do this by first having lots and lots, and lots, and lots of photos that are labeled by the people as one thing or another, as a cat or a dog. But then the algorithm is able to start learning on its own and abstracting the elements of the photo that best represent cat or dog. It‚Äôs also used for translations going from one language, like English, to another, like French. The use of artificial intelligence programs has made enormous leaps in the ability of computers to do this automatically. Another one is games, like Go here. It was a very big deal when not very long ago a computer was able to beat the world champion of Go. And, it was thought to be this intuitive game that couldn‚Äôt really be explained. What‚Äôs fascinating about that is the computer actually taught itself how to play go. And, we‚Äôll talk a little more about that when we talk about the derivation of rules in another video. But all three of these can be good examples of artificial intelligence, simply by the sorts of things it‚Äôs able to do. And so, probably, this is the best working definition of AI. And, while it can include even simple regression models, which really don‚Äôt require much in the way of computing power, it usually refers to two approaches in particular. AI is usually referring to machine learning algorithms, and in particular, deep learning neural networks. I‚Äôm going to talk about those more elsewhere, but I did want to bring up one more important distinction when talking about AI. And that‚Äôs the difference between what is called strong or general AI, which is the idea that you can build a computer replica of the human brain that can solve any cognitive task. This is what happens in science fiction. You have a machine that‚Äôs just like a human in a box. And that was the original goal of artificial intelligence back in the 50s, but it turned out that has been very difficult. Instead, you also have what is called weak or narrow, or specific, or focused AI. And these are algorithms that focus on one specific well defined task. Like, is this a photo of a cat or a photo of a dog? That has been where the enormous progress in AI has been over the last several years. So with all this in mind, how does artificial intelligence compare and contrast to data science? Well, it‚Äôs a little bit like the fruit versus vegetable conundrum. Artificial intelligence means algorithms that learn from data. Broadly speaking, there‚Äôs an enormous amount of overlap between our concept of AI and the field of machine learning. Data science on the other hand is the collection of skills and techniques for dealing with challenging data. You can see that these two are not exclusive. There‚Äôs a lot of overlap between them, and AI nearly always involves the data science skillset. You basically can‚Äôt do modern AI without data science. But there‚Äôs an enormous amount of data science that does not involve artificial intelligence. And I‚Äôll say more about that as we go on in this course. If you want to draw a diagram, I personally think of it this way. If this is data science, here‚Äôs machine learning, ML. There‚Äôs a lot of overlap between those two, and then within machine learning there‚Äôs a specific approach called neural networks. Those have been amazingly productive, and AI refers to this diffuse, not well defined category that mostly overlaps with neural networks and with machine learning. And, it gets at some of the ambiguities, and some of the difficulty in separating these, which is why there‚Äôs no consistent definition, and why there‚Äôs so much debate over what one thing is, and what the other one is. But I will say this. Artificial intelligence has been enormously influential within the field of data science recently, even though data science has many other things that it does. This course focuses specifically on data science, but you‚Äôll see just how much of this information also applies to machine learning, to neural networks, and even to the field of artificial intelligence.&lt;/p&gt;

&lt;h3 id=&quot;machine-learning&quot;&gt;Machine learning&lt;/h3&gt;

&lt;p&gt;Back in the day a machine was just a machine. It did whatever machine things it did like stamping metal or turning a propellor or maybe washing your clothes with a fair amount of help on your part. But nowadays, machines have to do more than just their given mechanical function. Now a washing machine‚Äôs supposed to be smart. It‚Äôs supposed to learn about you and how you like your clothes and it‚Äôs supposed to adjust its functions according to its sensors and it‚Äôs supposed to send you a gentle message on your phone when it‚Äôs all done taking care of everything. This is a big change, not just for washing machines, but for so many other machines and for data science processies as well. This gets to the issue of machine learning and a very simple definition of that is the ability of algorithms to learn from data and to learn in such a way that they can improve their function in the future. Now, learning is a pretty universal thing. Here‚Äôs how humans learn. Humans, memorization is hard. I know this, I teach and memorization is something my students struggle with every semester. On the other hand, spotting patterns is often pretty easy for humans as is reacting well and adaptively to new situations that resemble the old ones in many but not all ways. On the other hand, the way that machines learn is a little bit different. Unlike humans, memorization is really easy for machines. You can give them a million digits, it‚Äôll remember it perfectly and give it right back to you. But for a machine, for an algorithm, spotting patterns, in terms of here‚Äôs a visual pattern, here‚Äôs a pattern over time, those are much harder for algorithms. And new situations can be very challenging for algorithms to take what they learned previously and adapt it to something that may differ in a few significant ways. But the general idea is that once you figure out how machines learn and the ways that you can work with that, you can do some useful things. So for instance, there‚Äôs the spam email and you get a new email and the algorithm can tell whether it‚Äôs a spam. I used a couple of different email providers and I will tell you, some of them are much better at this than others. There‚Äôs also image identification. For instance, telling whether this is a human face or who‚Äôs face it is. Or there‚Äôs the translation of languages where you enter text, either written or spoken, and it translates it back. A very complicated task for humans but something that machines have learned how to do much better than they used to. Still not 100% but getting closer all the time. Now, the important thing here is that you‚Äôre not specifying all the criteria in each of these examples and you‚Äôre not laying out a giant collection of if this then that statements in a flow chart. That would be something called an expert system. Those were created several decades ago and have been found to have limited utility and they‚Äôre certainly not responsible for the modern developments of machine learning. Instead, a more common approach it really is to just teach your machine. You train it and the way you do that is you show the algorithm millions of labeled examples. If you‚Äôre trying to teach it to identify photos of cats versus other animals, you give it millions of photos and you say this is a cat, this is a cat, this is not, this is not, this is. And then the algorithm finds its own distinctive features that are consistent across many of the examples of cats. Now what‚Äôs important here is that the features, the things in the pictures that the algorithm latches onto may not be relevant to humans. We look at things like the eyes and the whiskers and the nose and the ears. It might be looking at the curve on the outside of the cheek relevant to the height of one ear to another. It might be looking just at a small patch of lines around the nose. Those may not be the things that humans latch onto and then sometimes they‚Äôre not even visible to humans. It turns out that algorithms can find things that are very subtle, pixel by pixel changes in images or very faint sounds in audio patches or individual letters in text and it can respond to those. That‚Äôs both a blessing and a curse. It means that it can find things that humans don‚Äôt but it also can react in strange ways occasionally. But once you take all this training, you give your algorithm millions of labeled examples and it starts classifying things, well, then you want use something like a neural network which has been responsible for the major growth in machine learning and data science in the past five years or so. These diagrams here are different layouts of possible neural networks that go from the left to the right, some of them circle around or they return back to where they were. But all of these are different ways of taking the information and processing it. Now, the theory of neural networks or artificial neural networks has existed for years. The theory is not new. What‚Äôs different, however, is that computing power has recently caught up to the demands that the theory places and in addition, the availability of labeled data primarily thanks to social media has recently caught up too. And so now we have this perfect combination. The theory has existed but the computing power and the raw data that it needs have both arrived to make it possible to do these computations that in many ways resembles what goes on in the human brain and then allow it to think creatively about the data, find its own patterns and label things. Now, I do want to say something about the relationship between data science and machine learning. Data science can definitely be done without machine learning. Any traditional classification task, logistic regression, decision tree. That‚Äôs not usually machine learning and it‚Äôs very effective data science. Most predictive models or even something like a sentiment analysis of social media text. On the other hand, machine learning without data science, well, you know, not so much. It‚Äôs possible to do machine learning without extensive domain expertise so that‚Äôs one element of data science. On the other hand, you would nearly always want to do this in collaboration with some sort of topical expert. Mostly I like to think of machine learning as a subdiscipline of data science. And that just brings up one more thing I want to say. The neural networks and the deep learning neural networks in particular that have been responsible for nearly all of these amazing developments in machine learning are a little bit of a black box which means it‚Äôs hard to know exactly what the algorithm is looking at or how it‚Äôs processing the data and one result of that is it kind of limits your ability to interpret what‚Äôs going on even though the predictions in classifications can be amazingly accurate. I‚Äôll say more about neural networks and these issues elsewhere, but they highlight the trade-offs, the potential and the compromises that are inherent in some of these really exciting developments that have been taking place in one extraordinary influential part of the data science world.&lt;/p&gt;

&lt;h3 id=&quot;deep-learning-neural-networks&quot;&gt;Deep learning neural networks&lt;/h3&gt;

&lt;p&gt;If you‚Äôve ever been around a baby, you know that babies take very little steps. But the thing about baby steps is that you still get moving and eventually, babies grow and they take bigger steps. And before you know it, you‚Äôve got a world-class sprinter. And there‚Äôs a similar thing, I like to think, that happens with neural networks. And what happens here is that tiny steps with data can lead to amazing analytical results. Now, an artificial neural network in computing is modeled roughly after the neurons that are inside a biological brain. Those neurons are nothing more than simple on and off switches that are connecting with each other, but give rise to things like love and consciousness. In the computing version, the idea is to take some very basic pieces of information, and by connecting it with many other nodes, you can give rise to the sort of emergent behavior, which really is very high-level cognitive decisions and classifications. It works this way. Over here on the left, you start with an input layer. That‚Äôs where your raw data comes in. And then, it gets passed along to one or more hidden layers. That‚Äôs what makes it a neural network, that you have these hidden layers. And these lines all represent connections like the connections between neurons in a biological brain. And then, after going through several hidden layers, you have an output layer, which is where you get the final classification or decision about what‚Äôs happening. And I want to give you an example of how this might work. Now, please understand, this is an analogy. The actual operation of neural networks is much more complicated and sometimes a little more mysterious than what‚Äôs going on here. But let‚Äôs take a simple example where you‚Äôre taking your input data from a digital image. In that case, for each pixel in the image, you‚Äôre going to have basically five pieces of information. You‚Äôre going to have the X and Y coordinates of that pixel. And then, for that pixel, you‚Äôre going to have its red, green, and blue color components. And then you‚Äôre going to repeat these five things for every pixel in the image. But that‚Äôs your raw input data. Those are numerical values. And you put those into the input layer. And then, what it does is it starts to combine these different X and Y positions and the RGB colors, and then, it decides whether it has found a line. Does this represent a distinct line against a color background? So, that might be the first layer. And then, from there, it‚Äôs going to say, I‚Äôve found some lines, and now I‚Äôm going to see if I can combine those lines to determine whether that line is the edge of an object as opposed to some sort of service marker. And then, if I found edges, I can then take the information about edges and then combine that to determine what‚Äôs the shape that I‚Äôm looking at. Is it a circle, a square, a hexagon, or something much more complex than that? And then maybe, it takes all of this shape information and then it goes to the output layer, and it says what the actual object is. So, we‚Äôve gone from the X Y RGB pieces of information about each pixel, and we‚Äôve put that in. And we‚Äôve gone to lines and we‚Äôve gone to edges, and then to shapes, and then possibly to objects. That‚Äôs one idea of how a neural network, and especially a deep-learning neural network, which has many hidden layers, might work. If you want to see something that‚Äôs slightly more complicated, here‚Äôs an example. And neural networks can potentially have millions of neurons. There can be a lot of stuff going on in there. And they might be arranged in a lot of different ways. These are called feedforward ones, where the information starts on the left and just kind of keeps moving forward to the right. But there are a lot of other potential arrangements for the data transfers within a neural network. These are some of the possible examples. They behave slightly differently. You‚Äôll want to use some of the different versions in different circumstances. Fortunately, we have other entire courses dedicated to the design and implementation of artificial neural networks here. There is one interesting thing, though. Just like a human brain, things can get a little complicated in a neural network, or really massively complicated. And it can be hard to know exactly what it is that‚Äôs going on inside there. What that means is you actually have to resort to inference. You sometimes have to infer how the neural network is functioning. What is it looking at? How is it processing that information? And curiously, that means you actually have to use some of the same methods as psychological researchers who are trying to find out what‚Äôs going on inside a human brain. You are testing and then inferring the processes that are going on. One other really important thing to keep in mind about neural networks is there‚Äôs a collection of legal issues that apply to these that don‚Äôt quite apply the same way to other forms of machine learning or data science. For instance, the European Union‚Äôs General Data Protection Regulation, better known as just GDPR, is a collection of policies that govern privacy and really how organizations gather and process information that they get from people. One really important part of this that relates to neural networks is what‚Äôs called a right to explanation. If a person feels that they have been harmed by a decision made by a neural network, such as it refused a loan application, they can sue the organization and demand an explanation. How did it reach that process? Now, because neural networks are so complicated, they tend to be kind of opaque and it‚Äôs hard to know what‚Äôs going on. You may have a difficult situation explaining exactly how it got there. That‚Äôs a problem, because there are some very stiff fines associated with violations of the GDPR. So, you will want to get a little more information on this. Fortunately, we have another course called AI Accountability Essential Training, which addresses some of these issues, gives you the basic parameters. If you‚Äôre going to be using neural networks, you owe it to yourself to spend a little bit of time on the social context and on the legal context of how these things work. But the most exciting thing about them is the amazing progress that‚Äôs been made in machine learning over just the past few years with neural networks and deep-learning neural networks, in particular, to model the general processes going on in the human brain, and to be able to reach some very, very sophisticated and a highly accurate conclusions based on that processing.&lt;/p&gt;

&lt;h2 id=&quot;big-data&quot;&gt;Big data&lt;/h2&gt;

&lt;p&gt;There was a time just a few years ago when data science and big data were practically synonymous terms as were semi-magical words like Hadoop that brought up all the amazing things happening in data science. But things are a little different now, so it‚Äôs important to distinguish between the two fields. I‚Äôll start by reminding you what we‚Äôre talking about when we talk about big data. Big data is data that is characterized by any or all of three characteristics. Unusual volume, unusual velocity, and unusual variety. Again, singly or together can constitute big data. Let me talk about each of these in turn. First, volume. The amount of data that‚Äôs become available even over the last five years is really extraordinary. Things like customer transactions at the grocery store. The databases that track these transactions and compile them and consumer loyalty programs have hundreds of billions of rows of data on purchases. GPS data from a phone includes information from billions of people constantly throughout the day. Or scientific data, for instance, this image of the black hole in Messier 97 from the Event Horizon Telescope that was released in April of 2019. It involved half a ton of hard drives that had to be transported on airplanes to central processing locations because that was several times faster than trying to use the internet. Any one of these is an overwhelming dataset for normal method, and that brought about some of the most common technologies associated with the big data, distributed file systems like Hadoop, that made it possible to take these collections that were simply too big to fit on any one computer, any one drive, put it across many, and still be able to integrate them in ways that let you get collective intelligence out of them. Then there‚Äôs velocity. The prime culprit in this one is social media. YouTube gets 300 hours of new video uploaded every single minute. That gets about five billion views per day. Instagram had 95 million posts per day, and that was back in 2016 when it only had half as many users as it does not. And Facebook generates about four petabytes of data per day. The data is coming in so fast it‚Äôs a fire hose that no common methods that existed before the big data revolution could handle it. This required new ways of transporting data, integrating data, and being able to update your analyses constantly to match the new information. And then finally there‚Äôs the variety, probably one of the most important elements of big data. That included things like multimedia data, images, and video, and audio. Those don‚Äôt fit into spreadsheets. Or biometric data, facial recognition, your fingerprints, your heart readings, and when you move the mouse on your computer to find out where the cursor went, that‚Äôs a distinctive signature that‚Äôs recorded and identified for each user. And then there‚Äôs graph data. That‚Äôs the data about the social networks and the connections between people. That requires a very special kind of database. Again, doesn‚Äôt fit into the regular rows and columns of conventional dataset. So all of these showed extraordinary challenges for simply getting the data in, let alone knowing how to process it in useful ways. Now it is possible to distinguish big data and data science. For instance, you can do big data without necessarily requiring the full toolkit of data science, which includes computer programming, math and statistics, and domain expertise. So for instance, you might have a large dataset, but if it‚Äôs structured and very consistent, maybe you don‚Äôt have to do any special programming. Or you have streaming data. It‚Äôs coming in very fast. But only has a few variables, a few kinds of measurements. Again, you can set it up once and kind of run with it as you go. And so that might be considered big data, but doesn‚Äôt necessarily require the full range of skills of data science. You can also have data science without big data. And that‚Äôs anytime you have a creative combination of multiple datasets or you have unstructured texts like social media posts. Or you‚Äôre doing data visualization. You may not have large datasets with these, but you‚Äôre definitely going to need the programming ability and the mathematical ability as well as the topical expertise to make these work well. So now that I‚Äôve distinguished them I want to return to one particularly important question. You can find this on the internet. And the question is, is big data dead? Because it‚Äôs interest peaked about four or five years ago. And it looks like it‚Äôs been going down since then. So is big data passe? Is it no longer there? Well, it‚Äôs actually quite the opposite. It turns out that big data is alive and well. It‚Äôs everywhere. It has simply become the new normal for data. The practices that it introduced, the techniques that it made possible are used every single day now in the data science world. And so while it‚Äôs possible to separate big data and data science, the two become so integrated now that big data is simply taken for granted as an element of the new normal in the data world.&lt;/p&gt;

&lt;h2 id=&quot;predictive-analytics&quot;&gt;Predictive analytics&lt;/h2&gt;

&lt;p&gt;When a person is convicted of a crime, a judge has to decide what the appropriate response is and how that might help bring about positive outcomes. One interesting thing that can contribute to that is what‚Äôs called restorative justice. This is a form of justice that focuses on repair to the harm done as opposed to punishment, and it often involves, at the judge‚Äôs discretion and the victim‚Äôs desire, mediation between the victim and the offender. Now one of the interesting things about this is it‚Äôs a pretty easy procedure, and it has some very significant outcomes. Participating in restorative justice predicts improved outcomes on all of the following. People feel that they were able to tell their story and that their opinion was considered. They feel that the process or outcome was fair. They feel that the judge or mediator was fair. They feel that the offender was held accountable. An apology or forgiveness was offered. There‚Äôs a better perception of the other party at the end of all of this. The victim is less upset about the crime. The victim is less afraid of revictimization. Those are absolutely critical. And then one more is that there‚Äôs a lower recidivism rate. Offenders who go through restorative justice are less likely to commit crimes again in the future. All of these are very significant outcomes and can be predicted with this one relatively simple intervention of restorative justice. And so when a judge is trying to make a decision, this is one thing they can keep in mind in trying to predict a particular outcome. Now in the world of predictive analytics, where you‚Äôre using data to try to predict outcomes, the restorative justice is a very simple one based on simple analysis. Within data science and predictive analytics, you‚Äôll see more complicated things like, for instance, whether a person is more likely to click on a particular button or make a purchase based on a particular offer. You‚Äôre going to see medical researchers looking at things that can predict the risk of a disease as well as the responsiveness of particular treatments. You‚Äôll also look at things like the classification of photos, and what‚Äôs being predicted there is whether a machine can accurately predict what a human would do if they did the same particular task. These are all major topics within the field of predictive analytics. Now the relationship between data science and predictive analytics is very vaguely like this. Data science is there, predictive analytics is there, and there‚Äôs a lot of overlap. An enormous amount of the work in predictive analytics is done by data science researchers. There are a few important meeting points at that intersection between the two, so predictions that involve difficult data, if you‚Äôre using unstructured data like social media posts or a video that doesn‚Äôt fit into the nice rows and columns of a spreadsheet. You‚Äôre probably going to need data science to do that. Similarly, predictions that involve sophisticated models like the neural network we have here, those require some really high-end programming to make them happen. And so data science is going to be important to those particular kinds of predictive analytics projects. On the other hand, it‚Äôs entirely possible to do predictions without the full data science tool kit. If you have clean, quantitative data sets, nice rows and columns of numbers, then you‚Äôre in good shape. And if you‚Äôre using a common model like a linear regression or a decision tree, both of which are extremely effective, but they‚Äôre also pretty easy to do and pretty easy to interpret. So in these situations, you can do useful and accurate predictions without having to have the entire background of data science. Also, it‚Äôs possible to do data science without necessarily being involved in the business of predictions. If you‚Äôre doing things like clustering cases or counting how often something happens, or mapping like what we see here, or a data visualization, these can be significant areas of data science, depending both on the data that you‚Äôre bringing in and the methods that you‚Äôre using. But they don‚Äôt involve predictions per se, and so what this lets you know is that while data science can contribute significantly to the practice of predictive analytics, they are still distinguishable fields, and depending on your purposes, you may or may not need the full range of data science skills, the full took kit to get to your predictive purposes. But either way, you‚Äôre going to be able to get more insight into how people are likely to react and how you can best adapt to those situations.&lt;/p&gt;

&lt;h2 id=&quot;prescriptive-analytics&quot;&gt;Prescriptive analytics&lt;/h2&gt;

&lt;p&gt;Sometimes you just have to do the impossible. About 2500 years ago the Greek philosopher, Zeno of Elea, argued that it was impossible to get from point a to point b, like walking across your room. His reasoning was that before you could get all the way to point b, you first had to get halfway there. And before you could get the rest of the way you had to go halfway again. The process of getting halfway would occur an infinite number of times, which Zeno said was impossible. So you could never get to point b. Now aside from the fact that Zeno didn‚Äôt know that you could solve an infinite series problem with calculus, the obvious answer is that people walk from one part of the room to the other all the time so the theoretically impossible task was obviously possible and accomplished frequently. And that gets us to an interesting issue about cause and effect relationships. Now strictly speaking, three things need to happen to be able to say one thing causes another. The first is that there needs to be an observed correlation between the putative cause and the effect. That is, the effect needs to be more likely when the cause is present. If it‚Äôs not it can‚Äôt possible cause it. The second thing is temporal precedence, and that simply means that the cause needs to come before the effect if it‚Äôs going to be a cause. And both of those are pretty easy to establish. The first one you just need something like a correlation coefficient. The second one, you just need to show that the cause happened first. You can set that up pretty easily. But the third one‚Äôs the kicker. No other explanations for the association between the possible cause and effect. The connection between those two can‚Äôt be accounted for by anything else. The problem is, that part is theoretically impossible. You can‚Äôt show that there‚Äôs nothing else there. And so, while we go along pretty well with number one, number two, three is this huge sticking point. On the other hand, that doesn‚Äôt mean you can‚Äôt establish causality, it means you just kind of have to get close enough for practical purposes. Now let me go back and compare what I‚Äôm talking about here with cause and effect to something we‚Äôve seen previously. I‚Äôve spoken about predictive analytics. That is where you‚Äôre focusing on correlations because correlations are adequate for saying if this happens then this will probably happen as well. And there‚Äôs a huge amount of work in data science on predictive analytics, and really amazing things have come out of that. On the other hand, prescriptive analytics is about causation. You‚Äôre trying to specifically focus on things that you can do to make something happen that‚Äôs important to you. Now, the gold standard for establishing cause and effect is what‚Äôs called an RCT, or a randomized controlled trial. Theoretically they‚Äôre very simple. You assign a bunch of people to one situation or another. You do that randomly. You control all the other conditions, and then you see how things come out. Theoretically very simple to do, but I can tell you, given my training as an experimental research psychologist, they can be enormously difficult, often complex in practice. And so the theory is nice and clean, but the practice can be really difficult. There is one major exception to that, and that‚Äôs A/B testing for web design, where for instance, you set up your software to have one offer on this version and another offer on another version of your website and you see which one gets more clicks. That can be automated. And it is an experimental design and it‚Äôs randomized. It‚Äôs an example of what we‚Äôre looking for even though that‚Äôs a very simple one. But something more complex like that, like for instance, does making public transportation in the city have a direct effect on the influx of new businesses? That‚Äôs a huge experiment. That‚Äôs very, very difficult to do well. And so the gold standard is the randomized controlled trial, but often very difficult to do in reality. And that leads you to some of the more practical solutions, the alternatives that can help you get close to a cause and effect conclusion even if they can‚Äôt get to you 100% of the way. Those include things like what-if simulations. These are ways of manipulating data in a spreadsheet. They say, well, if this is true and if we have these parameters then what will we expect? And then you can simply see how that matches up with reality a little bit later. You can do optimization models. These are correlational models based on the information you have that say if we balance things out, so we spend a certain amount of time and money on this, a certain amount of time and money on this, or if we price things in a particular way, that will maximize an outcome. Again, it‚Äôs correlational, but it often gives you specific suggestions on what to do based on that past information. You can do what are called cross-lag correlations. This is where you have data at two or more specific points in time, and you‚Äôre able to see if changes in the cause at time one produce corresponding changes in the effect at time two and not vice versa. And then finally there‚Äôs the entire category of what are called quasi-experiments. These are a whole host of research designs that let you use correlational data to try to estimate the size of the causal relationship between the two variables. On the other hand, one of the easiest ways to isolate causality is simply to do things again and again. Iteration is critical. You may be familiar with this chart, which comes from the agile design process. You design, you develop, you try something at once. Well, test it and do it again. Make a variation, do it again, make a variation. And as you do that you will come close enough to causality through your repeated experience that you‚Äôll then be able to isolate and say this particular action is producing the cause that we want. That is the prescriptive analysis. That‚Äôs the result that you‚Äôre looking for. And now, let me say something about how prescriptive analytics and data science compare and contrast with one another. Specifically, you can have prescriptive analytics without requiring the full data science toolkit. If you‚Äôre doing experimental research and you have well-structured data. It‚Äôs nice and quantitative, you got complete data, and that includes most automated A/B experiments. You can do a very good prescriptive analysis without needing everything that goes into data science. On the other hand, there are times where you‚Äôre doing data science without necessarily trying to prescribe a particular plan of action. Predictive and descriptive work flow into that category. That includes things like classifying and clustering, doing trend analysis, identifying anomalies. And so, that‚Äôs when data science doesn‚Äôt need prescriptive analytics as opposed to when prescriptive analytics doesn‚Äôt need data science. And so they are distinguishable fields. On the other hand, I do want to finish with this one thing about causality which is so central to prescriptive analytics. Causality may be, at least in theory, impossible. But prescriptive analytics can get you close enough for any practical purposes and help put you and your organization on the right path to maximizing the outcomes that are most important to you.&lt;/p&gt;

&lt;h2 id=&quot;business-intelligence&quot;&gt;Business intelligence&lt;/h2&gt;

&lt;p&gt;It‚Äôs an article of faith for me that any organization will do better by using data to help with their strategy, and with their day-to-day decisions. But it reminds me of one of my favorite quotes from over 100 years ago. William James was one of the founders of American psychology and philosophy, and he‚Äôs best known for functionalism in psychology and pragmatism in philosophy, and he had this to say: he said, ‚ÄúMy thinking is first and last and always for the sake of my doing.‚Äù That was summarized by another prominent American psychologist, Susan Fiske, as, ‚ÄúThinking is for doing.‚Äù The point is, when we think, the way that our brain works, it‚Äôs not just there because it‚Äôs there, it‚Äôs there to serve a particular purpose. And I think the same thing is true about data and data science in general. In fact, I like to say data is for doing. The whole point of gathering data, the whole point of doing the analysis, is to get some insight that‚Äôs going to allow us to do something better. And truthfully, business intelligence is the one field that epitomizes this goal. Business intelligence, or B.I., is all about getting the insight to do something better in your business. And business intelligence methods, or B.I. methods, are pretty simple. They are designed to emphasize speed, and accessibility, and insight, right there. You can do them on your tablet, you can do them on your phone. And they often rely on structured dashboards, like these graphs that you see. Maybe you do a social media campaign, and you can go and see the analytics dashboard. Or you have videos on YouTube, or Vimeo, or someplace. You can get the analytics and see how well is this performing, who‚Äôs watching it and when. That‚Äôs a business intelligence dashboard of a form. So, if this is all about the goal of data, that data is for doing, and B.I. does that so well, where does data science come in to all of this? Well, it actually comes in sort of before the picture. Data science helps set things up for business intelligence, and I‚Äôll give you a few examples. Number one, data science can help tremendously in collecting, and cleaning, and preparing, and manipulating the data. In fact, some of the most important developments in business intelligence, say, for instance, companies like Domo. Their major property is about the way that they ingest and process the information to make it easily accessible to other people. Next, data science can be used to build the models that predict the particular outcomes. So, you will have a structure there in your data, that will be doing, for instance, a regression, or a decision tree, or some other model to make sense of the data. And while a person doesn‚Äôt have to specifically manipulate that, it‚Äôs available to them, and that‚Äôs what produces the outcomes that they‚Äôre seeing. And then finally, two of the most important things you can do in business intelligence are find trends, to predict what‚Äôs likely to happen next, and to flag anomalies. This one‚Äôs an outlier, something may be wrong here, or we may have a new case with potential hidden value. Any one of those is going to require some very strong data science to do it well. Even if the user-facing element is a very simple set of graphs on a tablet, the data science goes into the preparation and the offering of the information. And so really, I like to think of it this way: Data science is what makes business intelligence possible. You need data science to get the information together from so many different sources, and sometimes doing complex modeling. And also, I like to think, that business intelligence gives purpose to data science. It‚Äôs one of the things that helps fulfill the goal-driven, application-oriented element of data science. And so, data science makes B.I. possible, but B.I. really shows to the best extent how data science can be used to make practical decisions that make organizations function more effectively and more efficiently.&lt;/p&gt;</content><author><name>Learning Archive</name></author><category term="technology" /><category term="web" /><category term="markdown" /><category term="learning_archive" /><summary type="html">Artificial intelligence</summary></entry></feed>