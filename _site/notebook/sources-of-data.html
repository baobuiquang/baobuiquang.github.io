<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sources of Data</title>
    <meta name="description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
    <link rel="shortcut icon" href="../favicon.png">
    <!-- Jekyll -->
    
    





    
    <!-- PWA -->
    <link rel="manifest" href="../manifest.json">
    <meta name="theme-color" content="#ff0000" />
    <link rel="apple-touch-icon" href="../assets/apple-icon-180.png">
    <!-- <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"> -->
    <meta name="apple-mobile-web-app-status-bar-style" content="default">

    <!-- Fonts -->
    <link
        href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"
        rel="stylesheet">
    <link
        href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap"
        rel="stylesheet">
    <!-- Stylesheet -->
    <!-- <link href="../creatoper/creatoper.css" rel="stylesheet"> -->
    <link href="https://unpkg.com/creatoper@1.1.5/creatoper.css" rel="stylesheet">
    <link href="../creatoper/creatoper-control.css" rel="stylesheet">
    <link href="../styles.css" rel="stylesheet">
</head>

<body>
    <!-- Scroll To Top  -->
<div class="just-display-on-desktop">
    <button onclick="scrollToTop()" id="scrolltotop" class="just-display-on-desktop"
        style="transform: rotate(38deg);">&nwarr;</button>
</div>
    <div class="top-nav">
    <div class="name">
        <a href="/" class="nav-link">
            BUI QUANG BAO</a>
    </div>
    <div class="nav-btn">
        <label for="modal-1" style="cursor: pointer;">
            <img src="../assets/icon_feather/menu.svg" height="36px" class="svg-f" alt="menu">
        </label>
    </div>
    <div class="nav-link-wrap just-display-on-desktop">
        <a href="/notebook" class="nav-link">
            NOTEBOOK</a>
    </div>
</div>
<div class="menu">
    <input class="modal-state" id="modal-1" type="checkbox">
    <div class="modal">
        <label class="modal__bg" for="modal-1"></label>
        <div class="modal__inner">
            <label class="modal__close" for="modal-1">
                <img src="../assets/icon_feather/x.svg" height="40px" style="filter: invert(1.0); width: initial;"
                    alt="close">
            </label>
            <!-- Custom in modal -->
            <div class="modal-top">
                <div class="nav-link menu-text">
                    MENU
                </div>
            </div>
            <div class="modal-cta just-display-on-desktop">
                COVID-19 Virus Analysis and Diagnosis
                <a target="_blank" href="https://github.com/buiquangbao/COVID-19-Virus-Analysis-and-Diagnosis">
                    <button>
                        READ FULL ARTICLE
                    </button>
                </a>
            </div>
            <div class="modal-links">
                <div class="a-wrap">
                    <a href="/">
                        Bui Quang Bao
                    </a>
                </div>
                <div class="a-wrap">
                    <a href="/notebook">
                        Notebook
                    </a>
                </div>
                <div class="a-wrap">
                    <a href="https://buiquangbao.github.io/#experience">
                        Experiences
                    </a>
                </div>
                <div class="a-wrap">
                    <a href="https://buiquangbao.github.io/#projects">
                        Projects
                    </a>
                </div>
                <div class="a-wrap">
                    <a href="https://buiquangbao.github.io/#awards">
                        Awards
                    </a>
                </div>
                <div class="a-wrap">
                    <a href="https://buiquangbao.github.io/#certificates">
                        Certificates
                    </a>
                </div>
                <div class="a-wrap">
                    <a href="https://buiquangbao.github.io/#education">
                        Education
                    </a>
                </div>
                <div class="a-wrap">
                    <a href="https://buiquangbao.github.io/#contact">
                        Contact
                    </a>
                </div>
            </div>
            <div class="modal-bottom">
                <div class="modal-views">
                    <span class="fhs">
                        <script language="JavaScript">var fhsh = document.createElement('script'); var fhs_id_h = "3360434";
                            fhsh.src = "//freehostedscripts.net/ocount.php?site=" + fhs_id_h + "&name=&a=1";
                            document.head.appendChild(fhsh); document.write("<span id='h_" + fhs_id_h + "'></span>");
                        </script>
                    </span>
                    TOTAL VIEWS |
                    <a style="font-weight: 500;" href="/">
                        11
                    </a>
                    POSTS | SINCE 2020
                </div>
                <div class="modal-contact">
                    <div class="mobile-row-6">
                        <div class="col">
                            <a href="https://github.com/buiquangbao" target="_blank">
                                <img src="../assets/icon_feather/github.svg" class="svg-f" alt="github">
                            </a>
                        </div>
                        <div class="col">
                            <a href="https://codepen.io/buiquangbao" target="_blank">
                                <img src="../assets/icon_feather/codepen.svg" class="svg-f" alt="codepen">
                            </a>
                        </div>
                        <div class="col">
                            <a href="https://www.npmjs.com/package/creatoper" target="_blank">
                                <img src="../assets/icon_feather/package.svg" class="svg-f" alt="npm">
                            </a>
                        </div>
                        <div class="col">
                            <a href="https://www.linkedin.com/in/buiquangbao" target="_blank">
                                <img src="../assets/icon_feather/linkedin.svg" class="svg-f" alt="linkedin">
                            </a>
                        </div>
                        <div class="col">
                            <a href="https://twitter.com/quangbao_dev" target="_blank">
                                <img src="../assets/icon_feather/twitter.svg" class="svg-f" alt="twitter">
                            </a>
                        </div>
                        <div class="col">
                            <a href="https://www.instagram.com/quangbao_dev/" target="_blank">
                                <img src="../assets/icon_feather/instagram.svg" class="svg-f" alt="instagram">
                            </a>
                        </div>
                    </div>
                </div>
                <div class="modal-darkmode">
                    <div class="drk-text">
                        Dark Mode
                    </div>
                    <div style="float: right;">
                        <div class="darkmode-button">
                            <input type="checkbox" id="darkmodeCheck">
                            <span></span>
                            <label for="darkmodeCheck">&nbsp;</label>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

    <div class="row">
        <div class="col-3">
            <div class="post-in-series-wrap">
                <div style="margin-bottom: 2vh;">
                    <div style="font-weight: 500;">
                        DS101
                    </div>
                    <div style="font-size: 13px;">
                        
                        Data Science Fundamental
                        
                    </div>
                </div>
                <ol style="margin-bottom: 2vh;">
                    
                    <a href="/notebook/what-is-data-science" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                    
                    ">
                            <li style="
                        
                        ">
                                What is Data Science?
                            </li>
                        </div>
                    </a>
                    
                    <a href="/notebook/the-place-of-data-science" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                    
                    ">
                            <li style="
                        
                        ">
                                The place of Data Science
                            </li>
                        </div>
                    </a>
                    
                    <a href="/notebook/ethics-and-agency-in-ds" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                    
                    ">
                            <li style="
                        
                        ">
                                Ethics and Agency in Data Science
                            </li>
                        </div>
                    </a>
                    
                    <a href="/notebook/sources-of-data" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                    
                    ">
                            <li style="
                         color: var(--accent); font-weight: 500;
                        ">
                                Sources of Data
                            </li>
                        </div>
                    </a>
                    
                    <a href="/notebook/sources-of-rules" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                    
                    ">
                            <li style="
                        
                        ">
                                Sources of Rules
                            </li>
                        </div>
                    </a>
                    
                    <a href="/notebook/tools-for-ds" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                    
                    ">
                            <li style="
                        
                        ">
                                Tools for Data Science
                            </li>
                        </div>
                    </a>
                    
                    <a href="/notebook/mathematics-for-ds" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                    
                    ">
                            <li style="
                        
                        ">
                                Mathematics for Data Science
                            </li>
                        </div>
                    </a>
                    
                    <a href="/notebook/analyses-for-ds" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                    
                    ">
                            <li style="
                        
                        ">
                                Analyses for Data Science
                            </li>
                        </div>
                    </a>
                    
                    <a href="/notebook/acting-on-ds" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                    
                    ">
                            <li style="
                        
                        ">
                                Acting on Data Science
                            </li>
                        </div>
                    </a>
                    
                    <a href="/notebook/all-markdown-syntax" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                     display: none; 
                    ">
                            <li style="
                        
                        ">
                                All markdown syntax
                            </li>
                        </div>
                    </a>
                    
                    <a href="/notebook/publish-your-package" style="text-decoration: none; font-weight: 400;">
                        <div class="post-in-series" style="
                     display: none; 
                    ">
                            <li style="
                        
                        ">
                                Publish your package to npm
                            </li>
                        </div>
                    </a>
                    
                </ol>
            </div>



            &nbsp;
        </div>
        <div class="col-6">


            <div style="height: var(--padding-top-post);"></div>


            <div style="margin-bottom: 3vh;" class="just-display-on-desktop">
                
                
                <a href="/tag/technology" style="border: none;">
                    <mark class="tag">
                        technology
                    </mark>
                </a>
                
                
                <a href="/tag/web" style="border: none;">
                    <mark class="tag">
                        web
                    </mark>
                </a>
                
                
                <a href="/tag/markdown" style="border: none;">
                    <mark class="tag">
                        markdown
                    </mark>
                </a>
                
                
                <a href="/tag/learning_archive" style="border: none;">
                    <mark class="tag">
                        learning_archive
                    </mark>
                </a>
                
            </div>
            <h1 style="margin-bottom: 3vh;">Sources of Data</h1>
            <div style="font-size: 14px; margin-bottom: 3vh;">
                <!-- Bui Quang Bao -->
                
                <!-- Learning Archive -->
                
                In
                <a href="/tag/learning_archive" style="border: none;">
                    Learning Archive</a>
                

            </div>
            <div class="content">
                <h2 id="1-data-preparation">1. Data preparation</h2>

<p>Anybody who’s cooked knows how time-consuming food prep can be, and that doesn’t say anything about actually going to the market, finding the ingredients, putting things together in bowls and sorting them, let alone cooking the food. And it turns out there’s a similar kind of thing that happens in data science, and that’s the data preparation part. The rule of thumb is that 80% of the time on any data science project is typically spent just getting the data ready. So data preparation, 80%, and everything else falls into about 20%, and you know, that can seem massively inefficient and you may wonder what is your motivation to go through something that’s so time consuming and really, this drudgery? Well, if you want in one phrase, it’s GIGO, that is garbage in, garbage out. That’s a truism from computer science. The information you’re going to get from your analysis is only as good as the information that you put into it, and if you want to put in really starker terms, there’s a wonderful phrase from Twitter, and it says most people who think that they want machine learning or AI really just need linear regression on cleaned-up data. Linear regression is a very basic, simple, and useful procedure and it lets you know just as a rule of thumb, if your data is properly prepared, then the analysis can be something that is quick and clean and easy and easy to interpret. Now when it comes to data preparation and data science, one of the most common phrases you’ll hear is tidy data, which seems a little bit silly, but the concept comes from data scientist Hadley Wickham, and it refers to a way of getting your data set up so it can be easily imported into a program and easily organized and manipulated. And it revolves around some of these very basic principles. Number one, each column in your file is equivalent to a variable, and each row in your file is the same thing as a case or observation. Also you should have one sheet per file. If you have an Excel sheet, you know you can have lots of different sheets in it, but a CSV file has only one sheet and also that each file should have just one level of observations. So you might have a sheet on orders, another one on the SKUs, another one on individual clients, another one on companies and so on and so forth. If you do this, then it makes it very easy to import the data and to get the program up and running. Now this stuff may seem really obvious and you say, well, why do we even have to explain that? It’s because data in spreadsheets frequently is not tidy. You have things like titles and you have images and figures and graphs and you have merged cells and you have color to indicate some data value or you have sub-tables within the sheet or you have summary values or you have comments and notes that might actually contain important data. All of that can be useful if you’re never going beyond that particular spreadsheet, but if you’re trying to take it into another program, all of that gets in the way. And then there are other problems that show up in any kind of data. Things like, for instance, do you actually know what the variable and value labels are? Do you know what the name of this variable is? ‘Cause sometimes, they’re cryptic. Or what does a three on employment status mean? Do you have missing values where you should have data? Do you have misspelled text? If people are writing down the name of the town that they live in or the company they work for, they could write that really in infinite number of ways. Or in a spreadsheet, it’s not uncommon for numbers to accidentally be represented in the spreadsheet as text, and then you can’t do numerical manipulations with it. And then there’s a question of what to do with outliers and then there’s metadata, things like where did the data come from? Who’s the sample? How was it processed? All of this is information you need to have in order to have a clean dataset that you know the context and the circumstances around it that you can analyze it. And that’s to say nothing about trying to get data out of things like scanned PDFs or print tables or print graphs, all of which require either a lot of manual transcription or a lot of very fancy coding. I mean, even take something as simple as emojis, which are now a significant and meaningful piece of communication, especially in social media. This is the rolling on the floor laughing emoji. There are at least 17 different ways of coding this digitally. Here’s a few of them, and if you’re going to be using this as information, you need to prepare your data to code all of these in one single way so that you can then look at these summaries all together and try to get some meaning out of it. I know it’s a lot of work, but just like food prep, is a necessary step to get something beautiful and delicious. Data prep is a necessary, vital step to get something meaningful and actionable out of your data. So give it the time and the attention it deserves, you’ll be richly rewarded.</p>

<h2 id="2-in-house-data">2. In-house data</h2>

<p>Data science projects can feel like this epic expedition or this massive group project. But sometimes, you can get started right here, right now. That is, your organization may already have the data that you need. And there a few major advantages to using this kind of in-house data. The first is it’s fast. It’s the fastest way to start. It’s right there and it’s ready to go. An interesting one is that certain restrictions on data may not apply for use that is entirely within the boundaries of your organization. So if you have data that includes individual identifiers, you may be able to use that for your organization’s own research. Next, you may actually be able to talk with the people who gathered the data in the first place. You can have questions for them. They can tell you how they sampled it, what the things mean, why they did it in this particular way, all of that can save you an enormous amount of time and headaches, and also it may be in the same format that you currently need. And what that means, is that pieces may fit together perfectly. They may have the same code, they may have the same software, they may use the same standards and style guides, and that can save you a lot of time. One the other hand, there are some potential downsides to in-house data. Number one is, if it was an ad-hoc project, it may not be well documented. They may have just kind of thrown it together, and never quite put in all the information about it. It may not be well maintained. Maybe they gathered it five years ago, and kind of let it slip since then. And the biggest one is the data simply may not exist. Maybe what you need really isn’t there in your organization. And so, in-house isn’t an option in that particular case. So, there are some potential downsides. But really, the benefits are so big, are so meaningful, that it’s always worth your time to take just a few minutes to look around and see what’s there, and get started on your project right now.</p>

<h2 id="3-open-data">3. Open data</h2>

<p>I love my public library here in Salt Lake City and this right here is the main library downtown. It’s a beautiful building. There’s desk by a wall of windows looking out over the Wasatch Mountains, it’s a great place to be but even more, I love it because it’s a library and they have books there. They have a collection of over half a million books and other resources including rows and rows of beautiful books on architecture and landscaping that I could never purchase on my own. I love to browse the shelves, find something unexpected and beautiful, go by the windows and enjoy it. Open data is the like the public library of data science. It’s where you can get beautiful things that you might not be able to gather or process on your own and it’s all for the public benefit. Basically it’s data that is free because it has no cost and it’s free to use that you can integrate in your projects and do something amazing. Now there are a few major sources of open data. Number one is government data, number two is scientific data and the third one is data from social media and tech companies and I want to give you a few examples of each of these. Let’s start with government sources. In the United States, the biggest and most obvious one is data.gov which is home of open data from the U.S Federal Government where you can find an enormous amount of datasets on anything from consumer spending, to local government to finance, a wide range of topics that would be really useful for data science. For example, you can look up the Housing Affordability Data System which is going to actually have a big impact on where you can employ people to work for you and where potential customers are for your products and services. If you’re in the United Kingdom, then data.gov.uk is going to be the major choice or if you’re in Sweden, you’ve got open data sources from the government there too. In fact, the Open Knowledge Foundation Runs what they call the Global Open Data Index which gives you information about the open data sources in every country around. At a state-by-state level, you also have open data sources. I live in Utah and this utah.gov’s Open Data Catalog and you can even drill down to the city level, it’s within the same website but now I can focus on specific cases from my city. For scientific data, one great source is the ISCU’s World Data System which is a way of hosting a wide range of scientific datasets that you can use in your own work. There’s also “Nature”, that’s the major science journal has put together a resource called Scientific Data which is designed to both house and facilitate the sharing of the datasets used within this kind of research. There’s the Open Science Data Cloud which by the way, is not to be confused with the Open Data Science Conference, a wonderful event. And then there’s also the Center for Open Science which gives you the ability to house your actual research there, so many opportunities to upload data, to find data, to download and share it and then share your results with other people. And then in social data, one of the favorite ones is Google Trends where you can look at national trends on search terms, so for instance, here’s one that is looking at the relative popularity over time of the terms data science, machine learning, and artificial intelligence. You can also use Google Correlate to get a state-by-state view or weekly time series. If you simply click on Compare US states here in the corner and then enter the term that you want, you can do data science and from this, you can see that California’s high on data science, those are standard deviations there, 1.8 standard deviations above the natural average. Washington is high. Massachusetts is at the very top and that gives you a great way of looking at the relative interest in topics across the country and you can download the data in CSV format and then Yahoo! Finance is one of the great sources for stock market information. And then Twitter allows you to search by tags, by users and it lets you look at things like #DataScience and you can download that information by using the Twitter APIs and include that in your own analyses. So, there’s an enormous range of possibilities with open data. It’s like bringing the best of library availability and teamwork to your own data science projects, so find what’s out there and get the information you need to answer your specific questions and get rolling with the actionable insights you need.</p>

<h2 id="4-apis">4. APIs</h2>

<p>When You draw a picture or write a letter, chances are that you can draw well with one of your hands, your dominant hand and not so much with the other. I recently heard someone describe this as having a well developed API for your dominant hand but only a clunky one for the non-dominant hand. An API or Application Programming Interface isn’t a source of data but rather it’s a way of sharing data, it can take data from one application to another or from a server to your computer. It’s the thing that routes the data, translates it, and gets it ready for use. I want to show you a simple example of how this works. So I’ve gone to this website that has what’s called the JSON Placeholder. JSON stands for JavaScript Object Notation, it’s a data format and if we scroll down here, you’ll see this little, tiny piece of code and what it says is go to this web address and get the data there and then show it, include it and you can just click on this to see what it does. There’s the data in JSON format. If you want to go to just this web address directly, you can and there’s the same data. You can include this information in a Python script or a R script or some other web application that you are developing. It brings it in and it allows you to get up and running very, very quickly. Now API’s can be used for a million different things, three very common categories include social API’s that allow you to access data from Twitter or Facebook and other sources as well as use them as logins for your own sites. Utilities, things like Dropbox and Google Maps so you can include that information in your own apps. Or commerce, Stripe for processing payments or MailChimp for email marketing or things like Slack or a million other applications. The data can be opened which means all you need is the address to get it or it maybe proprietary maybe you have to have a subscription or you purchase it and then you’ll need to log in. But the general process is the same. You include this byte code and it brings the data in and gets you up and running, you can then use that data in data analysis, so it becomes one step of a data science project or maybe your creating a app, you can make a commercial application that relies on data that it pulls from any of several different API’s like weather and directions. Really the idea here is that API’s are teamwork. API’s facilitate the process of bringing things together and then adding value to your analysis and to the data science based services that you offer.</p>

<h2 id="5-scraping-data">5. Scraping data</h2>

<p>Watts Towers in Los Angeles is a collection of sculptures and structures by Simon Rodia that are nearly a hundred feet tall and made from things that he found around him. Scrap pieces of rebar, pieces of porcelain, tile, glass, bottles, sea shells, mirrors, broken pottery and so on. But the towers are testament to what a creative and persistent person can do with the things that they find all around them. Data scraping is, in a sense, the found art of data science. It’s when you take the data that’s around you, tables on pages and graphs in newspapers, and integrate that information into your data science work. Unlike the data that’s available with API’s or Application Programming Interfaces, which is specifically designed for sharing, Data scraping is for data that isn’t necessarily created with that integration in mind. But I need to immediately make a quick statement about ethics and data science. Even though it’s possible to scrape data from digital and print sources, there’s still legal and ethical constraints that you need to be aware of. For instance, you need to respect people’s privacy. If the data is private, you still need to maintain that privacy. You need to respect copyright. Just because something’s on the web doesn’t mean that you can use it for whatever you want. The idea here is Visible Doesn’t Mean Open just like in an open market just because it’s there in front of you and doesn’t have a price tag doesn’t mean it’s free. There are still these important elements of laws, policies, social practices that need to be maintained to not get yourself in some very serious trouble. And so keep that in mind when you’re doing Data scraping. So for instance, let’s say you’re at Wikipedia and you find a table with some data that you want. Here’s a list of dance companies. And you can actually copy and paste this information but you can also use some very, very simple tools for scraping. In fact, if you want to to put into a Google Sheet, there’s even a function that’s designed specifically for that. All you need to do is open up a Google Sheet, and then you use this function: IMPORT HTML. You just give it the address that you’re looking for, say that you’re importing a table and then if there’s more than one table you have to give it the numbers. This one only has one so I can just put in 1, and there’s the data. It just fills it in automatically. And that makes it possible for us to get this huge jumpstart on our analysis. Some other kinds of data that you might be interested in scraping might include things like reviews online and ranking data. You can use specialized apps for scraping consistently structured data or you can use packages and programming languages like Python and R. Now, one interesting version of this is taking something like a heat map or a choropleth is actually what it’s called in this particular case. Here’s a map that I created in Google Correlate on the relative popularity of the term data science. Now, if you’re on Google Correlate, this is interactive. You can hover over and you can download the data. But right now this is just a static image. And if you wanted to get data from this, you could, for instance, write a script that’d go through the image pixel by pixel, get the color of each pixel, you can then compare to the scales at the bottom left. You can get the x, y coordinates for each pixel and then compare that to a shape file of the U.S. and then ultimately put the data in a spreadsheet. It’s not an enormous amount of work and it is a way of recovering the data that was used to create a map like this. If you want to see a really good example of Data scraping after (mumbles) where you’re taking image data and getting something useful out of it, here at at publicintellegence.net, they’re reporting on a project that’s done with Syrian refugee camps and what they’re using here are satellite photos and then they’re using a machine learning algorithm to count the number of tents within the camp and that’s important because you see as it goes over time, it gets much, much larger. And this can be used to access the severity of a humanitarian crisis and really how well you can respond to it. But it’s the same basic principles of Data scraping. Taking something that was created in one way, just an image, but then using the computer algorithms to extract some information out of it and give you the ability to get some extra insight and figure out what your next steps in your project need to be.</p>

<h2 id="6-creating-data">6. Creating data</h2>

<p>Sometimes you need something special, something that’s not already there. In the data science world, there’s a lot of data you can get from in house data, open data APIs, and even data scraping, but if you still can’t get the data you need to answer the questions you care about, then you can go the DIY route, and get your own data. There are several different ways to go about this. The first one I would recommend is just natural observation. See what people are doing. Go outside, see what’s happening in the real world, or observe online, see what people are saying about the topics that you’re interested in. Just watching is going to be the first, and most helpful way of gathering new data. Once you’ve watched a little bit about what’s happening, you can try having informal discussions with, for instance, potential clients. You can do this in person in a one on one, or a focus group setting. You can do it online through email, or through chat, and this time you’re asking specific questions to get the information you need to focus your own projects. If you’ve gone through that, you might consider doing formal interviews. This is where you have a list of questions, things you are specifically trying to focus on, and getting direct feedback from potential customers, and clients. And if you want to go one step beyond that, you can do surveys, or questionnaires, and you can start asking close ended questions. Ask people to check off, you know, excellent, good, or you can ask them to say yes, or no, or recommend something in particular. You usually don’t want to do that, however, until you’ve done the other things ahead of time, because this makes a lot of assumptions. And the idea here is that you already know what the range of responses are, and so make sure you don’t get ahead of yourself with this one. Throughout all of this, one general principle is important, especially in preliminary research, and that is that words are greater than numbers. When you’re trying to understand people’s experience, be as open ended as possible. Let them describe it in their own terms. You can narrow things down, and quantify them later by counting, for instance, how many people put this general response, or that general response, but start by letting people express themselves as freely as possible. Also, a couple of things to keep in mind when you’re trying to gather your own data. Number one is don’t get ahead of yourself. Again, start with the very big picture, and then slowly narrow it down. Don’t do a survey until you’ve done interviews. Don’t do interviews until you’ve watched how people are behaving. Start with the general information, and then move to more specific things as you focus on the things that you can actually have some influence on, the actionable steps in your own projects. Also be respectful of people’s time and information. They’re giving you something of value, so make sure that you are that. Don’t take more time than you need to. Don’t gather information you don’t need to, just what is necessary to help you get the insights you need in your own particular project. Now, another method of gathering data that can be extremely useful is an experiment. Now, I come from a background in experimental social psychology. The experiments are very time consuming, very labor intensive, but that’s not what we’re talking about in the ecommerce, and the tech world. Usually here we’re referring to what’s called AB Testing where for instance you prepare two different versions of a website, or a landing page, and you see how many people click on things as a result of those two different versions. That’s extremely quick, and easy, and effective, and people should constantly be doing that to try to refine, and focus on the most effective elements of their website. But through all of this, there is one overarching principle. When you’re gathering data, when you’re engaging with people, seriously, keep it simple, simpler is better, more focused is more efficient, and you’re going to be better able to interpret it to get useful, next steps out of the whole thing. I also want to mention two elements of research ethics that are especially important when you are gathering new data. Number one is informed consent. When you’re gathering data from people, they need to know what you want from them, and they also need to know what you’re going to do with it so they can make an informed decision about whether they want to participate. Remember, they’re giving you something of value. This is something that needs to be their own free choice. The second one, which can be expressed in many different ways is privacy. Also sometimes confidentiality, or anonymity. You need to keep identifiers to a minimum. Don’t gather information that you don’t need, and keep the data confidential, and protected. Now that’s actually a challenge because when you get this kind of data, it’s often of value, and it’s easy for it to get compromised. So maintaining privacy really is incumbent upon the researcher, and it’s part of building the trust, and the good faith for people to work with you, and others again in the future. And then finally to repeat something I said before, people are sharing something of value with you. So it’s always a good idea to show gratitude, and response both by saying thank you in the short term. And also by providing them with better, more useful services, they’re going to make things better in their own lives.</p>

<h2 id="7-passive-collection-of-training-data">7. Passive collection of training data</h2>

<p>Some things you can learn faster than others, and food is a good example. A lot of people love eating mussels, but some people eat them, get sick, and then never want to touch them again, and I’m one of those people. This is called a Conditioned Taste Aversion, and results from something that is psychologically unusual, and that’s one-trial learning. You only have to do it once to get in ingrained into your behavior, but if you’re working in data science, and especially, if you’re training a machine-learning algorithm, you’re going to need a lot more than one trial for learning. You’re going to need enormous amount of labeled training data to get your algorithm up and working properly. One of the interesting things about data science, is that gathering enormous amounts of data doesn’t always involve enormous amounts of work. In certain respects, you can just sit there and wait for it to come to you. That’s the beauty of Passive Data Collection. Now, there are a few examples of this, and are pretty common. One is, for instance, Photo Classification. You don’t have to classify the photos. The people who take the photos and load them online will often tag them for you, put titles on them, or share ‘em in a folder, that’s classification that comes around, that you can use in machine-learning without you having to go through the work personally. Or, Autonomous Cars, as they drive, they are gathering enormous amounts of information from the whole plethora of sensors that they have. That information is combined, it’s uploaded to distant servers, and that allows you to improve the way that your automobiles function. Or even Health Data, people who have smart watches are able to constantly gather information about their activity, the number of steps, how many flights they’ve walked, how long they’ve slept, their heart rate, all this information is gathered without you doing any extra work, and if you’re the provider of an app that measures health, you can get that information directly without having to do anything else for it. Now, there’s several benefits to this kind of passive collection of training data. Number one is that, you can get a lot of it. You can get enormous amounts of data very quickly, simply by setting up the procedure and then letting it roll, either automatically, or outsourcing it to the people who use it. The data that you gather can be either very general, or it can be very, very specific; general like, categorizing photos as cats or dogs, or specific like, knowing what a particular person’s heart rate is at a particular time of day. There are, on the other hand, some challenges associated with this passive data collection. One, and this is actually a huge issue, is that you need to ensure that you have adequate representation; things like categorizing photos. You need to make sure you have lots of different kinds of photos of lots of different kinds of things, and different kinds of people, so you can get all of those categorized, and that your algorithm understands the diversity of the information it will be encountering. You also need to check for shared meaning. What that is, for instance, is that something being happy, or beautiful, you need to make sure that people interpret that in the same way. You also need to check for limit cases. Think about, for instance, heart rate. Some people are going to have just higher heart rate than others, and you don’t want to have an algorithm that always says, anything above this level is a heart problem, anything below is fine, because that is going to vary from one person to another, and one situation to another, but what all of this does together, is it helps you assemble the massive amounts of data that you need to get critical work done in data science.</p>

<h2 id="8-self-generated-data">8. Self-generated data</h2>

<p>When I was growing up, I remember an ad for toys that said wind it up and watch it go. But now you can do a similar kind of thing with data science. You can do this looping back process. This is where computers, the algorithms in them, can engage themselves to create the data they need for machine learning algorithms. It’s a little bit like the mythical self-consuming snake that comes all the way back around. And the reason this is important is because you need data for training your machine learning algorithms so they can determine how to categorize something or the best way to proceed and having the machines generate it by engaging with themselves is an amazingly efficient and useful way of doing that. There are at least three different versions of this and I’m giving a little bit of my own terminology here. The first one is what I am calling external reinforcement learning. Now reinforcement learning is a very common term. It means an algorithm that is designed to reach a particular outcome like for instance running through the levels of a game. I’m calling it external because it’s focusing on some outside contingency and it’s this method for example that allowed Google’s Deep Mind algorithms teach an AI to actually learn on its own how to move through the levels of a video game. It was a major accomplishment to do it with no instruction except for move forward and get to the end. There’s also generative adversarial networks and they are used a lot in things like generating audio or video or images that seem photorealistic. It’s both exciting and scary at the same time. The idea here is that one neural network generates an image and that a second neural network tries to determine whether that image is legitimate or whether it’s been modified in some way. And that if it gets caught with a fake, then the other one has to learn how to do it better. Again, this has gotten to the point where you can have photorealistic face switching in videos. Again, both exciting and scary, but done with what’s called a generative adversarial network. And then there’s another kind of reinforcement learning which I’m calling internal and this is the kind where the algorithm works with itself and the best example of this is Deep Mind again learning to play chess and go and other games by playing millions and millions of games against itself in just some a few hours and mastering the game completely. Now there are a few important benefits to this. One of course is that you can get millions of variations and millions of trials, a gargantuan amount of data very, very quickly. And that sometimes the algorithms can create scenarios that humans wouldn’t. I mean, something they wouldn’t even think of or deem possible. And this kind of data is needed for creating the rules that go into the algorithms of machine learning that I’ll talk about later. And the reason this is so important is because the kind of data that you can create so quickly using this method is exactly the kind of data both the variability and the quantity that you need for creating effective rules in machine learning algorithms which is what we’re going to turn to next.</p>

            </div>
            <br>
            Thanks for reading!
            <div style="margin: 6vh 0; height: 0.5px; width: 100%; border-bottom: var(--border);"></div>
            <p style="margin-bottom: 7vh; text-align: center;">
                <a href="/" style="font-weight: 400; border: none;">
                    <script>document.write(new Date().getFullYear())</script>
                    &copy;
                    Bui Quang Bao
                </a>
            </p>


        </div>
        <div class="col-3">
            &nbsp;
        </div>
    </div>

    <script>
    // Darkmode
    function enableDarkMode() {
        document.body.classList.add("darkmode");
        localStorage.setItem("isDarkMode", "true");
        document.getElementById("darkmodeCheck").checked = true;
    }
    function disableDarkMode() {
        document.body.classList.remove("darkmode");
        localStorage.setItem("isDarkMode", "false");
        document.getElementById("darkmodeCheck").checked = false;
    }
    let isDarkMode = localStorage.getItem("isDarkMode");
    if (isDarkMode === "true") {
        enableDarkMode();
        document.getElementById("darkmodeCheck").checked = true;
    }
    const darkmodeButton = document.querySelector(".darkmode-button");
    darkmodeButton.addEventListener("click", function () {
        isDarkMode = localStorage.getItem("isDarkMode");
        isDarkMode === "true" ? disableDarkMode() : enableDarkMode();
    });
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<!-- <script src="https://kit.fontawesome.com/0422542145.js" crossorigin="anonymous"></script> -->
<script src="https://unpkg.com/creatoper@1.1.5/creatoper.js" defer></script>
<script src="../scripts.js" defer></script>



<!-- ------ DON'T DELETE THIS LINE ------ -->
<div id="test-scripts" style="display: none;">> Error!</div>
</body>

</html>